{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dfd35c9-6c4a-4f31-ad8c-2af95fa42cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Scores_df = pd.read_csv(\"/Users/rajeshwarrao/Downloads/Result_Scores_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "209a9a99-5d76-4693-9a08-b6875c19588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Status (Single/ partner)</th>\n",
       "      <th>Residence Zip Code</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>...</th>\n",
       "      <th>Score_LS_Exercise</th>\n",
       "      <th>Score_LS_Drug_abuse</th>\n",
       "      <th>Score_LS_Smoking</th>\n",
       "      <th>Score_LS_Alcoholic</th>\n",
       "      <th>Score_LS_Professional_hazards</th>\n",
       "      <th>Score_LS_Other_factors</th>\n",
       "      <th>Score_SF_Awareness_level</th>\n",
       "      <th>Score_SF_Social_media_activity</th>\n",
       "      <th>Score_SF_Peer_group_influence</th>\n",
       "      <th>Score_SF_Other_social_activities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>60 +</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>577</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>578</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>579</td>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>No Job</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>580</td>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>581</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    S.N                   Name     ID Age group Sex  \\\n",
       "0             0  265.0  Frist_1 Last name_133  NM265  below 30   M   \n",
       "1             1  202.0  Frist_2 Last name_102  NM202     30-45   F   \n",
       "2             2  490.0  Frist_2 Last name_246  NM490     30-45   F   \n",
       "3             3  580.0  Frist_2 Last name_291  NM580      60 +   F   \n",
       "4             4  274.0  Frist_2 Last name_138  NM274     30-45   F   \n",
       "..          ...    ...                    ...    ...       ...  ..   \n",
       "577         577   85.0   Frist_1 Last name_43  NM085  below 30   M   \n",
       "578         578   29.0   Frist_1 Last name_15  NM029  below 30   M   \n",
       "579         579  573.0  Frist_1 Last name_287  NM573  below 30   M   \n",
       "580         580  565.0  Frist_1 Last name_283  NM565  below 30   M   \n",
       "581         581    5.0    Frist_1 Last name_3  NM005  below 30   M   \n",
       "\n",
       "    Status (Single/ partner)        Residence Zip Code  Score_Demo  \\\n",
       "0                     Single   Area Deprovation Index1        40.0   \n",
       "1                    Partner  Area Deprovation Index 2        80.0   \n",
       "2                     Single  Area Deprovation Index 2        70.0   \n",
       "3                     Single  Area Deprovation Index 2        90.0   \n",
       "4                    Partner  Area Deprovation Index 2        80.0   \n",
       "..                       ...                       ...         ...   \n",
       "577                   Single   Area Deprovation Index1        40.0   \n",
       "578                  Partner   Area Deprovation Index1        50.0   \n",
       "579                  Partner   Area Deprovation Index1        50.0   \n",
       "580                   Single   Area Deprovation Index1        40.0   \n",
       "581                   Single   Area Deprovation Index1        40.0   \n",
       "\n",
       "       Occupation  ... Score_LS_Exercise Score_LS_Drug_abuse  \\\n",
       "0    Professional  ...              10.0                10.0   \n",
       "1        Business  ...              10.0                10.0   \n",
       "2        Business  ...              10.0                10.0   \n",
       "3        Business  ...              10.0                10.0   \n",
       "4        Business  ...              10.0                10.0   \n",
       "..            ...  ...               ...                 ...   \n",
       "577  Professional  ...              20.0                20.0   \n",
       "578  Professional  ...              20.0                20.0   \n",
       "579        No Job  ...              20.0                20.0   \n",
       "580  Professional  ...              20.0                20.0   \n",
       "581  Professional  ...              20.0                20.0   \n",
       "\n",
       "     Score_LS_Smoking  Score_LS_Alcoholic Score_LS_Professional_hazards  \\\n",
       "0                10.0                20.0                          10.0   \n",
       "1                10.0                20.0                          10.0   \n",
       "2                10.0                20.0                          10.0   \n",
       "3                10.0                20.0                          10.0   \n",
       "4                10.0                20.0                          10.0   \n",
       "..                ...                 ...                           ...   \n",
       "577              10.0                10.0                          20.0   \n",
       "578              10.0                10.0                          20.0   \n",
       "579              10.0                10.0                          20.0   \n",
       "580              10.0                10.0                          20.0   \n",
       "581              10.0                10.0                          20.0   \n",
       "\n",
       "    Score_LS_Other_factors Score_SF_Awareness_level  \\\n",
       "0                     20.0                     20.0   \n",
       "1                     20.0                     20.0   \n",
       "2                     20.0                     20.0   \n",
       "3                     20.0                     20.0   \n",
       "4                     20.0                     20.0   \n",
       "..                     ...                      ...   \n",
       "577                   20.0                     10.0   \n",
       "578                   20.0                     10.0   \n",
       "579                   20.0                     10.0   \n",
       "580                   20.0                     10.0   \n",
       "581                   20.0                     10.0   \n",
       "\n",
       "     Score_SF_Social_media_activity  Score_SF_Peer_group_influence  \\\n",
       "0                              20.0                           10.0   \n",
       "1                              20.0                           10.0   \n",
       "2                              20.0                           10.0   \n",
       "3                              20.0                           10.0   \n",
       "4                              20.0                           10.0   \n",
       "..                              ...                            ...   \n",
       "577                            10.0                           20.0   \n",
       "578                            10.0                           20.0   \n",
       "579                            10.0                           20.0   \n",
       "580                            10.0                           20.0   \n",
       "581                            10.0                           20.0   \n",
       "\n",
       "    Score_SF_Other_social_activities  \n",
       "0                                 10  \n",
       "1                                 10  \n",
       "2                                 10  \n",
       "3                                 10  \n",
       "4                                 10  \n",
       "..                               ...  \n",
       "577                               10  \n",
       "578                               10  \n",
       "579                               10  \n",
       "580                               10  \n",
       "581                               10  \n",
       "\n",
       "[582 rows x 81 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77ccf419-3f7a-4ae5-b16b-f324547bcc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S.N</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>FBP Level</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th># of Provider visist YTD</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Formulary visist YTD</th>\n",
       "      <th>Days of Therapy missed</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>...</th>\n",
       "      <th>Score_LS_Exercise</th>\n",
       "      <th>Score_LS_Drug_abuse</th>\n",
       "      <th>Score_LS_Smoking</th>\n",
       "      <th>Score_LS_Alcoholic</th>\n",
       "      <th>Score_LS_Professional_hazards</th>\n",
       "      <th>Score_LS_Other_factors</th>\n",
       "      <th>Score_SF_Awareness_level</th>\n",
       "      <th>Score_SF_Social_media_activity</th>\n",
       "      <th>Score_SF_Peer_group_influence</th>\n",
       "      <th>Score_SF_Other_social_activities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>290.500000</td>\n",
       "      <td>291.500000</td>\n",
       "      <td>70.962199</td>\n",
       "      <td>148.276553</td>\n",
       "      <td>56.013746</td>\n",
       "      <td>1.991409</td>\n",
       "      <td>70.429553</td>\n",
       "      <td>1.994845</td>\n",
       "      <td>4.548110</td>\n",
       "      <td>41.735395</td>\n",
       "      <td>...</td>\n",
       "      <td>12.491409</td>\n",
       "      <td>10.463918</td>\n",
       "      <td>11.254296</td>\n",
       "      <td>17.508591</td>\n",
       "      <td>11.254296</td>\n",
       "      <td>19.982818</td>\n",
       "      <td>17.508591</td>\n",
       "      <td>17.508591</td>\n",
       "      <td>11.254296</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>168.153204</td>\n",
       "      <td>168.153204</td>\n",
       "      <td>18.713400</td>\n",
       "      <td>26.748788</td>\n",
       "      <td>12.191239</td>\n",
       "      <td>1.631740</td>\n",
       "      <td>12.493896</td>\n",
       "      <td>1.631755</td>\n",
       "      <td>2.955829</td>\n",
       "      <td>13.831246</td>\n",
       "      <td>...</td>\n",
       "      <td>4.328876</td>\n",
       "      <td>2.105129</td>\n",
       "      <td>3.314903</td>\n",
       "      <td>4.328876</td>\n",
       "      <td>3.314903</td>\n",
       "      <td>0.414513</td>\n",
       "      <td>4.328876</td>\n",
       "      <td>4.328876</td>\n",
       "      <td>3.314903</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>145.250000</td>\n",
       "      <td>146.250000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>290.500000</td>\n",
       "      <td>291.500000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>435.750000</td>\n",
       "      <td>436.750000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>581.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         S.N  Score_Demo   FBP Level   Score_Fin  \\\n",
       "count  582.000000  582.000000  582.000000  499.000000  582.000000   \n",
       "mean   290.500000  291.500000   70.962199  148.276553   56.013746   \n",
       "std    168.153204  168.153204   18.713400   26.748788   12.191239   \n",
       "min      0.000000    1.000000   40.000000  120.000000   30.000000   \n",
       "25%    145.250000  146.250000   52.500000  120.000000   50.000000   \n",
       "50%    290.500000  291.500000   70.000000  150.000000   60.000000   \n",
       "75%    435.750000  436.750000   80.000000  150.000000   60.000000   \n",
       "max    581.000000  582.000000  100.000000  200.000000   80.000000   \n",
       "\n",
       "       # of Provider visist YTD  Score_Medical_History  Formulary visist YTD  \\\n",
       "count                582.000000             582.000000            582.000000   \n",
       "mean                   1.991409              70.429553              1.994845   \n",
       "std                    1.631740              12.493896              1.631755   \n",
       "min                    0.000000              45.000000              0.000000   \n",
       "25%                    1.000000              60.000000              1.000000   \n",
       "50%                    2.000000              70.000000              2.000000   \n",
       "75%                    3.000000              80.000000              3.000000   \n",
       "max                    5.000000             100.000000              5.000000   \n",
       "\n",
       "       Days of Therapy missed  Score_Medication_History  ...  \\\n",
       "count              582.000000                582.000000  ...   \n",
       "mean                 4.548110                 41.735395  ...   \n",
       "std                  2.955829                 13.831246  ...   \n",
       "min                  0.000000                 20.000000  ...   \n",
       "25%                  3.000000                 30.000000  ...   \n",
       "50%                  5.000000                 40.000000  ...   \n",
       "75%                  7.000000                 50.000000  ...   \n",
       "max                 18.000000                 60.000000  ...   \n",
       "\n",
       "       Score_LS_Exercise  Score_LS_Drug_abuse  Score_LS_Smoking  \\\n",
       "count         582.000000           582.000000        582.000000   \n",
       "mean           12.491409            10.463918         11.254296   \n",
       "std             4.328876             2.105129          3.314903   \n",
       "min            10.000000            10.000000         10.000000   \n",
       "25%            10.000000            10.000000         10.000000   \n",
       "50%            10.000000            10.000000         10.000000   \n",
       "75%            10.000000            10.000000         10.000000   \n",
       "max            20.000000            20.000000         20.000000   \n",
       "\n",
       "       Score_LS_Alcoholic  Score_LS_Professional_hazards  \\\n",
       "count          582.000000                     582.000000   \n",
       "mean            17.508591                      11.254296   \n",
       "std              4.328876                       3.314903   \n",
       "min             10.000000                      10.000000   \n",
       "25%             20.000000                      10.000000   \n",
       "50%             20.000000                      10.000000   \n",
       "75%             20.000000                      10.000000   \n",
       "max             20.000000                      20.000000   \n",
       "\n",
       "       Score_LS_Other_factors  Score_SF_Awareness_level  \\\n",
       "count              582.000000                582.000000   \n",
       "mean                19.982818                 17.508591   \n",
       "std                  0.414513                  4.328876   \n",
       "min                 10.000000                 10.000000   \n",
       "25%                 20.000000                 20.000000   \n",
       "50%                 20.000000                 20.000000   \n",
       "75%                 20.000000                 20.000000   \n",
       "max                 20.000000                 20.000000   \n",
       "\n",
       "       Score_SF_Social_media_activity  Score_SF_Peer_group_influence  \\\n",
       "count                      582.000000                     582.000000   \n",
       "mean                        17.508591                      11.254296   \n",
       "std                          4.328876                       3.314903   \n",
       "min                         10.000000                      10.000000   \n",
       "25%                         20.000000                      10.000000   \n",
       "50%                         20.000000                      10.000000   \n",
       "75%                         20.000000                      10.000000   \n",
       "max                         20.000000                      20.000000   \n",
       "\n",
       "       Score_SF_Other_social_activities  \n",
       "count                             582.0  \n",
       "mean                               10.0  \n",
       "std                                 0.0  \n",
       "min                                10.0  \n",
       "25%                                10.0  \n",
       "50%                                10.0  \n",
       "75%                                10.0  \n",
       "max                                10.0  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5763b75a-6caf-41aa-86ea-d339e5813b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>Score_Physical_Status</th>\n",
       "      <th>Score_Lifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S.N                   Name     ID  Score_Demo  Score_Fin  \\\n",
       "0    265.0  Frist_1 Last name_133  NM265        40.0       30.0   \n",
       "1    202.0  Frist_2 Last name_102  NM202        80.0       40.0   \n",
       "2    490.0  Frist_2 Last name_246  NM490        70.0       60.0   \n",
       "3    580.0  Frist_2 Last name_291  NM580        90.0       40.0   \n",
       "4    274.0  Frist_2 Last name_138  NM274        80.0       70.0   \n",
       "..     ...                    ...    ...         ...        ...   \n",
       "577   85.0   Frist_1 Last name_43  NM085        40.0       50.0   \n",
       "578   29.0   Frist_1 Last name_15  NM029        50.0       60.0   \n",
       "579  573.0  Frist_1 Last name_287  NM573        50.0       60.0   \n",
       "580  565.0  Frist_1 Last name_283  NM565        40.0       40.0   \n",
       "581    5.0    Frist_1 Last name_3  NM005        40.0       40.0   \n",
       "\n",
       "     Score_Medical_History  Score_Medication_History  Score_Physical_Status  \\\n",
       "0                     70.0                      60.0                   50.0   \n",
       "1                     85.0                      60.0                   50.0   \n",
       "2                     85.0                      60.0                   50.0   \n",
       "3                     90.0                      60.0                   50.0   \n",
       "4                     85.0                      60.0                   50.0   \n",
       "..                     ...                       ...                    ...   \n",
       "577                   90.0                      60.0                   80.0   \n",
       "578                   90.0                      60.0                   80.0   \n",
       "579                   80.0                      60.0                   80.0   \n",
       "580                   65.0                      40.0                   80.0   \n",
       "581                   65.0                      30.0                   80.0   \n",
       "\n",
       "     Score_Lifestyle  \n",
       "0               80.0  \n",
       "1               80.0  \n",
       "2               80.0  \n",
       "3               80.0  \n",
       "4               80.0  \n",
       "..               ...  \n",
       "577            100.0  \n",
       "578            100.0  \n",
       "579            100.0  \n",
       "580            100.0  \n",
       "581            100.0  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_21384/1248983833.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Demo_df['target']=Demo_df['Score_Demo'].astype(float)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m Demo_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtarget\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39mDemo_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore_Demo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mfloat\u001b[39m)\n\u001b[1;32m      8\u001b[0m Demo_df\u001b[38;5;241m=\u001b[39mDemo_df\u001b[38;5;241m.\u001b[39mdrop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mScore_Demo\u001b[39m\u001b[38;5;124m'\u001b[39m,axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241m.\u001b[39mdescribe()\n\u001b[1;32m     10\u001b[0m display(Demo_df)\n\u001b[1;32m     11\u001b[0m Demo_df\u001b[38;5;241m.\u001b[39mdescribe()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Demo_columns = [ 'Age group','Sex','Status (Single/ partner)','Residence Zip Code','Score_Demo']\n",
    "Demo_df=Scores_df[Demo_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "Demo_df['target']=Demo_df['Score_Demo'].astype(float)\n",
    "Demo_df=Demo_df.drop('Score_Demo',axis=1)\n",
    "df.describe()\n",
    "display(Demo_df)\n",
    "Demo_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b0e6bfb-1ae3-40bf-84ec-d8325be29341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 21:34:37.148153: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "    Age group Sex Status (Single/ partner)        Residence Zip Code  target\n",
      "0    below 30   M                   Single   Area Deprovation Index1    40.0\n",
      "1       30-45   F                  Partner  Area Deprovation Index 2    80.0\n",
      "2       30-45   F                   Single  Area Deprovation Index 2    70.0\n",
      "3        60 +   F                   Single  Area Deprovation Index 2    90.0\n",
      "4       30-45   F                  Partner  Area Deprovation Index 2    80.0\n",
      "..        ...  ..                      ...                       ...     ...\n",
      "577  below 30   M                   Single   Area Deprovation Index1    40.0\n",
      "578  below 30   M                  Partner   Area Deprovation Index1    50.0\n",
      "579  below 30   M                  Partner   Area Deprovation Index1    50.0\n",
      "580  below 30   M                   Single   Area Deprovation Index1    40.0\n",
      "581  below 30   M                   Single   Area Deprovation Index1    40.0\n",
      "\n",
      "[582 rows x 5 columns]\n",
      "X_train\n",
      "    Age group Sex Status (Single/ partner)        Residence Zip Code\n",
      "433     45-60   F                  Partner   Area Deprovation Index1\n",
      "209     45-60   M                  Partner   Area Deprovation Index1\n",
      "184     30-45   F                  Partner  Area Deprovation Index 2\n",
      "177     30-45   F                   Single  Area Deprovation Index 2\n",
      "410     45-60   F                  Partner   Area Deprovation Index1\n",
      "..        ...  ..                      ...                       ...\n",
      "71      30-45   F                  Partner  Area Deprovation Index 2\n",
      "106      60 +   F                  Partner  Area Deprovation Index 2\n",
      "270      60 +   F                   Single  Area Deprovation Index 2\n",
      "435     45-60   F                  Partner   Area Deprovation Index1\n",
      "102     45-60   M                   Single   Area Deprovation Index1\n",
      "\n",
      "[465 rows x 4 columns]\n",
      "test1\n",
      "433    4\n",
      "209    3\n",
      "184    4\n",
      "177    3\n",
      "410    4\n",
      "      ..\n",
      "71     4\n",
      "106    6\n",
      "270    5\n",
      "435    4\n",
      "102    2\n",
      "Name: target_encoded, Length: 465, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 21:34:49.378083: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE4.1 SSE4.2\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-10-14 21:34:50.031738: W tensorflow/core/framework/op_kernel.cc:1807] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential/Cast' defined at (most recent call last):\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_21384/3496551992.py\", line 51, in <module>\n      model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 650, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 762, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential/Cast'\nCast string to float is not supported\n\t [[{{node sequential/Cast}}]] [Op:__inference_train_function_978]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential/Cast' defined at (most recent call last):\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_21384/3496551992.py\", line 51, in <module>\n      model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 650, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 762, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential/Cast'\nCast string to float is not supported\n\t [[{{node sequential/Cast}}]] [Op:__inference_train_function_978]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "#Demo_df['Score_Demo']=Demo_df['Score_Demo'].astype(str)\n",
    "print(\"test\")\n",
    "#print(Demo_df)\n",
    "df=Demo_df\n",
    "print(df)\n",
    "#df['target']=df['target'].astype(float)\n",
    "df.describe()\n",
    "'''\n",
    "df['target']=df['Score_Demo']\n",
    "df=df.drop('Score_Demo',axis=1)\n",
    "df.describe()\n",
    "'''\n",
    "# Preprocess the data, e.g., encoding categorical variables and scaling numerical features\n",
    "\n",
    "# Encode class labels (assuming 'target' is the column containing class labels)\n",
    "label_encoder = LabelEncoder()\n",
    "df['target_encoded'] = label_encoder.fit_transform(df['target'])\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df.drop(['target', 'target_encoded'], axis=1)\n",
    "y = df['target_encoded']\n",
    "\n",
    "#y = tf.convert_to_tensor(y, dtype=tf.int64)\n",
    "#y = y.tolist()\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "# Define the deep learning model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='softmax')  # Multiclass classification with 5 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(\"test1\")\n",
    "print(y_train)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0acc7d7f-a6be-41a8-9f23-63b89d2b9f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6068376068376068\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00        17\n",
      "           1       0.51      1.00      0.68        18\n",
      "           2       0.00      0.00      0.00         8\n",
      "           3       0.62      1.00      0.77        33\n",
      "           4       0.00      0.00      0.00        12\n",
      "           5       0.00      0.00      0.00         9\n",
      "           6       0.69      1.00      0.82        20\n",
      "\n",
      "    accuracy                           0.61       117\n",
      "   macro avg       0.26      0.43      0.32       117\n",
      "weighted avg       0.37      0.61      0.46       117\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# Replace 'your_dataset.csv' with the path to your dataset\n",
    "df = Demo_df\n",
    "\n",
    "# Split the DataFrame into features (X) and target (y)\n",
    "X = df.iloc[:, :-1]  # All columns except the last one\n",
    "y = df.iloc[:, -1]   # The last column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the text data using TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the max_features parameter\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['Age group'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['Age group'])\n",
    "\n",
    "# Train a text classification model (e.g., Multinomial Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:\\n', classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fb3e5215-77a8-45f6-9fcf-05f81cd3752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with float values: []\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - 1s 23ms/step - loss: 1.8599 - accuracy: 0.2225 - val_loss: 1.6890 - val_accuracy: 0.4681\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.6526 - accuracy: 0.3756 - val_loss: 1.4221 - val_accuracy: 0.4681\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 0s 12ms/step - loss: 1.3904 - accuracy: 0.5263 - val_loss: 1.1214 - val_accuracy: 0.7872\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 1.1177 - accuracy: 0.6866 - val_loss: 0.8938 - val_accuracy: 0.8936\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.8613 - accuracy: 0.8278 - val_loss: 0.6776 - val_accuracy: 0.8936\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.6076 - accuracy: 0.8923 - val_loss: 0.4696 - val_accuracy: 0.8936\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.4183 - accuracy: 0.8947 - val_loss: 0.3172 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 0s 9ms/step - loss: 0.2920 - accuracy: 0.9713 - val_loss: 0.2210 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 0.1934 - accuracy: 1.0000 - val_loss: 0.1509 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 0s 11ms/step - loss: 0.1379 - accuracy: 1.0000 - val_loss: 0.0962 - val_accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 3ms/step - loss: 0.0951 - accuracy: 1.0000\n",
      "Accuracy: 1.0\n",
      "4/4 [==============================] - 0s 3ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       100.0       1.00      1.00      1.00        20\n",
      "        40.0       1.00      1.00      1.00        17\n",
      "        50.0       1.00      1.00      1.00        18\n",
      "        60.0       1.00      1.00      1.00         8\n",
      "        70.0       1.00      1.00      1.00        33\n",
      "        80.0       1.00      1.00      1.00        12\n",
      "        90.0       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00       117\n",
      "   macro avg       1.00      1.00      1.00       117\n",
      "weighted avg       1.00      1.00      1.00       117\n",
      "\n",
      "1/1 [==============================] - 0s 19ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LabelEncoder.inverse_transform() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m input_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbelow 30,M,Single,Area Deprovation Index1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m predicted_class,predictions \u001b[38;5;241m=\u001b[39m predict_single_string(input_string)\n\u001b[0;32m--> 102\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: LabelEncoder.inverse_transform() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your DataFrame\n",
    "# Let's assume you have a DataFrame 'df' with multiple text columns and the target column 'target'\n",
    "df=Demo_df\n",
    "df['target']=df['target'].astype(str)\n",
    "# Preprocess the text data\n",
    "# Combine multiple text columns into one, assuming they are named 'text_col1', 'text_col2', ...\n",
    "text_columns = ['Age group','Sex','Status (Single/ partner)','Residence Zip Code']\n",
    "df['combined_text'] = df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['target_encoded'] = label_encoder.fit_transform(df['target'])\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = df['combined_text']\n",
    "y = df['target_encoded']\n",
    "\n",
    "float_columns = []\n",
    "for column in df.columns:\n",
    "    for value in df[column]:\n",
    "        if isinstance(value, float):\n",
    "            float_columns.append(column)\n",
    "\n",
    "# Remove duplicates if needed\n",
    "float_columns = list(set(float_columns))\n",
    "\n",
    "# Print the columns containing float values\n",
    "print(\"Columns with float values:\", float_columns)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 10000  # Set the maximum number of words in your vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text data to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_sequence_length = 100  # Set the maximum sequence length\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Build a text classification model\n",
    "embedding_dim = 64  # Set the embedding dimension\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(len(label_encoder.classes_), activation='softmax')  # Output layer for multiclass classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert predicted class indices back to class labels\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "def predict_single_string(input_string):\n",
    "    # Preprocess the input string similar to how you preprocessed your training data\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_string])\n",
    "    input_padded = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(input_padded)\n",
    "\n",
    "    # Assuming it's a classification task, you can get the predicted class\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "    return predicted_class,predictions\n",
    "\n",
    "input_string = \"below 30,M,Single,Area Deprovation Index1\"\n",
    "predicted_class,predictions = predict_single_string(input_string)\n",
    "y_pred_labels = label_encoder.inverse_transform(np.argmax(predictions),axis=1)\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Predicted Value: {y_pred_labels}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513b2eb4-eacf-4d99-be38-446371133210",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b660d7d-b648-44c5-9ae7-7858fcbd99f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "text_data = \"below 30,M,Single,Area Deprovation Index1\" \n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")  # `num_words` limits the vocabulary size\n",
    "\n",
    "# Fit the tokenizer on the text data\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to a uniform length (e.g., maxlen=100)\n",
    "max_length = 100  # Adjust this based on your specific requirements\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Inspect the tokenized and padded sequences\n",
    "print(\"Tokenized sequences:\")\n",
    "print(sequences)\n",
    "\n",
    "print(\"\\nPadded sequences:\")\n",
    "print(padded_sequences)\n",
    "input=np.array(padded_sequences)\n",
    "y_pred = model.predict(input)\n",
    "print(y_pred)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(y_pred_classes)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "print(y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7623260f-91ae-48b2-9d0a-1996ac577689",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Demo_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "text_columns = ['Age group','Sex','Status (Single/ partner)','Residence Zip Code']\n",
    "tokenizer.fit_on_texts(data['Age group'] + ' ' + data['Sex']+ ' ' + data['Status (Single/ partner)']+ ' ' + data['Residence Zip Code'])\n",
    "X = tokenizer.texts_to_sequences(data['Age group'] + ' ' + data['Sex']+ ' ' + data['Status (Single/ partner)']+ ' ' + data['Residence Zip Code'])\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "text_input_1 = Input(shape=(max_sequence_length,))\n",
    "text_input_2 = Input(shape=(max_sequence_length,))\n",
    "vocab_size=1000\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input_1)\n",
    "concatenated_input = Concatenate()([embedding_layer, text_input_2])\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[text_input_1, text_input_2], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, X_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_text = \"below 30,M,Single,Area Deprovation Index1\"\n",
    "single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_sequence, single_input_sequence])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd326232-1da2-4c48-857e-4b7af48246fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Demo_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "text_columns = ['Age group','Sex','Status (Single/ partner)','Residence Zip Code']\n",
    "\n",
    "tokenizer.fit_on_texts(data['Age group'] + ' ' + data['Sex']+ ' ' + data['Status (Single/ partner)']+ ' ' + data['Residence Zip Code'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Age group'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Sex'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Status (Single/ partner)'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Residence Zip Code'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test,X3_train, X3_test,X4_train, X4_test, y_train, y_test = train_test_split(X1, X2,X3, X4, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Age = Input(shape=(max_sequence_length,))\n",
    "Sex = Input(shape=(max_sequence_length,))\n",
    "Status = Input(shape=(max_sequence_length,))\n",
    "Zip = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Age)\n",
    "concatenated_input = Concatenate()([embedding_layer, Sex])\n",
    "concatenated_input = Concatenate()([concatenated_input, Status])\n",
    "concatenated_input = Concatenate()([concatenated_input, Zip])\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Age, Sex,Status,Zip], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Age = \"your single input string here for text_column_1\"\n",
    "single_input_Sex = \"your single input string here for text_column_2\"\n",
    "single_input_Status = \"your single input string here for text_column_2\"\n",
    "single_input_Zip = \"your single input string here for text_column_2\"\n",
    "\n",
    "single_input_sequence_Age = tokenizer.texts_to_sequences([single_input_Age])\n",
    "single_input_sequence_Sex = tokenizer.texts_to_sequences([single_input_Sex])\n",
    "single_input_sequence_Status = tokenizer.texts_to_sequences([single_input_Status])\n",
    "single_input_sequence_Zip = tokenizer.texts_to_sequences([single_input_Zip])\n",
    "\n",
    "single_input_sequence_Age = pad_sequences(single_input_sequence_Age, maxlen=max_sequence_length)\n",
    "single_input_sequence_Sex = pad_sequences(single_input_sequence_Sex, maxlen=max_sequence_length)\n",
    "single_input_sequence_Status = pad_sequences(single_input_sequence_Status, maxlen=max_sequence_length)\n",
    "single_input_sequence_Zip = pad_sequences(single_input_sequence_Zip, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_sequence_Age, single_input_sequence_Sex,single_input_sequence_Status,single_input_sequence_Zip])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31fd3dae-cff3-445f-a5b1-679e79331364",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Demo_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "text_columns = ['Age group','Sex','Status (Single/ partner)','Residence Zip Code']\n",
    "\n",
    "tokenizer.fit_on_texts(data['Age group'] + ' ' + data['Sex'] + ' ' + data['Status (Single/ partner)'] + ' ' + data['Residence Zip Code'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Age group'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Sex'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Status (Single/ partner)'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Residence Zip Code'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Age = Input(shape=(max_sequence_length,))\n",
    "Sex = Input(shape=(max_sequence_length,))\n",
    "Status = Input(shape=(max_sequence_length,))\n",
    "Zip = Input(shape=(max_sequence_length,))\n",
    "embedding_layer_Age = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Age)\n",
    "embedding_layer_Sex = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Sex)\n",
    "embedding_layer_Status = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Status)\n",
    "embedding_layer_Zip = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Zip)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Age, embedding_layer_Sex, embedding_layer_Status, embedding_layer_Zip])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Age, Sex, Status, Zip], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Age = \"below 30\"\n",
    "single_input_Sex = \"M\"\n",
    "single_input_Status = \"Single\"\n",
    "single_input_Zip = \"Area Deprovation Index1\"\n",
    "\n",
    "single_input_sequence_Age = tokenizer.texts_to_sequences([single_input_Age])\n",
    "single_input_sequence_Sex = tokenizer.texts_to_sequences([single_input_Sex])\n",
    "single_input_sequence_Status = tokenizer.texts_to_sequences([single_input_Status])\n",
    "single_input_sequence_Zip = tokenizer.texts_to_sequences([single_input_Zip])\n",
    "\n",
    "single_input_sequence_Age = pad_sequences(single_input_sequence_Age, maxlen=max_sequence_length)\n",
    "single_input_sequence_Sex = pad_sequences(single_input_sequence_Sex, maxlen=max_sequence_length)\n",
    "single_input_sequence_Status = pad_sequences(single_input_sequence_Status, maxlen=max_sequence_length)\n",
    "single_input_sequence_Zip = pad_sequences(single_input_sequence_Zip, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_sequence_Age, single_input_sequence_Sex, single_input_sequence_Status, single_input_sequence_Zip])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c281a6-f1cb-41ae-968c-314f7b065d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_dmeo(model, tokenizer, max_sequence_length, age, sex, status, zip_code):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{age} {sex} {status} {zip_code}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Age = tokenizer.texts_to_sequences([age])\n",
    "    single_input_sequence_Sex = tokenizer.texts_to_sequences([sex])\n",
    "    single_input_sequence_Status = tokenizer.texts_to_sequences([status])\n",
    "    single_input_sequence_Zip = tokenizer.texts_to_sequences([zip_code])\n",
    "    \n",
    "    single_input_sequence_Age = pad_sequences(single_input_sequence_Age, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Sex = pad_sequences(single_input_sequence_Sex, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Status = pad_sequences(single_input_sequence_Status, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Zip = pad_sequences(single_input_sequence_Zip, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Age, single_input_sequence_Sex, single_input_sequence_Status, single_input_sequence_Zip])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "age = \"below 30\"\n",
    "sex = \"M\"\n",
    "status = \"Single\"\n",
    "zip_code = \"Area Deprovation Index 2\"\n",
    "predicted_class = predict_class_dmeo(model, tokenizer, max_sequence_length, age, sex, status, zip_code)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d73a2ea-5536-450f-868c-1182a8608300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/rajeshwarrao/Downloads/Demo_Score.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb4c4b69-4b34-48d6-976b-e5a990afbf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/Demo_Score.h5')\n",
    "\n",
    "age = \"below 30\"\n",
    "sex = \"M\"\n",
    "status = \"Single\"\n",
    "zip_code = \"Area Deprovation Index 2\"\n",
    "\n",
    "# Now, you can use the loaded_model for predictions\n",
    "predicted_class = predict_class_dmeo(loaded_model, tokenizer, max_sequence_length, age, sex, status, zip_code)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54f79f4-f304-440b-a6f8-180da5304717",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "Fin_columns = [ 'Occupation','Own House','Own Car','FBP Level','Score_Fin']\n",
    "Fin_df=Scores_df[Fin_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "Fin_df.fillna(0, inplace=True)\n",
    "Fin_df['target']=Fin_df['Score_Fin'].astype(float)\n",
    "Fin_df['FBP Level']=Fin_df['FBP Level'].astype(str)\n",
    "Fin_df=Fin_df.drop('Score_Fin',axis=1)\n",
    "display(Fin_df)\n",
    "Fin_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe0ffd2-c9b2-4ba7-a84d-b67639b3c448",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Fin_df\n",
    "print(Fin_df.describe())\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "Fin_columns = [ 'Occupation','Own House','Own Car','FBP Level','Score_Fin']\n",
    "\n",
    "text_columns = ['Occupation','Own House','Own Car','FBP Level']\n",
    "\n",
    "tokenizer.fit_on_texts(data['Occupation'] + ' ' + data['Own House'] + ' ' + data['Own Car'] + ' ' + data['FBP Level'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Occupation'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Own House'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Own Car'])\n",
    "X4 = tokenizer.texts_to_sequences(data['FBP Level'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "print(data['target'])\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Occupation = Input(shape=(max_sequence_length,))\n",
    "Own_House = Input(shape=(max_sequence_length,))\n",
    "Own_Car = Input(shape=(max_sequence_length,))\n",
    "FBP_Level = Input(shape=(max_sequence_length,))\n",
    "embedding_layer_Occupation = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Occupation)\n",
    "embedding_layer_Own_House = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Own_House)\n",
    "embedding_layer_Own_Car = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Own_Car)\n",
    "embedding_layer_FBP_Level = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(FBP_Level)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Occupation, embedding_layer_Own_House, embedding_layer_Own_Car, embedding_layer_FBP_Level])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Occupation, Own_House, Own_Car, FBP_Level], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Occupation = \"Business\"\n",
    "single_input_Own_House = \"Y\"\n",
    "single_input_Own_Car = \"Y\"\n",
    "single_input_FBP_Level = \"120\"\n",
    "\n",
    "single_input_Occupation = tokenizer.texts_to_sequences([single_input_Occupation])\n",
    "single_input_Own_House = tokenizer.texts_to_sequences([single_input_Own_House])\n",
    "single_input_Own_Car = tokenizer.texts_to_sequences([single_input_Own_Car])\n",
    "single_input_FBP_Level = tokenizer.texts_to_sequences([single_input_FBP_Level])\n",
    "\n",
    "single_input_Occupation = pad_sequences(single_input_Occupation, maxlen=max_sequence_length)\n",
    "single_input_Own_House = pad_sequences(single_input_Own_House, maxlen=max_sequence_length)\n",
    "single_input_Own_Car = pad_sequences(single_input_Own_Car, maxlen=max_sequence_length)\n",
    "single_input_FBP_Level = pad_sequences(single_input_FBP_Level, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Occupation, single_input_Own_House, single_input_Own_Car, single_input_FBP_Level])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b1d81b4-b77a-473b-a9c7-6681352fb09c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dbb7e4-2d9e-44c0-be63-a718e3ceac97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Fin_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "Fin_columns = ['Occupation', 'Own House', 'Own Car', 'FBP Level', 'Score_Fin']\n",
    "\n",
    "tokenizer.fit_on_texts(data['Occupation'] + ' ' + data['Own House'] + ' ' + data['Own Car'] + ' ' + data['FBP Level'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Occupation'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Own House'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Own Car'])\n",
    "X4 = tokenizer.texts_to_sequences(data['FBP Level'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Occupation = Input(shape=(max_sequence_length,))\n",
    "Own_House = Input(shape=(max_sequence_length,))\n",
    "Own_Car = Input(shape=(max_sequence_length,))\n",
    "FBP_Level = Input(shape=(max_sequence_length,))\n",
    "embedding_layer_Occupation = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Occupation)\n",
    "embedding_layer_Own_House = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Own_House)\n",
    "embedding_layer_Own_Car = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Own_Car)\n",
    "embedding_layer_FBP_Level = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(FBP_Level)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Occupation, embedding_layer_Own_House, embedding_layer_Own_Car, embedding_layer_FBP_Level])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Occupation, Own_House, Own_Car, FBP_Level], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Occupation = \"Business\"\n",
    "single_input_Own_House = \"Y\"\n",
    "single_input_Own_Car = \"Y\"\n",
    "single_input_FBP_Level = \"130.0\"\n",
    "\n",
    "single_input_Occupation = tokenizer.texts_to_sequences([single_input_Occupation])\n",
    "single_input_Own_House = tokenizer.texts_to_sequences([single_input_Own_House])\n",
    "single_input_Own_Car = tokenizer.texts_to_sequences([single_input_Own_Car])\n",
    "single_input_FBP_Level = tokenizer.texts_to_sequences([single_input_FBP_Level])\n",
    "\n",
    "single_input_Occupation = pad_sequences(single_input_Occupation, maxlen=max_sequence_length)\n",
    "single_input_Own_House = pad_sequences(single_input_Own_House, maxlen=max_sequence_length)\n",
    "single_input_Own_Car = pad_sequences(single_input_Own_Car, maxlen=max_sequence_length)\n",
    "single_input_FBP_Level = pad_sequences(single_input_FBP_Level, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Occupation, single_input_Own_House, single_input_Own_Car, single_input_FBP_Level])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/Fin_Score.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01067567-2ea6-4d85-a5e8-8d38acb1670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "Fin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d44717-f9b6-4ff1-a2e5-dbeef3cc515c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_class_fin(model, tokenizer, max_sequence_length, Occupation, Own_House, Own_Car, FBP_Level):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Occupation} {Own_House} {Own_Car} {FBP_Level}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Occupation = tokenizer.texts_to_sequences([Occupation])\n",
    "    single_input_sequence_Own_House = tokenizer.texts_to_sequences([Own_House])\n",
    "    single_input_sequence_Own_Car = tokenizer.texts_to_sequences([Own_Car])\n",
    "    single_input_sequence_FBP_Level = tokenizer.texts_to_sequences([FBP_Level])\n",
    "    \n",
    "    single_input_sequence_Occupation = pad_sequences(single_input_sequence_Occupation, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Own_House = pad_sequences(single_input_sequence_Own_House, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Own_Car = pad_sequences(single_input_sequence_Own_Car, maxlen=max_sequence_length)\n",
    "    single_input_sequence_FBP_Level = pad_sequences(single_input_sequence_FBP_Level, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Occupation, single_input_sequence_Own_House, single_input_sequence_Own_Car, single_input_sequence_FBP_Level])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "Occupation = \"Business\"\n",
    "Own_House = \"Y\"\n",
    "Own_Car = \"Y\"\n",
    "FBP_Level = \"120.0\"\n",
    "predicted_class = predict_class_fin(model, tokenizer, max_sequence_length, Occupation, Own_House, Own_Car, FBP_Level)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d34d8b2-d82e-485a-84d2-6b74c3dd3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/Fin_Score.h5')\n",
    "\n",
    "Occupation = \"Business\"\n",
    "Own_House = \"Y\"\n",
    "Own_Car = \"Y\"\n",
    "FBP_Level = \"120.0\"\n",
    "predicted_class = predict_class_fin(model, tokenizer, max_sequence_length, Occupation, Own_House, Own_Car, FBP_Level)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1d0a7d8-c268-439a-a6d2-b1c066d7e98d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "MH_columns = [ 'Family History','Annual Wellness vists','Known conditions','# of Provider visist YTD','Score_Medical_History']\n",
    "MH_df=Scores_df[MH_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "MH_df.fillna(0, inplace=True)\n",
    "MH_df['target']=MH_df['Score_Medical_History'].astype(float)\n",
    "MH_df['# of Provider visist YTD']=MH_df['# of Provider visist YTD'].astype(str)\n",
    "MH_df['Family History']=MH_df['Family History'].astype(str)\n",
    "MH_df['Annual Wellness vists']=MH_df['Annual Wellness vists'].astype(str)\n",
    "MH_df['Known conditions']=MH_df['Known conditions'].astype(str)\n",
    "\n",
    "MH_df=MH_df.drop('Score_Medical_History',axis=1)\n",
    "display(MH_df)\n",
    "MH_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = MH_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "MH_columns = [ 'Family History','Annual Wellness vists','Known conditions','# of Provider visist YTD','Score_Medical_History']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['Family History'] + ' ' + data['Annual Wellness vists'] + ' ' + data['Known conditions'] + ' ' + data['# of Provider visist YTD'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Family History'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Annual Wellness vists'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Known conditions'])\n",
    "X4 = tokenizer.texts_to_sequences(data['# of Provider visist YTD'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Family_History = Input(shape=(max_sequence_length,))\n",
    "Annual_Wellness_vists = Input(shape=(max_sequence_length,))\n",
    "Known_conditions = Input(shape=(max_sequence_length,))\n",
    "Provider_visist_YTD = Input(shape=(max_sequence_length,))\n",
    "embedding_layer_Family_History = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Family_History)\n",
    "embedding_layer_Annual_Wellness_vists = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Annual_Wellness_vists)\n",
    "embedding_layer_Known_conditions = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Known_conditions)\n",
    "embedding_layer_Provider_visist_YTD = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Provider_visist_YTD)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Family_History, embedding_layer_Annual_Wellness_vists, embedding_layer_Known_conditions, embedding_layer_Provider_visist_YTD])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Family_History, Annual_Wellness_vists, Known_conditions, Provider_visist_YTD], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Family_History = \"Diabetic\"\n",
    "single_input_Annual_Wellness_vists = \"Y\"\n",
    "single_input_Known_conditions = \"Diabetic\"\n",
    "single_input_Provider_visist_YTD = \"0.0\"\n",
    "\n",
    "single_input_Family_History = tokenizer.texts_to_sequences([single_input_Family_History])\n",
    "single_input_Annual_Wellness_vists = tokenizer.texts_to_sequences([single_input_Annual_Wellness_vists])\n",
    "single_input_Known_conditions = tokenizer.texts_to_sequences([single_input_Known_conditions])\n",
    "single_input_Provider_visist_YTD = tokenizer.texts_to_sequences([single_input_Provider_visist_YTD])\n",
    "\n",
    "single_input_Family_History = pad_sequences(single_input_Family_History, maxlen=max_sequence_length)\n",
    "single_input_Annual_Wellness_vists = pad_sequences(single_input_Annual_Wellness_vists, maxlen=max_sequence_length)\n",
    "single_input_Known_conditions = pad_sequences(single_input_Known_conditions, maxlen=max_sequence_length)\n",
    "single_input_Provider_visist_YTD = pad_sequences(single_input_Provider_visist_YTD, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Family_History, single_input_Annual_Wellness_vists, single_input_Known_conditions, single_input_Provider_visist_YTD])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/MH_Score.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba02b2b-59a7-468e-8b3b-96be82c3f2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_class_mh(model, tokenizer, max_sequence_length, Family_History, Annual_Wellness_vists, Known_conditions, Provider_visist_YTD):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Family_History} {Annual_Wellness_vists} {Known_conditions} {Provider_visist_YTD}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Family_History = tokenizer.texts_to_sequences([Family_History])\n",
    "    single_input_sequence_Annual_Wellness_vists = tokenizer.texts_to_sequences([Annual_Wellness_vists])\n",
    "    single_input_sequence_Known_conditions = tokenizer.texts_to_sequences([Known_conditions])\n",
    "    single_input_sequence_Provider_visist_YTD = tokenizer.texts_to_sequences([Provider_visist_YTD])\n",
    "    \n",
    "    single_input_sequence_Family_History = pad_sequences(single_input_sequence_Family_History, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Annual_Wellness_vists = pad_sequences(single_input_sequence_Annual_Wellness_vists, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Known_conditions = pad_sequences(single_input_sequence_Known_conditions, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Provider_visist_YTD = pad_sequences(single_input_sequence_Provider_visist_YTD, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Family_History, single_input_sequence_Annual_Wellness_vists, single_input_sequence_Known_conditions, single_input_sequence_Provider_visist_YTD])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "Family_History = \"Diabetic\"\n",
    "Annual_Wellness_vists = \"Y\"\n",
    "Known_conditions = \"Diabetic\"\n",
    "Provider_visist_YTD = \"0.0\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_mh(model, tokenizer, max_sequence_length,Family_History, Annual_Wellness_vists, Known_conditions, Provider_visist_YTD)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b78dfbf7-594e-49b1-940e-0f45dfa4d0bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/MH_Score.h5')\n",
    "\n",
    "Family_History = \"Diabetic\"\n",
    "Annual_Wellness_vists = \"Y\"\n",
    "Known_conditions = \"Diabetic\"\n",
    "Provider_visist_YTD = \"0.0\"\n",
    "predicted_class = predict_class_mh(model, tokenizer, max_sequence_length, Family_History, Annual_Wellness_vists, Known_conditions, Provider_visist_YTD)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4685a26-19c1-48a8-bf4a-7eeb92d908ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "MedH_columns = [ 'Past medications','Last refill date','Refill due date','Formulary visist YTD','Days of Therapy missed','Score_Medication_History']\n",
    "MedH_df=Scores_df[MedH_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "MedH_df.fillna(0, inplace=True)\n",
    "MedH_df['target']=MedH_df['Score_Medication_History'].astype(float)\n",
    "MedH_df['Last refill date']=MedH_df['Last refill date'].astype(str)\n",
    "MedH_df['Refill due date']=MedH_df['Refill due date'].astype(str)\n",
    "MedH_df['Formulary visist YTD']=MedH_df['Formulary visist YTD'].astype(str)\n",
    "MedH_df['Days of Therapy missed']=MedH_df['Days of Therapy missed'].astype(str)\n",
    "\n",
    "MedH_df=MedH_df.drop('Score_Medication_History',axis=1)\n",
    "display(MedH_df)\n",
    "MedH_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = MedH_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "MedH_columns = [ 'Past medications','Last refill date','Refill due date','Formulary visist YTD','Days of Therapy missed','Score_Medication_History']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['Past medications'] + ' ' + data['Last refill date'] + ' ' + data['Refill due date'] + ' ' + data['Formulary visist YTD']+ ' ' + data['Days of Therapy missed'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Past medications'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Last refill date'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Refill due date'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Formulary visist YTD'])\n",
    "X5 = tokenizer.texts_to_sequences(data['Days of Therapy missed'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "X5 = pad_sequences(X5, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4,X5, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Past_medications = Input(shape=(max_sequence_length,))\n",
    "Last_refill_date = Input(shape=(max_sequence_length,))\n",
    "Refill_due_date = Input(shape=(max_sequence_length,))\n",
    "Formulary_visist_YTD = Input(shape=(max_sequence_length,))\n",
    "Days_Therapy_missed = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embedding_layer_Past_medications = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Past_medications)\n",
    "embedding_layer_Last_refill_date = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Last_refill_date)\n",
    "embedding_layer_Refill_due_date = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Refill_due_date)\n",
    "embedding_layer_Formulary_visist_YTD = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Formulary_visist_YTD)\n",
    "embedding_layer_Days_Therapy_missed = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Days_Therapy_missed)\n",
    "\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Past_medications, embedding_layer_Last_refill_date, embedding_layer_Refill_due_date, embedding_layer_Formulary_visist_YTD,embedding_layer_Days_Therapy_missed])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Past_medications, Last_refill_date, Refill_due_date, Formulary_visist_YTD,Days_Therapy_missed], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train, X5_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Past_medications = \"Y\"\n",
    "single_input_Last_refill_date = \"2023-02-02\"\n",
    "single_input_Refill_due_date = \"2023-02-09\"\n",
    "single_input_Formulary_visist_YTD = \"0.0\"\n",
    "single_input_Days_Therapy_missed = \"0.0\"\n",
    "\n",
    "\n",
    "\n",
    "single_input_Past_medications = tokenizer.texts_to_sequences([single_input_Past_medications])\n",
    "single_input_Last_refill_date = tokenizer.texts_to_sequences([single_input_Last_refill_date])\n",
    "single_input_Refill_due_date = tokenizer.texts_to_sequences([single_input_Refill_due_date])\n",
    "single_input_Formulary_visist_YTD = tokenizer.texts_to_sequences([single_input_Formulary_visist_YTD])\n",
    "single_input_Days_Therapy_missed = tokenizer.texts_to_sequences([single_input_Days_Therapy_missed])\n",
    "\n",
    "\n",
    "\n",
    "single_input_Past_medications = pad_sequences(single_input_Family_History, maxlen=max_sequence_length)\n",
    "single_input_Last_refill_date = pad_sequences(single_input_Last_refill_date, maxlen=max_sequence_length)\n",
    "single_input_Refill_due_date = pad_sequences(single_input_Refill_due_date, maxlen=max_sequence_length)\n",
    "single_input_Formulary_visist_YTD = pad_sequences(single_input_Formulary_visist_YTD, maxlen=max_sequence_length)\n",
    "single_input_Days_Therapy_missed = pad_sequences(single_input_Days_Therapy_missed, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Past_medications, single_input_Last_refill_date, single_input_Refill_due_date, single_input_Formulary_visist_YTD,single_input_Days_Therapy_missed])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/MedH_Score.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7380ac22-c6ed-4713-baa1-d0f0680c30e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_class_medh(model, tokenizer, max_sequence_length, Past_medications, Last_refill_date, Refill_due_date, Formulary_visist_YTD,Days_Therapy_missed):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Past_medications} {Last_refill_date} {Refill_due_date} {Formulary_visist_YTD} {Days_Therapy_missed}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Past_medications = tokenizer.texts_to_sequences([Past_medications])\n",
    "    single_input_sequence_Last_refill_date = tokenizer.texts_to_sequences([Last_refill_date])\n",
    "    single_input_sequence_Refill_due_date = tokenizer.texts_to_sequences([Refill_due_date])\n",
    "    single_input_sequence_Formulary_visist_YTD = tokenizer.texts_to_sequences([Formulary_visist_YTD])\n",
    "    single_input_sequence_Days_Therapy_missed = tokenizer.texts_to_sequences([Days_Therapy_missed])\n",
    "    \n",
    "    single_input_sequence_Past_medications = pad_sequences(single_input_sequence_Past_medications, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Last_refill_date = pad_sequences(single_input_sequence_Last_refill_date, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Refill_due_date = pad_sequences(single_input_sequence_Refill_due_date, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Formulary_visist_YTD = pad_sequences(single_input_sequence_Formulary_visist_YTD, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Days_Therapy_missed = pad_sequences(single_input_sequence_Days_Therapy_missed, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Past_medications, single_input_sequence_Last_refill_date, single_input_sequence_Refill_due_date, single_input_sequence_Formulary_visist_YTD,single_input_sequence_Days_Therapy_missed])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "Past_medications = \"Y\"\n",
    "Last_refill_date = \"2023-02-02\"\n",
    "Refill_due_date = \"2023-02-09\"\n",
    "Formulary_visist_YTD = \"0.0\"\n",
    "Days_Therapy_missed = \"0.0\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_medh(model, tokenizer, max_sequence_length,Past_medications, Last_refill_date, Refill_due_date, Formulary_visist_YTD,Days_Therapy_missed)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cb0a3f-fbcc-4c57-853d-e3315b09cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/MedH_Score.h5')\n",
    "\n",
    "Past_medications = \"Y\"\n",
    "Last_refill_date = \"2023-02-02\"\n",
    "Refill_due_date = \"2023-02-09\"\n",
    "Formulary_visist_YTD = \"0.0\"\n",
    "Days_Therapy_missed = \"0\"\n",
    "\n",
    "predicted_class = predict_class_medh(model, tokenizer, max_sequence_length, Past_medications, Last_refill_date, Refill_due_date, Formulary_visist_YTD,Days_Therapy_missed)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae5e43c-b5fd-492a-8de3-6bcba78f279d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "PS_columns = [ 'able to drive','can walk long distance','Dental','Visison','Other disability','Score_Physical_Status']\n",
    "PS_df=Scores_df[PS_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "PS_df.fillna(0, inplace=True)\n",
    "PS_df['target']=PS_df['Score_Physical_Status'].astype(float)\n",
    "PS_df['able to drive']=PS_df['able to drive'].astype(str)\n",
    "PS_df['can walk long distance']=PS_df['can walk long distance'].astype(str)\n",
    "PS_df['Dental']=PS_df['Dental'].astype(str)\n",
    "PS_df['Visison']=PS_df['Visison'].astype(str)\n",
    "PS_df['Other disability']=PS_df['Other disability'].astype(str)\n",
    "\n",
    "PS_df=PS_df.drop('Score_Physical_Status',axis=1)\n",
    "display(PS_df)\n",
    "PS_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = PS_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "PS_columns = [ 'able to drive','can walk long distance','Dental','Visison','Other disability','Score_Physical_Status']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['able to drive'] + ' ' + data['can walk long distance'] + ' ' + data['Dental'] + ' ' + data['Visison']+ ' ' + data['Other disability'])\n",
    "X1 = tokenizer.texts_to_sequences(data['able to drive'])\n",
    "X2 = tokenizer.texts_to_sequences(data['can walk long distance'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Dental'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Visison'])\n",
    "X5 = tokenizer.texts_to_sequences(data['Other disability'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "X5 = pad_sequences(X5, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4,X5, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "able_to_drive = Input(shape=(max_sequence_length,))\n",
    "can_walk_long_distance = Input(shape=(max_sequence_length,))\n",
    "Dental = Input(shape=(max_sequence_length,))\n",
    "Visison = Input(shape=(max_sequence_length,))\n",
    "Other_disability = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embedding_layer_able_to_drive = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(able_to_drive)\n",
    "embedding_layer_can_walk_long_distance = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(can_walk_long_distance)\n",
    "embedding_layer_Dental = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Dental)\n",
    "embedding_layer_Visison = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Visison)\n",
    "embedding_layer_Other_disability = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Other_disability)\n",
    "\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_able_to_drive, embedding_layer_can_walk_long_distance, embedding_layer_Dental, embedding_layer_Visison,embedding_layer_Other_disability])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[able_to_drive, can_walk_long_distance, Dental, Visison,Other_disability], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train, X5_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_able_to_drive = \"Y\"\n",
    "single_input_Last_can_walk_long_distance = \"Y\"\n",
    "single_input_Dental = \"N\"\n",
    "single_input_Visison = \"N\"\n",
    "single_input_Other_disability = \"N\"\n",
    "\n",
    "\n",
    "\n",
    "single_input_able_to_drive = tokenizer.texts_to_sequences([single_input_able_to_drive])\n",
    "single_input_Last_can_walk_long_distance = tokenizer.texts_to_sequences([single_input_Last_can_walk_long_distance])\n",
    "single_input_Dental = tokenizer.texts_to_sequences([single_input_Dental])\n",
    "single_input_Visison = tokenizer.texts_to_sequences([single_input_Visison])\n",
    "single_input_Other_disability = tokenizer.texts_to_sequences([single_input_Other_disability])\n",
    "\n",
    "\n",
    "\n",
    "single_input_able_to_drive = pad_sequences(single_input_able_to_drive, maxlen=max_sequence_length)\n",
    "single_input_Last_can_walk_long_distance = pad_sequences(single_input_Last_can_walk_long_distance, maxlen=max_sequence_length)\n",
    "single_input_Dental = pad_sequences(single_input_Dental, maxlen=max_sequence_length)\n",
    "single_input_Visison = pad_sequences(single_input_Visison, maxlen=max_sequence_length)\n",
    "single_input_Other_disability = pad_sequences(single_input_Other_disability, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_able_to_drive, single_input_Last_can_walk_long_distance, single_input_Dental, single_input_Visison,single_input_Other_disability])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/PS_Score.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c473b33a-c78e-4209-b87c-4a730bf3f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_class_ps(model, tokenizer, max_sequence_length, able_to_drive, can_walk_long_distance, Dental, Visison,Other_disability):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{able_to_drive} {can_walk_long_distance} {Dental} {Visison} {Other_disability}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_able_to_drive = tokenizer.texts_to_sequences([able_to_drive])\n",
    "    single_input_sequence_can_walk_long_distance = tokenizer.texts_to_sequences([can_walk_long_distance])\n",
    "    single_input_sequence_Dental = tokenizer.texts_to_sequences([Dental])\n",
    "    single_input_sequence_Visison = tokenizer.texts_to_sequences([Visison])\n",
    "    single_input_sequence_Other_disability = tokenizer.texts_to_sequences([Other_disability])\n",
    "    \n",
    "    single_input_sequence_able_to_drive = pad_sequences(single_input_sequence_able_to_drive, maxlen=max_sequence_length)\n",
    "    single_input_sequence_can_walk_long_distance = pad_sequences(single_input_sequence_can_walk_long_distance, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Dental = pad_sequences(single_input_sequence_Dental, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Visison = pad_sequences(single_input_sequence_Visison, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Other_disability = pad_sequences(single_input_sequence_Other_disability, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_able_to_drive, single_input_sequence_can_walk_long_distance, single_input_sequence_Dental, single_input_sequence_Visison,single_input_sequence_Other_disability])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "able_to_drive = \"N\"\n",
    "can_walk_long_distance = \"N\"\n",
    "Dental = \"N\"\n",
    "Visison = \"Y\"\n",
    "Other_disability = \"N\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_ps(model, tokenizer, max_sequence_length,able_to_drive, can_walk_long_distance, Dental, Visison,Other_disability)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d90f57f7-2795-4b89-9f41-fce27c1ba674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/PS_Score.h5')\n",
    "\n",
    "able_to_drive = \"N\"\n",
    "can_walk_long_distance = \"N\"\n",
    "Dental = \"N\"\n",
    "Visison = \"Y\"\n",
    "Other_disability = \"N\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_ps(loaded_model, tokenizer, max_sequence_length,able_to_drive, can_walk_long_distance, Dental, Visison,Other_disability)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02e2af4e-b5cc-43e4-aaf0-cb25dcba50b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "LS_columns = [ 'Workouts( Walking/ Exercise)','Alcoholic','Smoking','Professional hazards','Drug abuse','Other factors','Score_Lifestyle']\n",
    "LS_df=Scores_df[LS_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "LS_df.fillna(0, inplace=True)\n",
    "LS_df['target']=LS_df['Score_Lifestyle'].astype(float)\n",
    "LS_df['Workouts( Walking/ Exercise)']=LS_df['Workouts( Walking/ Exercise)'].astype(str)\n",
    "LS_df['Alcoholic']=LS_df['Alcoholic'].astype(str)\n",
    "LS_df['Smoking']=LS_df['Smoking'].astype(str)\n",
    "LS_df['Professional hazards']=LS_df['Professional hazards'].astype(str)\n",
    "LS_df['Drug abuse']=LS_df['Drug abuse'].astype(str)\n",
    "LS_df['Other factors']=LS_df['Other factors'].astype(str)\n",
    "\n",
    "LS_df=LS_df.drop('Score_Lifestyle',axis=1)\n",
    "display(LS_df)\n",
    "LS_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = LS_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "LS_columns = [ 'Workouts( Walking/ Exercise)','Alcoholic','Smoking','Professional hazards','Drug abuse','Other factors','Score_Lifestyle']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['Workouts( Walking/ Exercise)'] + ' ' + data['Alcoholic'] + ' ' + data['Smoking'] + ' ' + data['Professional hazards']+ ' ' + data['Drug abuse']+ ' ' + data['Other factors'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Workouts( Walking/ Exercise)'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Alcoholic'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Smoking'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Professional hazards'])\n",
    "X5 = tokenizer.texts_to_sequences(data['Drug abuse'])\n",
    "X6 = tokenizer.texts_to_sequences(data['Other factors'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "X5 = pad_sequences(X5, maxlen=max_sequence_length)\n",
    "X6 = pad_sequences(X6, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test,X6_train, X6_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4,X5,X6, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Workouts = Input(shape=(max_sequence_length,))\n",
    "Alcoholic = Input(shape=(max_sequence_length,))\n",
    "Smoking = Input(shape=(max_sequence_length,))\n",
    "Professional_hazards = Input(shape=(max_sequence_length,))\n",
    "Drug_abuse = Input(shape=(max_sequence_length,))\n",
    "Other_factors = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embedding_layer_Workouts = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Workouts)\n",
    "embedding_layer_Alcoholic = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Alcoholic)\n",
    "embedding_layer_Smoking = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Smoking)\n",
    "embedding_layer_Professional_hazards = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Professional_hazards)\n",
    "embedding_layer_Drug_abuse = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Drug_abuse)\n",
    "embedding_layer_Other_factors = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Other_factors)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Workouts, embedding_layer_Alcoholic, embedding_layer_Smoking, embedding_layer_Professional_hazards,embedding_layer_Drug_abuse,embedding_layer_Other_factors])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Workouts, Alcoholic, Smoking, Professional_hazards,Drug_abuse,Other_factors], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train, X5_train, X6_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Workouts = \"Y\"\n",
    "single_input_Alcoholic = \"N\"\n",
    "single_input_Smoking = \"N\"\n",
    "single_input_Professional_hazards = \"N\"\n",
    "single_input_Drug_abuse = \"N\"\n",
    "single_input_Other_factors = \"N\"\n",
    "\n",
    "\n",
    "single_input_Workouts = tokenizer.texts_to_sequences([single_input_Workouts])\n",
    "single_input_Alcoholic = tokenizer.texts_to_sequences([single_input_Alcoholic])\n",
    "single_input_Smoking = tokenizer.texts_to_sequences([single_input_Smoking])\n",
    "single_input_Professional_hazards = tokenizer.texts_to_sequences([single_input_Professional_hazards])\n",
    "single_input_Drug_abuse = tokenizer.texts_to_sequences([single_input_Drug_abuse])\n",
    "single_input_Other_factors = tokenizer.texts_to_sequences([single_input_Other_factors])\n",
    "\n",
    "\n",
    "single_input_Workouts = pad_sequences(single_input_Workouts, maxlen=max_sequence_length)\n",
    "single_input_Alcoholic = pad_sequences(single_input_Alcoholic, maxlen=max_sequence_length)\n",
    "single_input_Smoking = pad_sequences(single_input_Smoking, maxlen=max_sequence_length)\n",
    "single_input_Professional_hazards = pad_sequences(single_input_Professional_hazards, maxlen=max_sequence_length)\n",
    "single_input_Drug_abuse = pad_sequences(single_input_Drug_abuse, maxlen=max_sequence_length)\n",
    "single_input_Other_factors = pad_sequences(single_input_Other_factors, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Workouts, single_input_Alcoholic, single_input_Smoking, single_input_Professional_hazards,single_input_Drug_abuse,single_input_Other_factors])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/LS_Score.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c9e1a-aaf4-4e6b-9aad-2df3c1feaf58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_ls(model, tokenizer, max_sequence_length, Workouts, Alcoholic, Smoking, Professional_hazards,Drug_abuse,Other_factors):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Workouts} {Alcoholic} {Smoking} {Professional_hazards} {Drug_abuse} {Other_factors}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Workouts = tokenizer.texts_to_sequences([Workouts])\n",
    "    single_input_sequence_Alcoholic = tokenizer.texts_to_sequences([Alcoholic])\n",
    "    single_input_sequence_Smoking = tokenizer.texts_to_sequences([Smoking])\n",
    "    single_input_sequence_Professional_hazards = tokenizer.texts_to_sequences([Professional_hazards])\n",
    "    single_input_sequence_Drug_abuse = tokenizer.texts_to_sequences([Drug_abuse])\n",
    "    single_input_sequence_Other_factors = tokenizer.texts_to_sequences([Other_factors])\n",
    "    \n",
    "    single_input_sequence_Workouts = pad_sequences(single_input_sequence_Workouts, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Alcoholic = pad_sequences(single_input_sequence_Alcoholic, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Smoking = pad_sequences(single_input_sequence_Smoking, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Professional_hazards = pad_sequences(single_input_sequence_Professional_hazards, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Drug_abuse = pad_sequences(single_input_sequence_Drug_abuse, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Other_factors = pad_sequences(single_input_sequence_Other_factors, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Workouts, single_input_sequence_Alcoholic, single_input_sequence_Smoking, single_input_sequence_Professional_hazards,single_input_sequence_Drug_abuse,single_input_sequence_Other_factors])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "Workouts = \"Y\"\n",
    "Alcoholic = \"N\"\n",
    "Smoking = \"N\"\n",
    "Professional_hazards = \"N\"\n",
    "Drug_abuse = \"N\"\n",
    "Other_factors = \"N\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_ls(model, tokenizer, max_sequence_length,Workouts, Alcoholic, Smoking, Professional_hazards,Drug_abuse,Other_factors)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a266bb-1847-4d5b-935c-2a877776fea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/LS_Score.h5')\n",
    "\n",
    "Workouts = \"Y\"\n",
    "Alcoholic = \"N\"\n",
    "Smoking = \"N\"\n",
    "Professional_hazards = \"N\"\n",
    "Drug_abuse = \"N\"\n",
    "Other_factors = \"N\"\n",
    "\n",
    "predicted_class = predict_class_ls(loaded_model, tokenizer, max_sequence_length, Workouts, Alcoholic, Smoking, Professional_hazards,Drug_abuse,Other_factors)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4544a5-3537-4b80-a934-ee738f6bf276",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "SF_columns = [ 'Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities','Score_Social_Factors']\n",
    "SF_df=Scores_df[SF_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "SF_df.fillna(0, inplace=True)\n",
    "SF_df['target']=SF_df['Score_Social_Factors'].astype(float)\n",
    "SF_df['Awareness level']=SF_df['Awareness level'].astype(str)\n",
    "SF_df['Social media activity']=SF_df['Social media activity'].astype(str)\n",
    "SF_df['Social media influence']=SF_df['Social media influence'].astype(str)\n",
    "SF_df['Peer group influence']=SF_df['Peer group influence'].astype(str)\n",
    "SF_df['Other social activities']=SF_df['Other social activities'].astype(str)\n",
    "\n",
    "SF_df=SF_df.drop('Score_Social_Factors',axis=1)\n",
    "display(SF_df)\n",
    "SF_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = SF_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "SF_columns = [ 'Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities','Score_Social_Factors']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['Awareness level'] + ' ' + data['Social media activity'] + ' ' + data['Social media influence'] + ' ' + data['Peer group influence']+ ' ' + data['Other social activities'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Awareness level'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Social media activity'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Social media influence'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Peer group influence'])\n",
    "X5 = tokenizer.texts_to_sequences(data['Other social activities'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "X5 = pad_sequences(X5, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test,X6_train, X6_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4,X5,X6, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Awareness_level = Input(shape=(max_sequence_length,))\n",
    "Social_media_activity = Input(shape=(max_sequence_length,))\n",
    "Social_media_influence = Input(shape=(max_sequence_length,))\n",
    "Peer_group_influence = Input(shape=(max_sequence_length,))\n",
    "Other_social_activities = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embedding_layer_Awareness_level = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Awareness_level)\n",
    "embedding_layer_Social_media_activity = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Social_media_activity)\n",
    "embedding_layer_Social_media_influence = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Social_media_influence)\n",
    "embedding_layer_Peer_group_influence = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Peer_group_influence)\n",
    "embedding_layer_Other_social_activities = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Other_social_activities)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Awareness_level, embedding_layer_Social_media_activity, embedding_layer_Social_media_influence, embedding_layer_Peer_group_influence,embedding_layer_Other_social_activities])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Awareness_level, Social_media_activity, Social_media_influence, Peer_group_influence,Other_social_activities], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train, X5_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Awareness_level = \"High\"\n",
    "single_input_Social_media_activity = \"High\"\n",
    "single_input_Social_media_influence = \"N\"\n",
    "single_input_Peer_group_influence = \"Low\"\n",
    "single_input_Other_social_activities = \"Low\"\n",
    "\n",
    "\n",
    "single_input_Awareness_level = tokenizer.texts_to_sequences([single_input_Awareness_level])\n",
    "single_input_Social_media_activity = tokenizer.texts_to_sequences([single_input_Social_media_activity])\n",
    "single_input_Social_media_influence = tokenizer.texts_to_sequences([single_input_Social_media_influence])\n",
    "single_input_Peer_group_influence = tokenizer.texts_to_sequences([single_input_Peer_group_influence])\n",
    "single_input_Other_social_activities = tokenizer.texts_to_sequences([single_input_Other_social_activities])\n",
    "\n",
    "\n",
    "single_input_Awareness_level = pad_sequences(single_input_Awareness_level, maxlen=max_sequence_length)\n",
    "single_input_Social_media_activity = pad_sequences(single_input_Social_media_activity, maxlen=max_sequence_length)\n",
    "single_input_Social_media_influence = pad_sequences(single_input_Social_media_influence, maxlen=max_sequence_length)\n",
    "single_input_Peer_group_influence = pad_sequences(single_input_Peer_group_influence, maxlen=max_sequence_length)\n",
    "single_input_Other_social_activities = pad_sequences(single_input_Other_social_activities, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Awareness_level, single_input_Social_media_activity, single_input_Social_media_influence, single_input_Peer_group_influence,single_input_Other_social_activities])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/SF_Score.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba8316f6-afd6-4328-a026-5e66503d40a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class_sf(model, tokenizer, max_sequence_length, Awareness_level, Social_media_activity, Social_media_influence, Peer_group_influence,Other_social_activities):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Awareness_level} {Social_media_activity} {Social_media_influence} {Peer_group_influence} {Other_social_activities}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Awareness_level = tokenizer.texts_to_sequences([Awareness_level])\n",
    "    single_input_sequence_Social_media_activity = tokenizer.texts_to_sequences([Social_media_activity])\n",
    "    single_input_sequence_Social_media_influence = tokenizer.texts_to_sequences([Social_media_influence])\n",
    "    single_input_sequence_Peer_group_influence = tokenizer.texts_to_sequences([Peer_group_influence])\n",
    "    single_input_sequence_Other_social_activities = tokenizer.texts_to_sequences([Other_social_activities])\n",
    "    \n",
    "    single_input_sequence_Awareness_level = pad_sequences(single_input_sequence_Awareness_level, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Social_media_activity = pad_sequences(single_input_sequence_Social_media_activity, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Social_media_influence = pad_sequences(single_input_sequence_Social_media_influence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Peer_group_influence = pad_sequences(single_input_sequence_Peer_group_influence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Other_social_activities = pad_sequences(single_input_sequence_Other_social_activities, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Awareness_level, single_input_sequence_Social_media_activity, single_input_sequence_Social_media_influence, single_input_sequence_Peer_group_influence,single_input_sequence_Other_social_activities])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "Awareness_level = \"High\"\n",
    "Social_media_activity = \"High\"\n",
    "Social_media_influence = \"N\"\n",
    "Peer_group_influence = \"Low\"\n",
    "Other_social_activities = \"Low\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_sf(model, tokenizer, max_sequence_length,Awareness_level, Social_media_activity, Social_media_influence, Peer_group_influence,Other_social_activities)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090b8834-2bfd-4755-b581-3572ab9f77ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/SF_Score.h5')\n",
    "\n",
    "Awareness_level = \"Low\"\n",
    "Social_media_activity = \"Low\"\n",
    "Social_media_influence = \"N\"\n",
    "Peer_group_influence = \"High\"\n",
    "Other_social_activities = \"Low\"\n",
    "\n",
    "predicted_class = predict_class_sf(loaded_model, tokenizer, max_sequence_length, Awareness_level, Social_media_activity, Social_media_influence, Peer_group_influence,Other_social_activities)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8f43c05-1fb2-429e-8c44-bd025a940d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341d1f1-e9ca-42c9-9946-9fbd1e040432",
   "metadata": {},
   "outputs": [],
   "source": [
    "SF_columns = [ 'Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities','Score_Social_Factors']\n",
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle','Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores1_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9266fe3b-5002-42b4-9238-690220e3b271",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_Scores1_df = Scores1_df.sort_values(by='Score_Demo', ascending=False)\n",
    "\n",
    "# Calculate the index for selecting the top 10%\n",
    "top_percentage = 0.10\n",
    "num_rows = len(sorted_Scores1_df)\n",
    "top_index = int(top_percentage * num_rows)\n",
    "\n",
    "# Select the top rows\n",
    "top_10_Demo_df = sorted_Scores1_df.head(top_index)\n",
    "display(top_10_Demo_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47184d29-a2d7-4eac-aa62-c217c847ad24",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_Scores1_df = Scores1_df.sort_values(by='Score_Fin', ascending=False)\n",
    "\n",
    "# Calculate the index for selecting the top 10%\n",
    "top_percentage = 0.10\n",
    "num_rows = len(sorted_Scores1_df)\n",
    "top_index = int(top_percentage * num_rows)\n",
    "\n",
    "# Select the top rows\n",
    "top_10_Fin_df = sorted_Scores1_df.head(top_index)\n",
    "display(top_10_Fin_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad6e8bc8-c49a-4e93-aadc-343be8e2ef66",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle','Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities']\n",
    "\n",
    "sorted_Scores1_df = Scores1_df.sort_values(by='Score_Medical_History', ascending=False)\n",
    "\n",
    "# Calculate the index for selecting the top 10%\n",
    "top_percentage = 0.10\n",
    "num_rows = len(sorted_Scores1_df)\n",
    "top_index = int(top_percentage * num_rows)\n",
    "\n",
    "# Select the top rows\n",
    "top_10_MH_df = sorted_Scores1_df.head(top_index)\n",
    "display(top_10_MH_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4703d8de-ea5a-49c7-a774-c973ff1bff7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle','Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities']\n",
    "\n",
    "sorted_Scores1_df = Scores1_df.sort_values(by='Score_Medication_History', ascending=False)\n",
    "\n",
    "# Calculate the index for selecting the top 10%\n",
    "top_percentage = 0.10\n",
    "num_rows = len(sorted_Scores1_df)\n",
    "top_index = int(top_percentage * num_rows)\n",
    "\n",
    "# Select the top rows\n",
    "top_10_MedH_df = sorted_Scores1_df.head(top_index)\n",
    "display(top_10_MedH_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3742a8af-1687-4a3a-9f79-714abc250097",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle','Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities']\n",
    "\n",
    "sorted_Scores1_df = Scores1_df.sort_values(by='Score_Physical_Status', ascending=False)\n",
    "\n",
    "# Calculate the index for selecting the top 10%\n",
    "top_percentage = 0.10\n",
    "num_rows = len(sorted_Scores1_df)\n",
    "top_index = int(top_percentage * num_rows)\n",
    "\n",
    "# Select the top rows\n",
    "top_10_PS_df = sorted_Scores1_df.head(top_index)\n",
    "display(top_10_PS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66daf3e8-c6e1-45b1-b42b-57aac002d0dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle','Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities']\n",
    "\n",
    "sorted_Scores1_df = Scores1_df.sort_values(by='Score_Lifestyle', ascending=False)\n",
    "\n",
    "# Calculate the index for selecting the top 10%\n",
    "top_percentage = 0.10\n",
    "num_rows = len(sorted_Scores1_df)\n",
    "top_index = int(top_percentage * num_rows)\n",
    "\n",
    "# Select the top rows\n",
    "top_10_LS_df = sorted_Scores1_df.head(top_index)\n",
    "display(top_10_LS_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01c14867-51e0-4ecb-b93c-6e242963a6fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "suffixes = ('_Demo','_Fin', '_MH', '_MedH', '_PS', '_LS')\n",
    "common_in_10_df = pd.merge(top_10_Demo_df, top_10_Fin_df, on='S.N', how='inner',suffixes=suffixes[:2])\n",
    "display(common_in_10_df)\n",
    "common_in_10_df = pd.merge(common_in_10_df, top_10_MH_df, on='S.N', how='inner', suffixes=(suffixes[2], '_MH'))\n",
    "display(common_in_10_df)\n",
    "common_in_10_df = pd.merge(common_in_10_df, top_10_MedH_df, on='S.N', how='inner', suffixes=(suffixes[3], '_MedH'))\n",
    "common_in_10_df = pd.merge(common_in_10_df, top_10_PS_df, on='S.N', how='inner', suffixes=(suffixes[4], '_PS'))\n",
    "common_in_10_df = pd.merge(common_in_10_df, top_10_LS_df, on='S.N', how='inner', suffixes=(suffixes[5], '_LS'))\n",
    "common_in_10_df.describe()\n",
    "common_in_10_df.head()\n",
    "common_in_10_df.count()\n",
    "All_top_10_df = pd.concat([top_10_Demo_df, top_10_Fin_df, top_10_MH_df, top_10_MedH_df, top_10_PS_df,top_10_LS_df], ignore_index=True)\n",
    "\n",
    "display(All_top_10_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed5869-75b2-4677-91d7-1d8a54cc63e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_top_10_AL=All_top_10_df['Awareness level'] == \"High\"\n",
    "All_top_10_SMA=All_top_10_df['Social media activity']=='High'  \n",
    "All_top_10_SMI=All_top_10_df['Social media influence']== 'Y'\n",
    "All_top_10_AL_SMA_SMI=All_top_10_AL&All_top_10_SMA&All_top_10_SMI\n",
    "All_top_10_AL_SMA_SMI_df=All_top_10_df[All_top_10_AL_SMA_SMI]\n",
    "display(All_top_10_AL_SMA_SMI_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becc784a-1a6e-4a2d-a7fa-8af74ed21c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_top_10_AL=All_top_10_df['Awareness level'] == \"High\"\n",
    "All_top_10_SMA=All_top_10_df['Social media activity']=='High'  \n",
    "All_top_10_PGI=All_top_10_df['Peer group influence']== 'Y'\n",
    "All_top_10_AL_SMA_PGI=All_top_10_AL&All_top_10_SMA&All_top_10_PGI\n",
    "All_top_10_AL_SMA_PGI_df=All_top_10_df[All_top_10_AL_SMA_PGI]\n",
    "display(All_top_10_AL_SMA_PGI_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997f217d-fb37-4026-819e-21287c0c42be",
   "metadata": {},
   "outputs": [],
   "source": [
    "All_top_10_AL_Not=All_top_10_df['Awareness level'] != \"High\"\n",
    "All_top_10_SMA_Not=All_top_10_df['Social media activity']!='High'  \n",
    "All_top_10_SMI_Not=All_top_10_df['Social media influence']!= 'Y'\n",
    "All_top_10_AL_SMA_SMI_Not=All_top_10_AL_Not&All_top_10_SMA_Not&All_top_10_SMI_Not\n",
    "All_top_10_AL_SMA_SMI_Not_df=All_top_10_df[All_top_10_AL_SMA_SMI_Not]\n",
    "display(All_top_10_AL_SMA_SMI_Not_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc9a53ae-69fc-4923-85f5-c203eecd91c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e7aa6d-6a56-40d5-94c2-d22688a95b4e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
