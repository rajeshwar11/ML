{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3dfd35c9-6c4a-4f31-ad8c-2af95fa42cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "Scores_df = pd.read_csv(\"/Users/rajeshwarrao/Downloads/Result_Scores_Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "209a9a99-5d76-4693-9a08-b6875c19588b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Age group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Status (Single/ partner)</th>\n",
       "      <th>Residence Zip Code</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>...</th>\n",
       "      <th>Score_LS_Exercise</th>\n",
       "      <th>Score_LS_Drug_abuse</th>\n",
       "      <th>Score_LS_Smoking</th>\n",
       "      <th>Score_LS_Alcoholic</th>\n",
       "      <th>Score_LS_Professional_hazards</th>\n",
       "      <th>Score_LS_Other_factors</th>\n",
       "      <th>Score_SF_Awareness_level</th>\n",
       "      <th>Score_SF_Social_media_activity</th>\n",
       "      <th>Score_SF_Peer_group_influence</th>\n",
       "      <th>Score_SF_Other_social_activities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>70.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>60 +</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>90.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>80.0</td>\n",
       "      <td>Business</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>577</td>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>578</td>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>579</td>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>50.0</td>\n",
       "      <td>No Job</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>580</td>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>581</td>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "      <td>Professional</td>\n",
       "      <td>...</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0    S.N                   Name     ID Age group Sex  \\\n",
       "0             0  265.0  Frist_1 Last name_133  NM265  below 30   M   \n",
       "1             1  202.0  Frist_2 Last name_102  NM202     30-45   F   \n",
       "2             2  490.0  Frist_2 Last name_246  NM490     30-45   F   \n",
       "3             3  580.0  Frist_2 Last name_291  NM580      60 +   F   \n",
       "4             4  274.0  Frist_2 Last name_138  NM274     30-45   F   \n",
       "..          ...    ...                    ...    ...       ...  ..   \n",
       "577         577   85.0   Frist_1 Last name_43  NM085  below 30   M   \n",
       "578         578   29.0   Frist_1 Last name_15  NM029  below 30   M   \n",
       "579         579  573.0  Frist_1 Last name_287  NM573  below 30   M   \n",
       "580         580  565.0  Frist_1 Last name_283  NM565  below 30   M   \n",
       "581         581    5.0    Frist_1 Last name_3  NM005  below 30   M   \n",
       "\n",
       "    Status (Single/ partner)        Residence Zip Code  Score_Demo  \\\n",
       "0                     Single   Area Deprovation Index1        40.0   \n",
       "1                    Partner  Area Deprovation Index 2        80.0   \n",
       "2                     Single  Area Deprovation Index 2        70.0   \n",
       "3                     Single  Area Deprovation Index 2        90.0   \n",
       "4                    Partner  Area Deprovation Index 2        80.0   \n",
       "..                       ...                       ...         ...   \n",
       "577                   Single   Area Deprovation Index1        40.0   \n",
       "578                  Partner   Area Deprovation Index1        50.0   \n",
       "579                  Partner   Area Deprovation Index1        50.0   \n",
       "580                   Single   Area Deprovation Index1        40.0   \n",
       "581                   Single   Area Deprovation Index1        40.0   \n",
       "\n",
       "       Occupation  ... Score_LS_Exercise Score_LS_Drug_abuse  \\\n",
       "0    Professional  ...              10.0                10.0   \n",
       "1        Business  ...              10.0                10.0   \n",
       "2        Business  ...              10.0                10.0   \n",
       "3        Business  ...              10.0                10.0   \n",
       "4        Business  ...              10.0                10.0   \n",
       "..            ...  ...               ...                 ...   \n",
       "577  Professional  ...              20.0                20.0   \n",
       "578  Professional  ...              20.0                20.0   \n",
       "579        No Job  ...              20.0                20.0   \n",
       "580  Professional  ...              20.0                20.0   \n",
       "581  Professional  ...              20.0                20.0   \n",
       "\n",
       "     Score_LS_Smoking  Score_LS_Alcoholic Score_LS_Professional_hazards  \\\n",
       "0                10.0                20.0                          10.0   \n",
       "1                10.0                20.0                          10.0   \n",
       "2                10.0                20.0                          10.0   \n",
       "3                10.0                20.0                          10.0   \n",
       "4                10.0                20.0                          10.0   \n",
       "..                ...                 ...                           ...   \n",
       "577              10.0                10.0                          20.0   \n",
       "578              10.0                10.0                          20.0   \n",
       "579              10.0                10.0                          20.0   \n",
       "580              10.0                10.0                          20.0   \n",
       "581              10.0                10.0                          20.0   \n",
       "\n",
       "    Score_LS_Other_factors Score_SF_Awareness_level  \\\n",
       "0                     20.0                     20.0   \n",
       "1                     20.0                     20.0   \n",
       "2                     20.0                     20.0   \n",
       "3                     20.0                     20.0   \n",
       "4                     20.0                     20.0   \n",
       "..                     ...                      ...   \n",
       "577                   20.0                     10.0   \n",
       "578                   20.0                     10.0   \n",
       "579                   20.0                     10.0   \n",
       "580                   20.0                     10.0   \n",
       "581                   20.0                     10.0   \n",
       "\n",
       "     Score_SF_Social_media_activity  Score_SF_Peer_group_influence  \\\n",
       "0                              20.0                           10.0   \n",
       "1                              20.0                           10.0   \n",
       "2                              20.0                           10.0   \n",
       "3                              20.0                           10.0   \n",
       "4                              20.0                           10.0   \n",
       "..                              ...                            ...   \n",
       "577                            10.0                           20.0   \n",
       "578                            10.0                           20.0   \n",
       "579                            10.0                           20.0   \n",
       "580                            10.0                           20.0   \n",
       "581                            10.0                           20.0   \n",
       "\n",
       "    Score_SF_Other_social_activities  \n",
       "0                                 10  \n",
       "1                                 10  \n",
       "2                                 10  \n",
       "3                                 10  \n",
       "4                                 10  \n",
       "..                               ...  \n",
       "577                               10  \n",
       "578                               10  \n",
       "579                               10  \n",
       "580                               10  \n",
       "581                               10  \n",
       "\n",
       "[582 rows x 81 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Scores_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77ccf419-3f7a-4ae5-b16b-f324547bcc60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>S.N</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>FBP Level</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th># of Provider visist YTD</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Formulary visist YTD</th>\n",
       "      <th>Days of Therapy missed</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>...</th>\n",
       "      <th>Score_LS_Exercise</th>\n",
       "      <th>Score_LS_Drug_abuse</th>\n",
       "      <th>Score_LS_Smoking</th>\n",
       "      <th>Score_LS_Alcoholic</th>\n",
       "      <th>Score_LS_Professional_hazards</th>\n",
       "      <th>Score_LS_Other_factors</th>\n",
       "      <th>Score_SF_Awareness_level</th>\n",
       "      <th>Score_SF_Social_media_activity</th>\n",
       "      <th>Score_SF_Peer_group_influence</th>\n",
       "      <th>Score_SF_Other_social_activities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>499.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>582.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>290.500000</td>\n",
       "      <td>291.500000</td>\n",
       "      <td>70.962199</td>\n",
       "      <td>148.276553</td>\n",
       "      <td>56.013746</td>\n",
       "      <td>1.991409</td>\n",
       "      <td>70.429553</td>\n",
       "      <td>1.994845</td>\n",
       "      <td>4.548110</td>\n",
       "      <td>41.735395</td>\n",
       "      <td>...</td>\n",
       "      <td>12.491409</td>\n",
       "      <td>10.463918</td>\n",
       "      <td>11.254296</td>\n",
       "      <td>17.508591</td>\n",
       "      <td>11.254296</td>\n",
       "      <td>19.982818</td>\n",
       "      <td>17.508591</td>\n",
       "      <td>17.508591</td>\n",
       "      <td>11.254296</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>168.153204</td>\n",
       "      <td>168.153204</td>\n",
       "      <td>18.713400</td>\n",
       "      <td>26.748788</td>\n",
       "      <td>12.191239</td>\n",
       "      <td>1.631740</td>\n",
       "      <td>12.493896</td>\n",
       "      <td>1.631755</td>\n",
       "      <td>2.955829</td>\n",
       "      <td>13.831246</td>\n",
       "      <td>...</td>\n",
       "      <td>4.328876</td>\n",
       "      <td>2.105129</td>\n",
       "      <td>3.314903</td>\n",
       "      <td>4.328876</td>\n",
       "      <td>3.314903</td>\n",
       "      <td>0.414513</td>\n",
       "      <td>4.328876</td>\n",
       "      <td>4.328876</td>\n",
       "      <td>3.314903</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>45.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>145.250000</td>\n",
       "      <td>146.250000</td>\n",
       "      <td>52.500000</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>30.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>290.500000</td>\n",
       "      <td>291.500000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>70.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>40.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>435.750000</td>\n",
       "      <td>436.750000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>581.000000</td>\n",
       "      <td>582.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>200.000000</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>60.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0         S.N  Score_Demo   FBP Level   Score_Fin  \\\n",
       "count  582.000000  582.000000  582.000000  499.000000  582.000000   \n",
       "mean   290.500000  291.500000   70.962199  148.276553   56.013746   \n",
       "std    168.153204  168.153204   18.713400   26.748788   12.191239   \n",
       "min      0.000000    1.000000   40.000000  120.000000   30.000000   \n",
       "25%    145.250000  146.250000   52.500000  120.000000   50.000000   \n",
       "50%    290.500000  291.500000   70.000000  150.000000   60.000000   \n",
       "75%    435.750000  436.750000   80.000000  150.000000   60.000000   \n",
       "max    581.000000  582.000000  100.000000  200.000000   80.000000   \n",
       "\n",
       "       # of Provider visist YTD  Score_Medical_History  Formulary visist YTD  \\\n",
       "count                582.000000             582.000000            582.000000   \n",
       "mean                   1.991409              70.429553              1.994845   \n",
       "std                    1.631740              12.493896              1.631755   \n",
       "min                    0.000000              45.000000              0.000000   \n",
       "25%                    1.000000              60.000000              1.000000   \n",
       "50%                    2.000000              70.000000              2.000000   \n",
       "75%                    3.000000              80.000000              3.000000   \n",
       "max                    5.000000             100.000000              5.000000   \n",
       "\n",
       "       Days of Therapy missed  Score_Medication_History  ...  \\\n",
       "count              582.000000                582.000000  ...   \n",
       "mean                 4.548110                 41.735395  ...   \n",
       "std                  2.955829                 13.831246  ...   \n",
       "min                  0.000000                 20.000000  ...   \n",
       "25%                  3.000000                 30.000000  ...   \n",
       "50%                  5.000000                 40.000000  ...   \n",
       "75%                  7.000000                 50.000000  ...   \n",
       "max                 18.000000                 60.000000  ...   \n",
       "\n",
       "       Score_LS_Exercise  Score_LS_Drug_abuse  Score_LS_Smoking  \\\n",
       "count         582.000000           582.000000        582.000000   \n",
       "mean           12.491409            10.463918         11.254296   \n",
       "std             4.328876             2.105129          3.314903   \n",
       "min            10.000000            10.000000         10.000000   \n",
       "25%            10.000000            10.000000         10.000000   \n",
       "50%            10.000000            10.000000         10.000000   \n",
       "75%            10.000000            10.000000         10.000000   \n",
       "max            20.000000            20.000000         20.000000   \n",
       "\n",
       "       Score_LS_Alcoholic  Score_LS_Professional_hazards  \\\n",
       "count          582.000000                     582.000000   \n",
       "mean            17.508591                      11.254296   \n",
       "std              4.328876                       3.314903   \n",
       "min             10.000000                      10.000000   \n",
       "25%             20.000000                      10.000000   \n",
       "50%             20.000000                      10.000000   \n",
       "75%             20.000000                      10.000000   \n",
       "max             20.000000                      20.000000   \n",
       "\n",
       "       Score_LS_Other_factors  Score_SF_Awareness_level  \\\n",
       "count              582.000000                582.000000   \n",
       "mean                19.982818                 17.508591   \n",
       "std                  0.414513                  4.328876   \n",
       "min                 10.000000                 10.000000   \n",
       "25%                 20.000000                 20.000000   \n",
       "50%                 20.000000                 20.000000   \n",
       "75%                 20.000000                 20.000000   \n",
       "max                 20.000000                 20.000000   \n",
       "\n",
       "       Score_SF_Social_media_activity  Score_SF_Peer_group_influence  \\\n",
       "count                      582.000000                     582.000000   \n",
       "mean                        17.508591                      11.254296   \n",
       "std                          4.328876                       3.314903   \n",
       "min                         10.000000                      10.000000   \n",
       "25%                         20.000000                      10.000000   \n",
       "50%                         20.000000                      10.000000   \n",
       "75%                         20.000000                      10.000000   \n",
       "max                         20.000000                      20.000000   \n",
       "\n",
       "       Score_SF_Other_social_activities  \n",
       "count                             582.0  \n",
       "mean                               10.0  \n",
       "std                                 0.0  \n",
       "min                                10.0  \n",
       "25%                                10.0  \n",
       "50%                                10.0  \n",
       "75%                                10.0  \n",
       "max                                10.0  \n",
       "\n",
       "[8 rows x 50 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Scores_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5763b75a-6caf-41aa-86ea-d339e5813b9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>Score_Physical_Status</th>\n",
       "      <th>Score_Lifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S.N                   Name     ID  Score_Demo  Score_Fin  \\\n",
       "0    265.0  Frist_1 Last name_133  NM265        40.0       30.0   \n",
       "1    202.0  Frist_2 Last name_102  NM202        80.0       40.0   \n",
       "2    490.0  Frist_2 Last name_246  NM490        70.0       60.0   \n",
       "3    580.0  Frist_2 Last name_291  NM580        90.0       40.0   \n",
       "4    274.0  Frist_2 Last name_138  NM274        80.0       70.0   \n",
       "..     ...                    ...    ...         ...        ...   \n",
       "577   85.0   Frist_1 Last name_43  NM085        40.0       50.0   \n",
       "578   29.0   Frist_1 Last name_15  NM029        50.0       60.0   \n",
       "579  573.0  Frist_1 Last name_287  NM573        50.0       60.0   \n",
       "580  565.0  Frist_1 Last name_283  NM565        40.0       40.0   \n",
       "581    5.0    Frist_1 Last name_3  NM005        40.0       40.0   \n",
       "\n",
       "     Score_Medical_History  Score_Medication_History  Score_Physical_Status  \\\n",
       "0                     70.0                      60.0                   50.0   \n",
       "1                     85.0                      60.0                   50.0   \n",
       "2                     85.0                      60.0                   50.0   \n",
       "3                     90.0                      60.0                   50.0   \n",
       "4                     85.0                      60.0                   50.0   \n",
       "..                     ...                       ...                    ...   \n",
       "577                   90.0                      60.0                   80.0   \n",
       "578                   90.0                      60.0                   80.0   \n",
       "579                   80.0                      60.0                   80.0   \n",
       "580                   65.0                      40.0                   80.0   \n",
       "581                   65.0                      30.0                   80.0   \n",
       "\n",
       "     Score_Lifestyle  \n",
       "0               80.0  \n",
       "1               80.0  \n",
       "2               80.0  \n",
       "3               80.0  \n",
       "4               80.0  \n",
       "..               ...  \n",
       "577            100.0  \n",
       "578            100.0  \n",
       "579            100.0  \n",
       "580            100.0  \n",
       "581            100.0  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1248983833.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Demo_df['target']=Demo_df['Score_Demo'].astype(float)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age group</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Status (Single/ partner)</th>\n",
       "      <th>Residence Zip Code</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60 +</td>\n",
       "      <td>F</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>30-45</td>\n",
       "      <td>F</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index 2</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Partner</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>below 30</td>\n",
       "      <td>M</td>\n",
       "      <td>Single</td>\n",
       "      <td>Area Deprovation Index1</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Age group Sex Status (Single/ partner)        Residence Zip Code  target\n",
       "0    below 30   M                   Single   Area Deprovation Index1    40.0\n",
       "1       30-45   F                  Partner  Area Deprovation Index 2    80.0\n",
       "2       30-45   F                   Single  Area Deprovation Index 2    70.0\n",
       "3        60 +   F                   Single  Area Deprovation Index 2    90.0\n",
       "4       30-45   F                  Partner  Area Deprovation Index 2    80.0\n",
       "..        ...  ..                      ...                       ...     ...\n",
       "577  below 30   M                   Single   Area Deprovation Index1    40.0\n",
       "578  below 30   M                  Partner   Area Deprovation Index1    50.0\n",
       "579  below 30   M                  Partner   Area Deprovation Index1    50.0\n",
       "580  below 30   M                   Single   Area Deprovation Index1    40.0\n",
       "581  below 30   M                   Single   Area Deprovation Index1    40.0\n",
       "\n",
       "[582 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>582.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>70.962199</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>18.713400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>40.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>52.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>70.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>100.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           target\n",
       "count  582.000000\n",
       "mean    70.962199\n",
       "std     18.713400\n",
       "min     40.000000\n",
       "25%     52.500000\n",
       "50%     70.000000\n",
       "75%     80.000000\n",
       "max    100.000000"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Demo_columns = [ 'Age group','Sex','Status (Single/ partner)','Residence Zip Code','Score_Demo']\n",
    "Demo_df=Scores_df[Demo_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "Demo_df['target']=Demo_df['Score_Demo'].astype(float)\n",
    "Demo_df=Demo_df.drop('Score_Demo',axis=1)\n",
    "df.describe()\n",
    "display(Demo_df)\n",
    "Demo_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7b0e6bfb-1ae3-40bf-84ec-d8325be29341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "    Age group Sex Status (Single/ partner)        Residence Zip Code  target  \\\n",
      "0    below 30   M                   Single   Area Deprovation Index1    40.0   \n",
      "1       30-45   F                  Partner  Area Deprovation Index 2    80.0   \n",
      "2       30-45   F                   Single  Area Deprovation Index 2    70.0   \n",
      "3        60 +   F                   Single  Area Deprovation Index 2    90.0   \n",
      "4       30-45   F                  Partner  Area Deprovation Index 2    80.0   \n",
      "..        ...  ..                      ...                       ...     ...   \n",
      "577  below 30   M                   Single   Area Deprovation Index1    40.0   \n",
      "578  below 30   M                  Partner   Area Deprovation Index1    50.0   \n",
      "579  below 30   M                  Partner   Area Deprovation Index1    50.0   \n",
      "580  below 30   M                   Single   Area Deprovation Index1    40.0   \n",
      "581  below 30   M                   Single   Area Deprovation Index1    40.0   \n",
      "\n",
      "     target_encoded  \n",
      "0                 0  \n",
      "1                 4  \n",
      "2                 3  \n",
      "3                 5  \n",
      "4                 4  \n",
      "..              ...  \n",
      "577               0  \n",
      "578               1  \n",
      "579               1  \n",
      "580               0  \n",
      "581               0  \n",
      "\n",
      "[582 rows x 6 columns]\n",
      "X_train\n",
      "    Age group Sex Status (Single/ partner)        Residence Zip Code\n",
      "433     45-60   F                  Partner   Area Deprovation Index1\n",
      "209     45-60   M                  Partner   Area Deprovation Index1\n",
      "184     30-45   F                  Partner  Area Deprovation Index 2\n",
      "177     30-45   F                   Single  Area Deprovation Index 2\n",
      "410     45-60   F                  Partner   Area Deprovation Index1\n",
      "..        ...  ..                      ...                       ...\n",
      "71      30-45   F                  Partner  Area Deprovation Index 2\n",
      "106      60 +   F                  Partner  Area Deprovation Index 2\n",
      "270      60 +   F                   Single  Area Deprovation Index 2\n",
      "435     45-60   F                  Partner   Area Deprovation Index1\n",
      "102     45-60   M                   Single   Area Deprovation Index1\n",
      "\n",
      "[465 rows x 4 columns]\n",
      "test1\n",
      "433    4\n",
      "209    3\n",
      "184    4\n",
      "177    3\n",
      "410    4\n",
      "      ..\n",
      "71     4\n",
      "106    6\n",
      "270    5\n",
      "435    4\n",
      "102    2\n",
      "Name: target_encoded, Length: 465, dtype: int64\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-16 23:04:21.324943: W tensorflow/core/framework/op_kernel.cc:1807] OP_REQUIRES failed at cast_op.cc:121 : UNIMPLEMENTED: Cast string to float is not supported\n"
     ]
    },
    {
     "ename": "UnimplementedError",
     "evalue": "Graph execution error:\n\nDetected at node 'sequential_30/Cast' defined at (most recent call last):\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3496551992.py\", line 51, in <module>\n      model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 650, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 762, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential_30/Cast'\nCast string to float is not supported\n\t [[{{node sequential_30/Cast}}]] [Op:__inference_train_function_25261]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnimplementedError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 51\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28mprint\u001b[39m(y_train)\n\u001b[0;32m---> 51\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m     54\u001b[0m loss, accuracy \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mUnimplementedError\u001b[0m: Graph execution error:\n\nDetected at node 'sequential_30/Cast' defined at (most recent call last):\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 195, in start\n      self.asyncio_loop.run_forever()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 603, in run_forever\n      self._run_once()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/base_events.py\", line 1909, in _run_once\n      handle._run()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3009, in run_cell\n      result = self._run_cell(\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3064, in _run_cell\n      result = runner(coro)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3269, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3448, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3508, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3496551992.py\", line 51, in <module>\n      model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 1023, in train_step\n      y_pred = self(x, training=True)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/training.py\", line 561, in __call__\n      return super().__call__(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/base_layer.py\", line 1132, in __call__\n      outputs = call_fn(inputs, *args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 96, in error_handler\n      return fn(*args, **kwargs)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/sequential.py\", line 413, in call\n      return super().call(inputs, training=training, mask=mask)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 511, in call\n      return self._run_internal_graph(inputs, training=training, mask=mask)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 650, in _run_internal_graph\n      y = self._conform_to_reference_input(y, ref_input=x)\n    File \"/Users/rajeshwarrao/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/engine/functional.py\", line 762, in _conform_to_reference_input\n      tensor = tf.cast(tensor, dtype=ref_input.dtype)\nNode: 'sequential_30/Cast'\nCast string to float is not supported\n\t [[{{node sequential_30/Cast}}]] [Op:__inference_train_function_25261]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "#Demo_df['Score_Demo']=Demo_df['Score_Demo'].astype(str)\n",
    "print(\"test\")\n",
    "#print(Demo_df)\n",
    "df=Demo_df\n",
    "print(df)\n",
    "#df['target']=df['target'].astype(float)\n",
    "df.describe()\n",
    "'''\n",
    "df['target']=df['Score_Demo']\n",
    "df=df.drop('Score_Demo',axis=1)\n",
    "df.describe()\n",
    "'''\n",
    "# Preprocess the data, e.g., encoding categorical variables and scaling numerical features\n",
    "\n",
    "# Encode class labels (assuming 'target' is the column containing class labels)\n",
    "label_encoder = LabelEncoder()\n",
    "df['target_encoded'] = label_encoder.fit_transform(df['target'])\n",
    "\n",
    "# Split the data into features and labels\n",
    "X = df.drop(['target', 'target_encoded'], axis=1)\n",
    "y = df['target_encoded']\n",
    "\n",
    "#y = tf.convert_to_tensor(y, dtype=tf.int64)\n",
    "#y = y.tolist()\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"X_train\")\n",
    "print(X_train)\n",
    "# Define the deep learning model\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
    "    keras.layers.Dense(32, activation='relu'),\n",
    "    keras.layers.Dense(5, activation='softmax')  # Multiclass classification with 5 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "print(\"test1\")\n",
    "print(y_train)\n",
    "model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0acc7d7f-a6be-41a8-9f23-63b89d2b9f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'text_column'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text_column'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 20\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Preprocess the text data using TF-IDF vectorization\u001b[39;00m\n\u001b[1;32m     19\u001b[0m tfidf_vectorizer \u001b[38;5;241m=\u001b[39m TfidfVectorizer(max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5000\u001b[39m)  \u001b[38;5;66;03m# You can adjust the max_features parameter\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m X_train_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mfit_transform(\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext_column\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     21\u001b[0m X_test_tfidf \u001b[38;5;241m=\u001b[39m tfidf_vectorizer\u001b[38;5;241m.\u001b[39mtransform(X_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_column\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# Train a text classification model (e.g., Multinomial Naive Bayes)\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3762\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'text_column'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Load your dataset into a Pandas DataFrame\n",
    "# Replace 'your_dataset.csv' with the path to your dataset\n",
    "df = Demo_df\n",
    "\n",
    "# Split the DataFrame into features (X) and target (y)\n",
    "X = df.iloc[:, :-1]  # All columns except the last one\n",
    "y = df.iloc[:, -1]   # The last column\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess the text data using TF-IDF vectorization\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=5000)  # You can adjust the max_features parameter\n",
    "X_train_tfidf = tfidf_vectorizer.fit_transform(X_train['Age group'])\n",
    "X_test_tfidf = tfidf_vectorizer.transform(X_test['Age group'])\n",
    "\n",
    "# Train a text classification model (e.g., Multinomial Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# Make predictions on the test data\n",
    "y_pred = classifier.predict(X_test_tfidf)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "classification_rep = classification_report(y_test, y_pred)\n",
    "\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print('Classification Report:\\n', classification_rep)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "fb3e5215-77a8-45f6-9fcf-05f81cd3752b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns with float values: []\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - 1s 13ms/step - loss: 1.8525 - accuracy: 0.2344 - val_loss: 1.6646 - val_accuracy: 0.3617\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1.5937 - accuracy: 0.4378 - val_loss: 1.4003 - val_accuracy: 0.4255\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 0s 10ms/step - loss: 1.2653 - accuracy: 0.5431 - val_loss: 1.0361 - val_accuracy: 0.6596\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 0s 8ms/step - loss: 0.9437 - accuracy: 0.6699 - val_loss: 0.7466 - val_accuracy: 0.8511\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.6707 - accuracy: 0.7871 - val_loss: 0.4989 - val_accuracy: 0.8936\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.4490 - accuracy: 0.9569 - val_loss: 0.3267 - val_accuracy: 1.0000\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.2849 - accuracy: 1.0000 - val_loss: 0.2040 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1784 - accuracy: 1.0000 - val_loss: 0.1278 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.1083 - accuracy: 1.0000 - val_loss: 0.0778 - val_accuracy: 1.0000\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 0s 7ms/step - loss: 0.0706 - accuracy: 1.0000 - val_loss: 0.0568 - val_accuracy: 1.0000\n",
      "4/4 [==============================] - 0s 2ms/step - loss: 0.0523 - accuracy: 1.0000\n",
      "Accuracy: 1.0\n",
      "4/4 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       100.0       1.00      1.00      1.00        20\n",
      "        40.0       1.00      1.00      1.00        17\n",
      "        50.0       1.00      1.00      1.00        18\n",
      "        60.0       1.00      1.00      1.00         8\n",
      "        70.0       1.00      1.00      1.00        33\n",
      "        80.0       1.00      1.00      1.00        12\n",
      "        90.0       1.00      1.00      1.00         9\n",
      "\n",
      "    accuracy                           1.00       117\n",
      "   macro avg       1.00      1.00      1.00       117\n",
      "weighted avg       1.00      1.00      1.00       117\n",
      "\n",
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "LabelEncoder.inverse_transform() got an unexpected keyword argument 'axis'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[80], line 102\u001b[0m\n\u001b[1;32m    100\u001b[0m input_string \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbelow 30,M,Single,Area Deprovation Index1\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    101\u001b[0m predicted_class,predictions \u001b[38;5;241m=\u001b[39m predict_single_string(input_string)\n\u001b[0;32m--> 102\u001b[0m y_pred_labels \u001b[38;5;241m=\u001b[39m \u001b[43mlabel_encoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minverse_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Class: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpredicted_class\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPredicted Value: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00my_pred_labels\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: LabelEncoder.inverse_transform() got an unexpected keyword argument 'axis'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Load your DataFrame\n",
    "# Let's assume you have a DataFrame 'df' with multiple text columns and the target column 'target'\n",
    "df=Demo_df\n",
    "df['target']=df['target'].astype(str)\n",
    "# Preprocess the text data\n",
    "# Combine multiple text columns into one, assuming they are named 'text_col1', 'text_col2', ...\n",
    "text_columns = ['Age group','Sex','Status (Single/ partner)','Residence Zip Code']\n",
    "df['combined_text'] = df[text_columns].apply(lambda x: ' '.join(x), axis=1)\n",
    "\n",
    "# Encode the target variable\n",
    "label_encoder = LabelEncoder()\n",
    "df['target_encoded'] = label_encoder.fit_transform(df['target'])\n",
    "\n",
    "# Split the data into features (X) and labels (y)\n",
    "X = df['combined_text']\n",
    "y = df['target_encoded']\n",
    "\n",
    "float_columns = []\n",
    "for column in df.columns:\n",
    "    for value in df[column]:\n",
    "        if isinstance(value, float):\n",
    "            float_columns.append(column)\n",
    "\n",
    "# Remove duplicates if needed\n",
    "float_columns = list(set(float_columns))\n",
    "\n",
    "# Print the columns containing float values\n",
    "print(\"Columns with float values:\", float_columns)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Tokenize the text data\n",
    "max_words = 10000  # Set the maximum number of words in your vocabulary\n",
    "tokenizer = Tokenizer(num_words=max_words, oov_token='<OOV>')\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "# Convert text data to sequences\n",
    "X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
    "X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "# Pad sequences to have the same length\n",
    "max_sequence_length = 100  # Set the maximum sequence length\n",
    "X_train_padded = pad_sequences(X_train_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "X_test_padded = pad_sequences(X_test_sequences, maxlen=max_sequence_length, padding='post', truncating='post')\n",
    "\n",
    "# Build a text classification model\n",
    "embedding_dim = 64  # Set the embedding dimension\n",
    "model = keras.Sequential([\n",
    "    keras.layers.Embedding(input_dim=max_words, output_dim=embedding_dim, input_length=max_sequence_length),\n",
    "    keras.layers.Flatten(),\n",
    "    keras.layers.Dense(128, activation='relu'),\n",
    "    keras.layers.Dense(len(label_encoder.classes_), activation='softmax')  # Output layer for multiclass classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train_padded, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test_padded, y_test)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test_padded)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "\n",
    "# Convert predicted class indices back to class labels\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "\n",
    "# Print classification report\n",
    "print(classification_report(y_test, y_pred_classes, target_names=label_encoder.classes_))\n",
    "\n",
    "\n",
    "def predict_single_string(input_string):\n",
    "    # Preprocess the input string similar to how you preprocessed your training data\n",
    "    input_sequence = tokenizer.texts_to_sequences([input_string])\n",
    "    input_padded = pad_sequences(input_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "    # Make predictions using the model\n",
    "    predictions = model.predict(input_padded)\n",
    "\n",
    "    # Assuming it's a classification task, you can get the predicted class\n",
    "    predicted_class = np.argmax(predictions[0])\n",
    "\n",
    "    return predicted_class,predictions\n",
    "\n",
    "input_string = \"below 30,M,Single,Area Deprovation Index1\"\n",
    "predicted_class,predictions = predict_single_string(input_string)\n",
    "y_pred_labels = label_encoder.inverse_transform(np.argmax(predictions),axis=1)\n",
    "\n",
    "print(f\"Predicted Class: {predicted_class}\")\n",
    "print(f\"Predicted Value: {y_pred_labels}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "513b2eb4-eacf-4d99-be38-446371133210",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14 12 11 ...  0  0  0]\n",
      " [12  5  7 ...  0  0  0]\n",
      " [12  5  7 ...  0  0  0]\n",
      " ...\n",
      " [12  5  7 ...  0  0  0]\n",
      " [ 6  7 13 ...  0  0  0]\n",
      " [14 12 11 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print(X_test_padded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "1b660d7d-b648-44c5-9ae7-7858fcbd99f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sequences:\n",
      "[[10], [2], [7], [3], [11], [], [12], [13], [], [14], [], [15], [4], [5], [16], [7], [2], [], [6], [8], [2], [6], [], [9], [2], [17], [8], [3], [18], [6], [19], [4], [3], [5], [], [4], [5], [9], [2], [20], [21]]\n",
      "\n",
      "Padded sequences:\n",
      "[[10  0  0 ...  0  0  0]\n",
      " [ 2  0  0 ...  0  0  0]\n",
      " [ 7  0  0 ...  0  0  0]\n",
      " ...\n",
      " [ 2  0  0 ...  0  0  0]\n",
      " [20  0  0 ...  0  0  0]\n",
      " [21  0  0 ...  0  0  0]]\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "[[0.13028352 0.02916628 0.06846163 0.06085423 0.5518586  0.10010442\n",
      "  0.05927129]\n",
      " [0.09733115 0.02520453 0.0572634  0.06386286 0.60436904 0.10322354\n",
      "  0.04874546]\n",
      " [0.12101342 0.02825298 0.06668641 0.05878294 0.564204   0.10295494\n",
      "  0.05810534]\n",
      " [0.11009415 0.0199475  0.04497774 0.05736488 0.6097632  0.1017804\n",
      "  0.05607212]\n",
      " [0.09039203 0.02142046 0.04834927 0.0628143  0.63109356 0.10306791\n",
      "  0.0428625 ]\n",
      " [0.11449898 0.02332818 0.05312143 0.05915675 0.59139264 0.10126388\n",
      "  0.05723822]\n",
      " [0.05471382 0.01153004 0.02267903 0.05967163 0.7152665  0.10253051\n",
      "  0.03360843]\n",
      " [0.09518871 0.01767824 0.0389059  0.05957804 0.6361064  0.10262757\n",
      "  0.04991513]\n",
      " [0.11449898 0.02332818 0.05312143 0.05915675 0.59139264 0.10126388\n",
      "  0.05723822]\n",
      " [0.11494714 0.15501362 0.4435049  0.04179025 0.16781059 0.0512752\n",
      "  0.02565824]\n",
      " [0.11449898 0.02332818 0.05312143 0.05915675 0.59139264 0.10126388\n",
      "  0.05723822]\n",
      " [0.11384561 0.02157825 0.04889445 0.05789102 0.59949356 0.10080574\n",
      "  0.05749131]\n",
      " [0.14251836 0.0265276  0.06078809 0.05510281 0.544855   0.09902\n",
      "  0.07118821]\n",
      " [0.05798231 0.00381931 0.00756459 0.04004708 0.75445163 0.08811261\n",
      "  0.04802251]\n",
      " [0.10886996 0.02407742 0.05443931 0.06026584 0.59602755 0.10259007\n",
      "  0.0537298 ]\n",
      " [0.12101342 0.02825298 0.06668641 0.05878294 0.564204   0.10295494\n",
      "  0.05810534]\n",
      " [0.09733115 0.02520453 0.0572634  0.06386286 0.60436904 0.10322354\n",
      "  0.04874546]\n",
      " [0.11449898 0.02332818 0.05312143 0.05915675 0.59139264 0.10126388\n",
      "  0.05723822]\n",
      " [0.20159604 0.01260804 0.02949239 0.0428182  0.5131519  0.08440405\n",
      "  0.11592939]\n",
      " [0.11007785 0.02378195 0.05299812 0.05998982 0.59465533 0.10173354\n",
      "  0.05676332]\n",
      " [0.09733115 0.02520453 0.0572634  0.06386286 0.60436904 0.10322354\n",
      "  0.04874546]\n",
      " [0.20159604 0.01260804 0.02949239 0.0428182  0.5131519  0.08440405\n",
      "  0.11592939]\n",
      " [0.11449898 0.02332818 0.05312143 0.05915675 0.59139264 0.10126388\n",
      "  0.05723822]\n",
      " [0.08761406 0.01703471 0.03748579 0.05588079 0.64829946 0.1045356\n",
      "  0.04914963]\n",
      " [0.09733115 0.02520453 0.0572634  0.06386286 0.60436904 0.10322354\n",
      "  0.04874546]\n",
      " [0.11025999 0.0207689  0.04671181 0.05800042 0.6060519  0.10209608\n",
      "  0.05611088]\n",
      " [0.11007785 0.02378195 0.05299812 0.05998982 0.59465533 0.10173354\n",
      "  0.05676332]\n",
      " [0.11009415 0.0199475  0.04497774 0.05736488 0.6097632  0.1017804\n",
      "  0.05607212]\n",
      " [0.10989685 0.02191991 0.04891749 0.05930819 0.60217845 0.10212752\n",
      "  0.0556515 ]\n",
      " [0.20159604 0.01260804 0.02949239 0.0428182  0.5131519  0.08440405\n",
      "  0.11592939]\n",
      " [0.11146758 0.02447409 0.0561041  0.05975636 0.5908631  0.10218807\n",
      "  0.05514669]\n",
      " [0.14251836 0.0265276  0.06078809 0.05510281 0.544855   0.09902\n",
      "  0.07118821]\n",
      " [0.11009415 0.0199475  0.04497774 0.05736488 0.6097632  0.1017804\n",
      "  0.05607212]\n",
      " [0.05798231 0.00381931 0.00756459 0.04004708 0.75445163 0.08811261\n",
      "  0.04802251]\n",
      " [0.11449898 0.02332818 0.05312143 0.05915675 0.59139264 0.10126388\n",
      "  0.05723822]\n",
      " [0.14251836 0.0265276  0.06078809 0.05510281 0.544855   0.09902\n",
      "  0.07118821]\n",
      " [0.05798231 0.00381931 0.00756459 0.04004708 0.75445163 0.08811261\n",
      "  0.04802251]\n",
      " [0.08761406 0.01703471 0.03748579 0.05588079 0.64829946 0.1045356\n",
      "  0.04914963]\n",
      " [0.09733115 0.02520453 0.0572634  0.06386286 0.60436904 0.10322354\n",
      "  0.04874546]\n",
      " [0.11804891 0.02424355 0.05548264 0.05899583 0.58481956 0.10072815\n",
      "  0.05768139]\n",
      " [0.11745861 0.02175775 0.04949642 0.05817422 0.5914894  0.10149591\n",
      "  0.06012768]]\n",
      "[4 4 4 4 4 4 4 4 4 2 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4 4\n",
      " 4 4 4 4]\n",
      "['70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '50.0'\n",
      " '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0'\n",
      " '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0'\n",
      " '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0' '70.0'\n",
      " '70.0']\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample text data\n",
    "text_data = \"below 30,M,Single,Area Deprovation Index1\" \n",
    "\n",
    "\n",
    "# Initialize a tokenizer\n",
    "tokenizer = Tokenizer(num_words=1000, oov_token=\"<OOV>\")  # `num_words` limits the vocabulary size\n",
    "\n",
    "# Fit the tokenizer on the text data\n",
    "tokenizer.fit_on_texts(text_data)\n",
    "\n",
    "# Convert text to sequences\n",
    "sequences = tokenizer.texts_to_sequences(text_data)\n",
    "\n",
    "# Pad sequences to a uniform length (e.g., maxlen=100)\n",
    "max_length = 100  # Adjust this based on your specific requirements\n",
    "padded_sequences = pad_sequences(sequences, maxlen=max_length, padding=\"post\", truncating=\"post\")\n",
    "\n",
    "# Inspect the tokenized and padded sequences\n",
    "print(\"Tokenized sequences:\")\n",
    "print(sequences)\n",
    "\n",
    "print(\"\\nPadded sequences:\")\n",
    "print(padded_sequences)\n",
    "input=np.array(padded_sequences)\n",
    "y_pred = model.predict(input)\n",
    "print(y_pred)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "print(y_pred_classes)\n",
    "y_pred_labels = label_encoder.inverse_transform(y_pred_classes)\n",
    "print(y_pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "7623260f-91ae-48b2-9d0a-1996ac577689",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 100, 64), (None, 100)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[85], line 32\u001b[0m\n\u001b[1;32m     30\u001b[0m vocab_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1000\u001b[39m\n\u001b[1;32m     31\u001b[0m embedding_layer \u001b[38;5;241m=\u001b[39m Embedding(input_dim\u001b[38;5;241m=\u001b[39mvocab_size, output_dim\u001b[38;5;241m=\u001b[39membedding_dim)(text_input_1)\n\u001b[0;32m---> 32\u001b[0m concatenated_input \u001b[38;5;241m=\u001b[39m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43membedding_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext_input_2\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m flatten_layer \u001b[38;5;241m=\u001b[39m Flatten()(concatenated_input)\n\u001b[1;32m     34\u001b[0m dense_layer \u001b[38;5;241m=\u001b[39m Dense(\u001b[38;5;241m128\u001b[39m, activation\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelu\u001b[39m\u001b[38;5;124m'\u001b[39m)(flatten_layer)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/layers/merging/concatenate.py:119\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    117\u001b[0m ranks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ranks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Get the only rank for the set.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m (rank,) \u001b[38;5;241m=\u001b[39m ranks\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 100, 64), (None, 100)]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Demo_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "text_columns = ['Age group','Sex','Status (Single/ partner)','Residence Zip Code']\n",
    "tokenizer.fit_on_texts(data['Age group'] + ' ' + data['Sex']+ ' ' + data['Status (Single/ partner)']+ ' ' + data['Residence Zip Code'])\n",
    "X = tokenizer.texts_to_sequences(data['Age group'] + ' ' + data['Sex']+ ' ' + data['Status (Single/ partner)']+ ' ' + data['Residence Zip Code'])\n",
    "X = pad_sequences(X, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "text_input_1 = Input(shape=(max_sequence_length,))\n",
    "text_input_2 = Input(shape=(max_sequence_length,))\n",
    "vocab_size=1000\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(text_input_1)\n",
    "concatenated_input = Concatenate()([embedding_layer, text_input_2])\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[text_input_1, text_input_2], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X_train, X_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_text = \"below 30,M,Single,Area Deprovation Index1\"\n",
    "single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_sequence, single_input_sequence])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "dd326232-1da2-4c48-857e-4b7af48246fb",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 100, 64), (None, 100)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[88], line 40\u001b[0m\n\u001b[1;32m     38\u001b[0m Zip \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_sequence_length,))\n\u001b[1;32m     39\u001b[0m embedding_layer \u001b[38;5;241m=\u001b[39m Embedding(input_dim\u001b[38;5;241m=\u001b[39mvocab_size, output_dim\u001b[38;5;241m=\u001b[39membedding_dim)(Age)\n\u001b[0;32m---> 40\u001b[0m concatenated_input \u001b[38;5;241m=\u001b[39m \u001b[43mConcatenate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43membedding_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mSex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m concatenated_input \u001b[38;5;241m=\u001b[39m Concatenate()([concatenated_input, Status])\n\u001b[1;32m     42\u001b[0m concatenated_input \u001b[38;5;241m=\u001b[39m Concatenate()([concatenated_input, Zip])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/tf/lib/python3.10/site-packages/keras/layers/merging/concatenate.py:119\u001b[0m, in \u001b[0;36mConcatenate.build\u001b[0;34m(self, input_shape)\u001b[0m\n\u001b[1;32m    117\u001b[0m ranks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mlen\u001b[39m(shape) \u001b[38;5;28;01mfor\u001b[39;00m shape \u001b[38;5;129;01min\u001b[39;00m shape_set)\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(ranks) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(err_msg)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;66;03m# Get the only rank for the set.\u001b[39;00m\n\u001b[1;32m    121\u001b[0m (rank,) \u001b[38;5;241m=\u001b[39m ranks\n",
      "\u001b[0;31mValueError\u001b[0m: A `Concatenate` layer requires inputs with matching shapes except for the concatenation axis. Received: input_shape=[(None, 100, 64), (None, 100)]"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Demo_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "text_columns = ['Age group','Sex','Status (Single/ partner)','Residence Zip Code']\n",
    "\n",
    "tokenizer.fit_on_texts(data['Age group'] + ' ' + data['Sex']+ ' ' + data['Status (Single/ partner)']+ ' ' + data['Residence Zip Code'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Age group'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Sex'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Status (Single/ partner)'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Residence Zip Code'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test,X3_train, X3_test,X4_train, X4_test, y_train, y_test = train_test_split(X1, X2,X3, X4, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Age = Input(shape=(max_sequence_length,))\n",
    "Sex = Input(shape=(max_sequence_length,))\n",
    "Status = Input(shape=(max_sequence_length,))\n",
    "Zip = Input(shape=(max_sequence_length,))\n",
    "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Age)\n",
    "concatenated_input = Concatenate()([embedding_layer, Sex])\n",
    "concatenated_input = Concatenate()([concatenated_input, Status])\n",
    "concatenated_input = Concatenate()([concatenated_input, Zip])\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Age, Sex,Status,Zip], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Age = \"your single input string here for text_column_1\"\n",
    "single_input_Sex = \"your single input string here for text_column_2\"\n",
    "single_input_Status = \"your single input string here for text_column_2\"\n",
    "single_input_Zip = \"your single input string here for text_column_2\"\n",
    "\n",
    "single_input_sequence_Age = tokenizer.texts_to_sequences([single_input_Age])\n",
    "single_input_sequence_Sex = tokenizer.texts_to_sequences([single_input_Sex])\n",
    "single_input_sequence_Status = tokenizer.texts_to_sequences([single_input_Status])\n",
    "single_input_sequence_Zip = tokenizer.texts_to_sequences([single_input_Zip])\n",
    "\n",
    "single_input_sequence_Age = pad_sequences(single_input_sequence_Age, maxlen=max_sequence_length)\n",
    "single_input_sequence_Sex = pad_sequences(single_input_sequence_Sex, maxlen=max_sequence_length)\n",
    "single_input_sequence_Status = pad_sequences(single_input_sequence_Status, maxlen=max_sequence_length)\n",
    "single_input_sequence_Zip = pad_sequences(single_input_sequence_Zip, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_sequence_Age, single_input_sequence_Sex,single_input_sequence_Status,single_input_sequence_Zip])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "31fd3dae-cff3-445f-a5b1-679e79331364",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "14/14 [==============================] - 1s 25ms/step - loss: 1.9692 - accuracy: 0.2153 - val_loss: 1.9131 - val_accuracy: 0.1277\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.8788 - accuracy: 0.2033 - val_loss: 1.8618 - val_accuracy: 0.2340\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.8080 - accuracy: 0.2536 - val_loss: 1.6649 - val_accuracy: 0.2340\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.6450 - accuracy: 0.3493 - val_loss: 1.5029 - val_accuracy: 0.5957\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.4377 - accuracy: 0.4809 - val_loss: 1.2342 - val_accuracy: 0.5957\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.1790 - accuracy: 0.5407 - val_loss: 0.9900 - val_accuracy: 0.9149\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.8725 - accuracy: 0.8517 - val_loss: 0.6744 - val_accuracy: 1.0000\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.6176 - accuracy: 0.9426 - val_loss: 0.4593 - val_accuracy: 1.0000\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4217 - accuracy: 0.9641 - val_loss: 0.2996 - val_accuracy: 0.9574\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.2782 - accuracy: 0.9928 - val_loss: 0.2120 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 51ms/step\n",
      "Predicted class: ['40.0']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Demo_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "text_columns = ['Age group','Sex','Status (Single/ partner)','Residence Zip Code']\n",
    "\n",
    "tokenizer.fit_on_texts(data['Age group'] + ' ' + data['Sex'] + ' ' + data['Status (Single/ partner)'] + ' ' + data['Residence Zip Code'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Age group'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Sex'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Status (Single/ partner)'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Residence Zip Code'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Age = Input(shape=(max_sequence_length,))\n",
    "Sex = Input(shape=(max_sequence_length,))\n",
    "Status = Input(shape=(max_sequence_length,))\n",
    "Zip = Input(shape=(max_sequence_length,))\n",
    "embedding_layer_Age = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Age)\n",
    "embedding_layer_Sex = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Sex)\n",
    "embedding_layer_Status = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Status)\n",
    "embedding_layer_Zip = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Zip)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Age, embedding_layer_Sex, embedding_layer_Status, embedding_layer_Zip])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Age, Sex, Status, Zip], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Age = \"below 30\"\n",
    "single_input_Sex = \"M\"\n",
    "single_input_Status = \"Single\"\n",
    "single_input_Zip = \"Area Deprovation Index1\"\n",
    "\n",
    "single_input_sequence_Age = tokenizer.texts_to_sequences([single_input_Age])\n",
    "single_input_sequence_Sex = tokenizer.texts_to_sequences([single_input_Sex])\n",
    "single_input_sequence_Status = tokenizer.texts_to_sequences([single_input_Status])\n",
    "single_input_sequence_Zip = tokenizer.texts_to_sequences([single_input_Zip])\n",
    "\n",
    "single_input_sequence_Age = pad_sequences(single_input_sequence_Age, maxlen=max_sequence_length)\n",
    "single_input_sequence_Sex = pad_sequences(single_input_sequence_Sex, maxlen=max_sequence_length)\n",
    "single_input_sequence_Status = pad_sequences(single_input_sequence_Status, maxlen=max_sequence_length)\n",
    "single_input_sequence_Zip = pad_sequences(single_input_sequence_Zip, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_sequence_Age, single_input_sequence_Sex, single_input_sequence_Status, single_input_sequence_Zip])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c6c281a6-f1cb-41ae-968c-314f7b065d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predicted class: 60.0\n"
     ]
    }
   ],
   "source": [
    "def predict_class_dmeo(model, tokenizer, max_sequence_length, age, sex, status, zip_code):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{age} {sex} {status} {zip_code}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Age = tokenizer.texts_to_sequences([age])\n",
    "    single_input_sequence_Sex = tokenizer.texts_to_sequences([sex])\n",
    "    single_input_sequence_Status = tokenizer.texts_to_sequences([status])\n",
    "    single_input_sequence_Zip = tokenizer.texts_to_sequences([zip_code])\n",
    "    \n",
    "    single_input_sequence_Age = pad_sequences(single_input_sequence_Age, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Sex = pad_sequences(single_input_sequence_Sex, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Status = pad_sequences(single_input_sequence_Status, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Zip = pad_sequences(single_input_sequence_Zip, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Age, single_input_sequence_Sex, single_input_sequence_Status, single_input_sequence_Zip])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "age = \"below 30\"\n",
    "sex = \"M\"\n",
    "status = \"Single\"\n",
    "zip_code = \"Area Deprovation Index 2\"\n",
    "predicted_class = predict_class_dmeo(model, tokenizer, max_sequence_length, age, sex, status, zip_code)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "5d73a2ea-5536-450f-868c-1182a8608300",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('/Users/rajeshwarrao/Downloads/Demo_Score.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "eb4c4b69-4b34-48d6-976b-e5a990afbf42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 189ms/step\n",
      "Predicted class: 60.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/Demo_Score.h5')\n",
    "\n",
    "age = \"below 30\"\n",
    "sex = \"M\"\n",
    "status = \"Single\"\n",
    "zip_code = \"Area Deprovation Index 2\"\n",
    "\n",
    "# Now, you can use the loaded_model for predictions\n",
    "predicted_class = predict_class_dmeo(loaded_model, tokenizer, max_sequence_length, age, sex, status, zip_code)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c54f79f4-f304-440b-a6f8-180da5304717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>Score_Physical_Status</th>\n",
       "      <th>Score_Lifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S.N                   Name     ID  Score_Demo  Score_Fin  \\\n",
       "0    265.0  Frist_1 Last name_133  NM265        40.0       30.0   \n",
       "1    202.0  Frist_2 Last name_102  NM202        80.0       40.0   \n",
       "2    490.0  Frist_2 Last name_246  NM490        70.0       60.0   \n",
       "3    580.0  Frist_2 Last name_291  NM580        90.0       40.0   \n",
       "4    274.0  Frist_2 Last name_138  NM274        80.0       70.0   \n",
       "..     ...                    ...    ...         ...        ...   \n",
       "577   85.0   Frist_1 Last name_43  NM085        40.0       50.0   \n",
       "578   29.0   Frist_1 Last name_15  NM029        50.0       60.0   \n",
       "579  573.0  Frist_1 Last name_287  NM573        50.0       60.0   \n",
       "580  565.0  Frist_1 Last name_283  NM565        40.0       40.0   \n",
       "581    5.0    Frist_1 Last name_3  NM005        40.0       40.0   \n",
       "\n",
       "     Score_Medical_History  Score_Medication_History  Score_Physical_Status  \\\n",
       "0                     70.0                      60.0                   50.0   \n",
       "1                     85.0                      60.0                   50.0   \n",
       "2                     85.0                      60.0                   50.0   \n",
       "3                     90.0                      60.0                   50.0   \n",
       "4                     85.0                      60.0                   50.0   \n",
       "..                     ...                       ...                    ...   \n",
       "577                   90.0                      60.0                   80.0   \n",
       "578                   90.0                      60.0                   80.0   \n",
       "579                   80.0                      60.0                   80.0   \n",
       "580                   65.0                      40.0                   80.0   \n",
       "581                   65.0                      30.0                   80.0   \n",
       "\n",
       "     Score_Lifestyle  \n",
       "0               80.0  \n",
       "1               80.0  \n",
       "2               80.0  \n",
       "3               80.0  \n",
       "4               80.0  \n",
       "..               ...  \n",
       "577            100.0  \n",
       "578            100.0  \n",
       "579            100.0  \n",
       "580            100.0  \n",
       "581            100.0  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1397768203.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Fin_df.fillna(0, inplace=True)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1397768203.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Fin_df['target']=Fin_df['Score_Fin'].astype(float)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1397768203.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  Fin_df['FBP Level']=Fin_df['FBP Level'].astype(str)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Own House</th>\n",
       "      <th>Own Car</th>\n",
       "      <th>FBP Level</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Professional</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>120.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Professional</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>120.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Professional</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>No Job</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Professional</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>150.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>Professional</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>150.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Occupation Own House Own Car FBP Level  target\n",
       "0    Professional         Y       Y       0.0    30.0\n",
       "1        Business         Y       Y       0.0    40.0\n",
       "2        Business         Y       Y     120.0    60.0\n",
       "3        Business         Y       Y       0.0    40.0\n",
       "4        Business         Y       N     120.0    70.0\n",
       "..            ...       ...     ...       ...     ...\n",
       "577  Professional         Y       Y     120.0    50.0\n",
       "578  Professional         Y       N     120.0    60.0\n",
       "579        No Job         Y       N       0.0    60.0\n",
       "580  Professional         Y       Y     150.0    40.0\n",
       "581  Professional         Y       Y     150.0    40.0\n",
       "\n",
       "[582 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>582.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>56.013746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>12.191239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>30.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>50.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>60.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>80.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           target\n",
       "count  582.000000\n",
       "mean    56.013746\n",
       "std     12.191239\n",
       "min     30.000000\n",
       "25%     50.000000\n",
       "50%     60.000000\n",
       "75%     60.000000\n",
       "max     80.000000"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "Fin_columns = [ 'Occupation','Own House','Own Car','FBP Level','Score_Fin']\n",
    "Fin_df=Scores_df[Fin_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "Fin_df.fillna(0, inplace=True)\n",
    "Fin_df['target']=Fin_df['Score_Fin'].astype(float)\n",
    "Fin_df['FBP Level']=Fin_df['FBP Level'].astype(str)\n",
    "Fin_df=Fin_df.drop('Score_Fin',axis=1)\n",
    "display(Fin_df)\n",
    "Fin_df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bfe0ffd2-c9b2-4ba7-a84d-b67639b3c448",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           target\n",
      "count  582.000000\n",
      "mean    56.013746\n",
      "std     12.191239\n",
      "min     30.000000\n",
      "25%     50.000000\n",
      "50%     60.000000\n",
      "75%     60.000000\n",
      "max     80.000000\n",
      "0      30.0\n",
      "1      40.0\n",
      "2      60.0\n",
      "3      40.0\n",
      "4      70.0\n",
      "       ... \n",
      "577    50.0\n",
      "578    60.0\n",
      "579    60.0\n",
      "580    40.0\n",
      "581    40.0\n",
      "Name: target, Length: 582, dtype: float64\n",
      "Epoch 1/10\n",
      "14/14 [==============================] - 1s 32ms/step - loss: 1.6584 - accuracy: 0.2536 - val_loss: 1.5595 - val_accuracy: 0.2128\n",
      "Epoch 2/10\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.6083 - accuracy: 0.2799 - val_loss: 1.4599 - val_accuracy: 0.3191\n",
      "Epoch 3/10\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.5659 - accuracy: 0.3421 - val_loss: 1.3979 - val_accuracy: 0.2979\n",
      "Epoch 4/10\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.4543 - accuracy: 0.4115 - val_loss: 1.3454 - val_accuracy: 0.4468\n",
      "Epoch 5/10\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.3672 - accuracy: 0.4139 - val_loss: 1.2238 - val_accuracy: 0.5319\n",
      "Epoch 6/10\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.2985 - accuracy: 0.4785 - val_loss: 1.0886 - val_accuracy: 0.6170\n",
      "Epoch 7/10\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.1907 - accuracy: 0.4976 - val_loss: 1.0336 - val_accuracy: 0.6809\n",
      "Epoch 8/10\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 1.0438 - accuracy: 0.6746 - val_loss: 0.8699 - val_accuracy: 0.8511\n",
      "Epoch 9/10\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.9165 - accuracy: 0.7608 - val_loss: 0.8050 - val_accuracy: 0.6809\n",
      "Epoch 10/10\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.8289 - accuracy: 0.7225 - val_loss: 0.7027 - val_accuracy: 0.8723\n",
      "1/1 [==============================] - 0s 48ms/step\n",
      "Predicted class: [50.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Fin_df\n",
    "print(Fin_df.describe())\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "Fin_columns = [ 'Occupation','Own House','Own Car','FBP Level','Score_Fin']\n",
    "\n",
    "text_columns = ['Occupation','Own House','Own Car','FBP Level']\n",
    "\n",
    "tokenizer.fit_on_texts(data['Occupation'] + ' ' + data['Own House'] + ' ' + data['Own Car'] + ' ' + data['FBP Level'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Occupation'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Own House'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Own Car'])\n",
    "X4 = tokenizer.texts_to_sequences(data['FBP Level'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "print(data['target'])\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Occupation = Input(shape=(max_sequence_length,))\n",
    "Own_House = Input(shape=(max_sequence_length,))\n",
    "Own_Car = Input(shape=(max_sequence_length,))\n",
    "FBP_Level = Input(shape=(max_sequence_length,))\n",
    "embedding_layer_Occupation = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Occupation)\n",
    "embedding_layer_Own_House = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Own_House)\n",
    "embedding_layer_Own_Car = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Own_Car)\n",
    "embedding_layer_FBP_Level = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(FBP_Level)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Occupation, embedding_layer_Own_House, embedding_layer_Own_Car, embedding_layer_FBP_Level])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Occupation, Own_House, Own_Car, FBP_Level], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Occupation = \"Business\"\n",
    "single_input_Own_House = \"Y\"\n",
    "single_input_Own_Car = \"Y\"\n",
    "single_input_FBP_Level = \"120\"\n",
    "\n",
    "single_input_Occupation = tokenizer.texts_to_sequences([single_input_Occupation])\n",
    "single_input_Own_House = tokenizer.texts_to_sequences([single_input_Own_House])\n",
    "single_input_Own_Car = tokenizer.texts_to_sequences([single_input_Own_Car])\n",
    "single_input_FBP_Level = tokenizer.texts_to_sequences([single_input_FBP_Level])\n",
    "\n",
    "single_input_Occupation = pad_sequences(single_input_Occupation, maxlen=max_sequence_length)\n",
    "single_input_Own_House = pad_sequences(single_input_Own_House, maxlen=max_sequence_length)\n",
    "single_input_Own_Car = pad_sequences(single_input_Own_Car, maxlen=max_sequence_length)\n",
    "single_input_FBP_Level = pad_sequences(single_input_FBP_Level, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Occupation, single_input_Own_House, single_input_Own_Car, single_input_FBP_Level])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "5b1d81b4-b77a-473b-a9c7-6681352fb09c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 27ms/step\n",
      "Predicted class: 60.0\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "69dbb7e4-2d9e-44c0-be63-a718e3ceac97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "14/14 [==============================] - 1s 25ms/step - loss: 1.6780 - accuracy: 0.2392 - val_loss: 1.5280 - val_accuracy: 0.2979\n",
      "Epoch 2/20\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.6330 - accuracy: 0.2919 - val_loss: 1.4541 - val_accuracy: 0.3191\n",
      "Epoch 3/20\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 1.5624 - accuracy: 0.3038 - val_loss: 1.4443 - val_accuracy: 0.3191\n",
      "Epoch 4/20\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.4764 - accuracy: 0.3182 - val_loss: 1.3545 - val_accuracy: 0.2979\n",
      "Epoch 5/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.3553 - accuracy: 0.4378 - val_loss: 1.2182 - val_accuracy: 0.5319\n",
      "Epoch 6/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.2482 - accuracy: 0.6005 - val_loss: 1.1053 - val_accuracy: 0.6809\n",
      "Epoch 7/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.1382 - accuracy: 0.6053 - val_loss: 0.9866 - val_accuracy: 0.6170\n",
      "Epoch 8/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 1.0021 - accuracy: 0.6842 - val_loss: 0.8550 - val_accuracy: 0.8085\n",
      "Epoch 9/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.8688 - accuracy: 0.7679 - val_loss: 0.7855 - val_accuracy: 0.9149\n",
      "Epoch 10/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.8097 - accuracy: 0.7799 - val_loss: 0.6906 - val_accuracy: 0.8936\n",
      "Epoch 11/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.6784 - accuracy: 0.8732 - val_loss: 0.5688 - val_accuracy: 0.9574\n",
      "Epoch 12/20\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.5749 - accuracy: 0.9306 - val_loss: 0.4918 - val_accuracy: 0.9574\n",
      "Epoch 13/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.5140 - accuracy: 0.9282 - val_loss: 0.4271 - val_accuracy: 1.0000\n",
      "Epoch 14/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.4343 - accuracy: 0.9522 - val_loss: 0.3296 - val_accuracy: 0.9787\n",
      "Epoch 15/20\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.3586 - accuracy: 0.9450 - val_loss: 0.2679 - val_accuracy: 1.0000\n",
      "Epoch 16/20\n",
      "14/14 [==============================] - 0s 18ms/step - loss: 0.3027 - accuracy: 0.9785 - val_loss: 0.2380 - val_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.2375 - accuracy: 0.9761 - val_loss: 0.2014 - val_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.1995 - accuracy: 1.0000 - val_loss: 0.1555 - val_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.1546 - accuracy: 1.0000 - val_loss: 0.1229 - val_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.1340 - accuracy: 0.9880 - val_loss: 0.1067 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 63ms/step\n",
      "Predicted class: [50.]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = Fin_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "Fin_columns = ['Occupation', 'Own House', 'Own Car', 'FBP Level', 'Score_Fin']\n",
    "\n",
    "tokenizer.fit_on_texts(data['Occupation'] + ' ' + data['Own House'] + ' ' + data['Own Car'] + ' ' + data['FBP Level'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Occupation'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Own House'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Own Car'])\n",
    "X4 = tokenizer.texts_to_sequences(data['FBP Level'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Occupation = Input(shape=(max_sequence_length,))\n",
    "Own_House = Input(shape=(max_sequence_length,))\n",
    "Own_Car = Input(shape=(max_sequence_length,))\n",
    "FBP_Level = Input(shape=(max_sequence_length,))\n",
    "embedding_layer_Occupation = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Occupation)\n",
    "embedding_layer_Own_House = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Own_House)\n",
    "embedding_layer_Own_Car = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Own_Car)\n",
    "embedding_layer_FBP_Level = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(FBP_Level)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Occupation, embedding_layer_Own_House, embedding_layer_Own_Car, embedding_layer_FBP_Level])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Occupation, Own_House, Own_Car, FBP_Level], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=20, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Occupation = \"Business\"\n",
    "single_input_Own_House = \"Y\"\n",
    "single_input_Own_Car = \"Y\"\n",
    "single_input_FBP_Level = \"130.0\"\n",
    "\n",
    "single_input_Occupation = tokenizer.texts_to_sequences([single_input_Occupation])\n",
    "single_input_Own_House = tokenizer.texts_to_sequences([single_input_Own_House])\n",
    "single_input_Own_Car = tokenizer.texts_to_sequences([single_input_Own_Car])\n",
    "single_input_FBP_Level = tokenizer.texts_to_sequences([single_input_FBP_Level])\n",
    "\n",
    "single_input_Occupation = pad_sequences(single_input_Occupation, maxlen=max_sequence_length)\n",
    "single_input_Own_House = pad_sequences(single_input_Own_House, maxlen=max_sequence_length)\n",
    "single_input_Own_Car = pad_sequences(single_input_Own_Car, maxlen=max_sequence_length)\n",
    "single_input_FBP_Level = pad_sequences(single_input_FBP_Level, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Occupation, single_input_Own_House, single_input_Own_Car, single_input_FBP_Level])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/Fin_Score.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "01067567-2ea6-4d85-a5e8-8d38acb1670d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Occupation</th>\n",
       "      <th>Own House</th>\n",
       "      <th>Own Car</th>\n",
       "      <th>FBP Level</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Professional</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Business</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Business</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>120.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Business</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>0.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Business</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>120.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Occupation Own House Own Car FBP Level  target\n",
       "0  Professional         Y       Y       0.0    30.0\n",
       "1      Business         Y       Y       0.0    40.0\n",
       "2      Business         Y       Y     120.0    60.0\n",
       "3      Business         Y       Y       0.0    40.0\n",
       "4      Business         Y       N     120.0    70.0"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Fin_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "95d44717-f9b6-4ff1-a2e5-dbeef3cc515c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted class: 60.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_class_fin(model, tokenizer, max_sequence_length, Occupation, Own_House, Own_Car, FBP_Level):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Occupation} {Own_House} {Own_Car} {FBP_Level}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Occupation = tokenizer.texts_to_sequences([Occupation])\n",
    "    single_input_sequence_Own_House = tokenizer.texts_to_sequences([Own_House])\n",
    "    single_input_sequence_Own_Car = tokenizer.texts_to_sequences([Own_Car])\n",
    "    single_input_sequence_FBP_Level = tokenizer.texts_to_sequences([FBP_Level])\n",
    "    \n",
    "    single_input_sequence_Occupation = pad_sequences(single_input_sequence_Occupation, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Own_House = pad_sequences(single_input_sequence_Own_House, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Own_Car = pad_sequences(single_input_sequence_Own_Car, maxlen=max_sequence_length)\n",
    "    single_input_sequence_FBP_Level = pad_sequences(single_input_sequence_FBP_Level, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Occupation, single_input_sequence_Own_House, single_input_sequence_Own_Car, single_input_sequence_FBP_Level])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "Occupation = \"Business\"\n",
    "Own_House = \"Y\"\n",
    "Own_Car = \"Y\"\n",
    "FBP_Level = \"120.0\"\n",
    "predicted_class = predict_class_fin(model, tokenizer, max_sequence_length, Occupation, Own_House, Own_Car, FBP_Level)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "2d34d8b2-d82e-485a-84d2-6b74c3dd3094",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 24ms/step\n",
      "Predicted class: 60.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/Fin_Score.h5')\n",
    "\n",
    "Occupation = \"Business\"\n",
    "Own_House = \"Y\"\n",
    "Own_Car = \"Y\"\n",
    "FBP_Level = \"120.0\"\n",
    "predicted_class = predict_class_fin(model, tokenizer, max_sequence_length, Occupation, Own_House, Own_Car, FBP_Level)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c1d0a7d8-c268-439a-a6d2-b1c066d7e98d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>Score_Physical_Status</th>\n",
       "      <th>Score_Lifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S.N                   Name     ID  Score_Demo  Score_Fin  \\\n",
       "0    265.0  Frist_1 Last name_133  NM265        40.0       30.0   \n",
       "1    202.0  Frist_2 Last name_102  NM202        80.0       40.0   \n",
       "2    490.0  Frist_2 Last name_246  NM490        70.0       60.0   \n",
       "3    580.0  Frist_2 Last name_291  NM580        90.0       40.0   \n",
       "4    274.0  Frist_2 Last name_138  NM274        80.0       70.0   \n",
       "..     ...                    ...    ...         ...        ...   \n",
       "577   85.0   Frist_1 Last name_43  NM085        40.0       50.0   \n",
       "578   29.0   Frist_1 Last name_15  NM029        50.0       60.0   \n",
       "579  573.0  Frist_1 Last name_287  NM573        50.0       60.0   \n",
       "580  565.0  Frist_1 Last name_283  NM565        40.0       40.0   \n",
       "581    5.0    Frist_1 Last name_3  NM005        40.0       40.0   \n",
       "\n",
       "     Score_Medical_History  Score_Medication_History  Score_Physical_Status  \\\n",
       "0                     70.0                      60.0                   50.0   \n",
       "1                     85.0                      60.0                   50.0   \n",
       "2                     85.0                      60.0                   50.0   \n",
       "3                     90.0                      60.0                   50.0   \n",
       "4                     85.0                      60.0                   50.0   \n",
       "..                     ...                       ...                    ...   \n",
       "577                   90.0                      60.0                   80.0   \n",
       "578                   90.0                      60.0                   80.0   \n",
       "579                   80.0                      60.0                   80.0   \n",
       "580                   65.0                      40.0                   80.0   \n",
       "581                   65.0                      30.0                   80.0   \n",
       "\n",
       "     Score_Lifestyle  \n",
       "0               80.0  \n",
       "1               80.0  \n",
       "2               80.0  \n",
       "3               80.0  \n",
       "4               80.0  \n",
       "..               ...  \n",
       "577            100.0  \n",
       "578            100.0  \n",
       "579            100.0  \n",
       "580            100.0  \n",
       "581            100.0  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1417821079.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MH_df.fillna(0, inplace=True)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1417821079.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MH_df['target']=MH_df['Score_Medical_History'].astype(float)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1417821079.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MH_df['# of Provider visist YTD']=MH_df['# of Provider visist YTD'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1417821079.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MH_df['Family History']=MH_df['Family History'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1417821079.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MH_df['Annual Wellness vists']=MH_df['Annual Wellness vists'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1417821079.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MH_df['Known conditions']=MH_df['Known conditions'].astype(str)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Family History</th>\n",
       "      <th>Annual Wellness vists</th>\n",
       "      <th>Known conditions</th>\n",
       "      <th># of Provider visist YTD</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Diabetic</td>\n",
       "      <td>Y</td>\n",
       "      <td>Diabetic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>70.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BP</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>N</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>BP</td>\n",
       "      <td>Y</td>\n",
       "      <td>Diabetic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Diabetic</td>\n",
       "      <td>Y</td>\n",
       "      <td>Diabetic</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>Diabetic</td>\n",
       "      <td>N</td>\n",
       "      <td>Diabetic</td>\n",
       "      <td>1.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Diabetic</td>\n",
       "      <td>2.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>0</td>\n",
       "      <td>Y</td>\n",
       "      <td>Diabetic</td>\n",
       "      <td>3.0</td>\n",
       "      <td>65.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Family History Annual Wellness vists Known conditions  \\\n",
       "0         Diabetic                     Y         Diabetic   \n",
       "1                0                     N                0   \n",
       "2                0                     N                0   \n",
       "3               BP                     N                0   \n",
       "4                0                     N                0   \n",
       "..             ...                   ...              ...   \n",
       "577             BP                     Y         Diabetic   \n",
       "578       Diabetic                     Y         Diabetic   \n",
       "579       Diabetic                     N         Diabetic   \n",
       "580              0                     Y         Diabetic   \n",
       "581              0                     Y         Diabetic   \n",
       "\n",
       "    # of Provider visist YTD  target  \n",
       "0                        0.0    70.0  \n",
       "1                        0.0    85.0  \n",
       "2                        0.0    85.0  \n",
       "3                        0.0    90.0  \n",
       "4                        0.0    85.0  \n",
       "..                       ...     ...  \n",
       "577                      0.0    90.0  \n",
       "578                      0.0    90.0  \n",
       "579                      1.0    80.0  \n",
       "580                      2.0    65.0  \n",
       "581                      3.0    65.0  \n",
       "\n",
       "[582 rows x 5 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 2.4232 - accuracy: 0.1459 - val_loss: 2.2941 - val_accuracy: 0.1915\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 2.3366 - accuracy: 0.1459 - val_loss: 2.2531 - val_accuracy: 0.1277\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 2.2265 - accuracy: 0.1914 - val_loss: 2.2230 - val_accuracy: 0.1277\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 2.1601 - accuracy: 0.2512 - val_loss: 2.1304 - val_accuracy: 0.2553\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 2.0726 - accuracy: 0.2536 - val_loss: 2.0272 - val_accuracy: 0.1702\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.9453 - accuracy: 0.3517 - val_loss: 1.9402 - val_accuracy: 0.3404\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.8386 - accuracy: 0.4043 - val_loss: 1.7810 - val_accuracy: 0.2979\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.7016 - accuracy: 0.4522 - val_loss: 1.6553 - val_accuracy: 0.2979\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.5680 - accuracy: 0.4641 - val_loss: 1.5355 - val_accuracy: 0.2979\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.4536 - accuracy: 0.5359 - val_loss: 1.3934 - val_accuracy: 0.4468\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.3090 - accuracy: 0.5550 - val_loss: 1.2540 - val_accuracy: 0.5319\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 1.1941 - accuracy: 0.5837 - val_loss: 1.1584 - val_accuracy: 0.6170\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.1026 - accuracy: 0.5861 - val_loss: 1.0972 - val_accuracy: 0.4468\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 0s 21ms/step - loss: 1.0361 - accuracy: 0.6077 - val_loss: 1.0566 - val_accuracy: 0.5532\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.9814 - accuracy: 0.6029 - val_loss: 1.0063 - val_accuracy: 0.5532\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.9325 - accuracy: 0.6651 - val_loss: 0.9937 - val_accuracy: 0.5532\n",
      "Epoch 17/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.8966 - accuracy: 0.6507 - val_loss: 0.9074 - val_accuracy: 0.5745\n",
      "Epoch 18/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.8564 - accuracy: 0.6507 - val_loss: 0.8617 - val_accuracy: 0.6596\n",
      "Epoch 19/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.8093 - accuracy: 0.6531 - val_loss: 0.8327 - val_accuracy: 0.6596\n",
      "Epoch 20/30\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.7886 - accuracy: 0.6914 - val_loss: 0.7691 - val_accuracy: 0.6383\n",
      "Epoch 21/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.7421 - accuracy: 0.7273 - val_loss: 0.7901 - val_accuracy: 0.5532\n",
      "Epoch 22/30\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 0.7360 - accuracy: 0.7010 - val_loss: 0.7539 - val_accuracy: 0.6596\n",
      "Epoch 23/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.6984 - accuracy: 0.7368 - val_loss: 0.7214 - val_accuracy: 0.6809\n",
      "Epoch 24/30\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.6780 - accuracy: 0.7679 - val_loss: 0.6967 - val_accuracy: 0.6809\n",
      "Epoch 25/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.6467 - accuracy: 0.7512 - val_loss: 0.6790 - val_accuracy: 0.6809\n",
      "Epoch 26/30\n",
      "14/14 [==============================] - 0s 19ms/step - loss: 0.6177 - accuracy: 0.8038 - val_loss: 0.6605 - val_accuracy: 0.7234\n",
      "Epoch 27/30\n",
      "14/14 [==============================] - 0s 22ms/step - loss: 0.5950 - accuracy: 0.7967 - val_loss: 0.6458 - val_accuracy: 0.7447\n",
      "Epoch 28/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5995 - accuracy: 0.7943 - val_loss: 0.6298 - val_accuracy: 0.7447\n",
      "Epoch 29/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5653 - accuracy: 0.8062 - val_loss: 0.5815 - val_accuracy: 0.7234\n",
      "Epoch 30/30\n",
      "14/14 [==============================] - 0s 20ms/step - loss: 0.5785 - accuracy: 0.7775 - val_loss: 0.5844 - val_accuracy: 0.7234\n",
      "1/1 [==============================] - 0s 53ms/step\n",
      "Predicted class: [70.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "MH_columns = [ 'Family History','Annual Wellness vists','Known conditions','# of Provider visist YTD','Score_Medical_History']\n",
    "MH_df=Scores_df[MH_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "MH_df.fillna(0, inplace=True)\n",
    "MH_df['target']=MH_df['Score_Medical_History'].astype(float)\n",
    "MH_df['# of Provider visist YTD']=MH_df['# of Provider visist YTD'].astype(str)\n",
    "MH_df['Family History']=MH_df['Family History'].astype(str)\n",
    "MH_df['Annual Wellness vists']=MH_df['Annual Wellness vists'].astype(str)\n",
    "MH_df['Known conditions']=MH_df['Known conditions'].astype(str)\n",
    "\n",
    "MH_df=MH_df.drop('Score_Medical_History',axis=1)\n",
    "display(MH_df)\n",
    "MH_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = MH_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "MH_columns = [ 'Family History','Annual Wellness vists','Known conditions','# of Provider visist YTD','Score_Medical_History']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['Family History'] + ' ' + data['Annual Wellness vists'] + ' ' + data['Known conditions'] + ' ' + data['# of Provider visist YTD'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Family History'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Annual Wellness vists'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Known conditions'])\n",
    "X4 = tokenizer.texts_to_sequences(data['# of Provider visist YTD'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Family_History = Input(shape=(max_sequence_length,))\n",
    "Annual_Wellness_vists = Input(shape=(max_sequence_length,))\n",
    "Known_conditions = Input(shape=(max_sequence_length,))\n",
    "Provider_visist_YTD = Input(shape=(max_sequence_length,))\n",
    "embedding_layer_Family_History = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Family_History)\n",
    "embedding_layer_Annual_Wellness_vists = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Annual_Wellness_vists)\n",
    "embedding_layer_Known_conditions = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Known_conditions)\n",
    "embedding_layer_Provider_visist_YTD = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Provider_visist_YTD)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Family_History, embedding_layer_Annual_Wellness_vists, embedding_layer_Known_conditions, embedding_layer_Provider_visist_YTD])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Family_History, Annual_Wellness_vists, Known_conditions, Provider_visist_YTD], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Family_History = \"Diabetic\"\n",
    "single_input_Annual_Wellness_vists = \"Y\"\n",
    "single_input_Known_conditions = \"Diabetic\"\n",
    "single_input_Provider_visist_YTD = \"0.0\"\n",
    "\n",
    "single_input_Family_History = tokenizer.texts_to_sequences([single_input_Family_History])\n",
    "single_input_Annual_Wellness_vists = tokenizer.texts_to_sequences([single_input_Annual_Wellness_vists])\n",
    "single_input_Known_conditions = tokenizer.texts_to_sequences([single_input_Known_conditions])\n",
    "single_input_Provider_visist_YTD = tokenizer.texts_to_sequences([single_input_Provider_visist_YTD])\n",
    "\n",
    "single_input_Family_History = pad_sequences(single_input_Family_History, maxlen=max_sequence_length)\n",
    "single_input_Annual_Wellness_vists = pad_sequences(single_input_Annual_Wellness_vists, maxlen=max_sequence_length)\n",
    "single_input_Known_conditions = pad_sequences(single_input_Known_conditions, maxlen=max_sequence_length)\n",
    "single_input_Provider_visist_YTD = pad_sequences(single_input_Provider_visist_YTD, maxlen=max_sequence_length)\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Family_History, single_input_Annual_Wellness_vists, single_input_Known_conditions, single_input_Provider_visist_YTD])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/MH_Score.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "eba02b2b-59a7-468e-8b3b-96be82c3f2a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 60ms/step\n",
      "Predicted class: 70.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_class_mh(model, tokenizer, max_sequence_length, Family_History, Annual_Wellness_vists, Known_conditions, Provider_visist_YTD):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Family_History} {Annual_Wellness_vists} {Known_conditions} {Provider_visist_YTD}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Family_History = tokenizer.texts_to_sequences([Family_History])\n",
    "    single_input_sequence_Annual_Wellness_vists = tokenizer.texts_to_sequences([Annual_Wellness_vists])\n",
    "    single_input_sequence_Known_conditions = tokenizer.texts_to_sequences([Known_conditions])\n",
    "    single_input_sequence_Provider_visist_YTD = tokenizer.texts_to_sequences([Provider_visist_YTD])\n",
    "    \n",
    "    single_input_sequence_Family_History = pad_sequences(single_input_sequence_Family_History, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Annual_Wellness_vists = pad_sequences(single_input_sequence_Annual_Wellness_vists, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Known_conditions = pad_sequences(single_input_sequence_Known_conditions, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Provider_visist_YTD = pad_sequences(single_input_sequence_Provider_visist_YTD, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Family_History, single_input_sequence_Annual_Wellness_vists, single_input_sequence_Known_conditions, single_input_sequence_Provider_visist_YTD])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "Family_History = \"Diabetic\"\n",
    "Annual_Wellness_vists = \"Y\"\n",
    "Known_conditions = \"Diabetic\"\n",
    "Provider_visist_YTD = \"0.0\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_mh(model, tokenizer, max_sequence_length,Family_History, Annual_Wellness_vists, Known_conditions, Provider_visist_YTD)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "b78dfbf7-594e-49b1-940e-0f45dfa4d0bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 25ms/step\n",
      "Predicted class: 70.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/MH_Score.h5')\n",
    "\n",
    "Family_History = \"Diabetic\"\n",
    "Annual_Wellness_vists = \"Y\"\n",
    "Known_conditions = \"Diabetic\"\n",
    "Provider_visist_YTD = \"0.0\"\n",
    "predicted_class = predict_class_mh(model, tokenizer, max_sequence_length, Family_History, Annual_Wellness_vists, Known_conditions, Provider_visist_YTD)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "b4685a26-19c1-48a8-bf4a-7eeb92d908ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>Score_Physical_Status</th>\n",
       "      <th>Score_Lifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S.N                   Name     ID  Score_Demo  Score_Fin  \\\n",
       "0    265.0  Frist_1 Last name_133  NM265        40.0       30.0   \n",
       "1    202.0  Frist_2 Last name_102  NM202        80.0       40.0   \n",
       "2    490.0  Frist_2 Last name_246  NM490        70.0       60.0   \n",
       "3    580.0  Frist_2 Last name_291  NM580        90.0       40.0   \n",
       "4    274.0  Frist_2 Last name_138  NM274        80.0       70.0   \n",
       "..     ...                    ...    ...         ...        ...   \n",
       "577   85.0   Frist_1 Last name_43  NM085        40.0       50.0   \n",
       "578   29.0   Frist_1 Last name_15  NM029        50.0       60.0   \n",
       "579  573.0  Frist_1 Last name_287  NM573        50.0       60.0   \n",
       "580  565.0  Frist_1 Last name_283  NM565        40.0       40.0   \n",
       "581    5.0    Frist_1 Last name_3  NM005        40.0       40.0   \n",
       "\n",
       "     Score_Medical_History  Score_Medication_History  Score_Physical_Status  \\\n",
       "0                     70.0                      60.0                   50.0   \n",
       "1                     85.0                      60.0                   50.0   \n",
       "2                     85.0                      60.0                   50.0   \n",
       "3                     90.0                      60.0                   50.0   \n",
       "4                     85.0                      60.0                   50.0   \n",
       "..                     ...                       ...                    ...   \n",
       "577                   90.0                      60.0                   80.0   \n",
       "578                   90.0                      60.0                   80.0   \n",
       "579                   80.0                      60.0                   80.0   \n",
       "580                   65.0                      40.0                   80.0   \n",
       "581                   65.0                      30.0                   80.0   \n",
       "\n",
       "     Score_Lifestyle  \n",
       "0               80.0  \n",
       "1               80.0  \n",
       "2               80.0  \n",
       "3               80.0  \n",
       "4               80.0  \n",
       "..               ...  \n",
       "577            100.0  \n",
       "578            100.0  \n",
       "579            100.0  \n",
       "580            100.0  \n",
       "581            100.0  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3615472717.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MedH_df.fillna(0, inplace=True)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3615472717.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MedH_df['target']=MedH_df['Score_Medication_History'].astype(float)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3615472717.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MedH_df['Last refill date']=MedH_df['Last refill date'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3615472717.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MedH_df['Refill due date']=MedH_df['Refill due date'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3615472717.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MedH_df['Formulary visist YTD']=MedH_df['Formulary visist YTD'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3615472717.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  MedH_df['Days of Therapy missed']=MedH_df['Days of Therapy missed'].astype(str)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Past medications</th>\n",
       "      <th>Last refill date</th>\n",
       "      <th>Refill due date</th>\n",
       "      <th>Formulary visist YTD</th>\n",
       "      <th>Days of Therapy missed</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>2023-02-02</td>\n",
       "      <td>2023-02-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-02-17</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>2023-02-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>2023-01-09</td>\n",
       "      <td>2023-02-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>2023-04-01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Y</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>2023-02-09</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Y</td>\n",
       "      <td>2023-01-08</td>\n",
       "      <td>2023-02-10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>Y</td>\n",
       "      <td>2023-03-02</td>\n",
       "      <td>2023-05-30</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Y</td>\n",
       "      <td>2023-03-04</td>\n",
       "      <td>2023-02-09</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>40.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>Y</td>\n",
       "      <td>2023-01-07</td>\n",
       "      <td>2023-02-07</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Past medications Last refill date Refill due date Formulary visist YTD  \\\n",
       "0                  Y       2023-02-02      2023-02-09                  0.0   \n",
       "1                  Y                0      2023-02-17                  0.0   \n",
       "2                  Y       2023-01-09      2023-02-09                  0.0   \n",
       "3                  Y       2023-01-09      2023-02-09                  0.0   \n",
       "4                  Y                0      2023-04-01                  0.0   \n",
       "..               ...              ...             ...                  ...   \n",
       "577                Y       2023-01-15      2023-02-09                  0.0   \n",
       "578                Y       2023-01-08      2023-02-10                  0.0   \n",
       "579                Y       2023-03-02      2023-05-30                  1.0   \n",
       "580                Y       2023-03-04      2023-02-09                  2.0   \n",
       "581                Y       2023-01-07      2023-02-07                  3.0   \n",
       "\n",
       "    Days of Therapy missed  target  \n",
       "0                      0.0    60.0  \n",
       "1                      7.0    60.0  \n",
       "2                      7.0    60.0  \n",
       "3                      3.0    60.0  \n",
       "4                      7.0    60.0  \n",
       "..                     ...     ...  \n",
       "577                    5.0    60.0  \n",
       "578                    5.0    60.0  \n",
       "579                    5.0    60.0  \n",
       "580                    5.0    40.0  \n",
       "581                    5.0    30.0  \n",
       "\n",
       "[582 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 1s 38ms/step - loss: 1.7271 - accuracy: 0.2249 - val_loss: 1.6260 - val_accuracy: 0.1915\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.6062 - accuracy: 0.2368 - val_loss: 1.6193 - val_accuracy: 0.2340\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 1.5918 - accuracy: 0.2344 - val_loss: 1.5969 - val_accuracy: 0.1915\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.5795 - accuracy: 0.2321 - val_loss: 1.5859 - val_accuracy: 0.2128\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.5604 - accuracy: 0.2392 - val_loss: 1.5601 - val_accuracy: 0.1915\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.5277 - accuracy: 0.2368 - val_loss: 1.4726 - val_accuracy: 0.2128\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.4586 - accuracy: 0.2751 - val_loss: 1.4328 - val_accuracy: 0.2340\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 1.3867 - accuracy: 0.3158 - val_loss: 1.3574 - val_accuracy: 0.2340\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 0s 23ms/step - loss: 1.3024 - accuracy: 0.3684 - val_loss: 1.2585 - val_accuracy: 0.4681\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 1.2064 - accuracy: 0.5239 - val_loss: 1.1667 - val_accuracy: 0.3617\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 1.1054 - accuracy: 0.5502 - val_loss: 1.0445 - val_accuracy: 0.5532\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 1.0091 - accuracy: 0.5813 - val_loss: 0.9542 - val_accuracy: 0.6170\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9208 - accuracy: 0.7560 - val_loss: 0.8273 - val_accuracy: 0.7447\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.8416 - accuracy: 0.7512 - val_loss: 0.7435 - val_accuracy: 0.6809\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.7898 - accuracy: 0.7488 - val_loss: 0.7062 - val_accuracy: 0.7234\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.7342 - accuracy: 0.7847 - val_loss: 0.6815 - val_accuracy: 0.7872\n",
      "Epoch 17/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.6674 - accuracy: 0.8421 - val_loss: 0.5960 - val_accuracy: 0.8936\n",
      "Epoch 18/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.6229 - accuracy: 0.8301 - val_loss: 0.5429 - val_accuracy: 0.8936\n",
      "Epoch 19/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.6072 - accuracy: 0.7871 - val_loss: 0.5748 - val_accuracy: 0.7872\n",
      "Epoch 20/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.5735 - accuracy: 0.7943 - val_loss: 0.4850 - val_accuracy: 0.8723\n",
      "Epoch 21/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.5203 - accuracy: 0.8947 - val_loss: 0.4799 - val_accuracy: 0.8936\n",
      "Epoch 22/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.5105 - accuracy: 0.8732 - val_loss: 0.4493 - val_accuracy: 0.8298\n",
      "Epoch 23/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4556 - accuracy: 0.9043 - val_loss: 0.4473 - val_accuracy: 0.8298\n",
      "Epoch 24/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.4500 - accuracy: 0.8708 - val_loss: 0.4518 - val_accuracy: 0.8936\n",
      "Epoch 25/30\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.4656 - accuracy: 0.8493 - val_loss: 0.4039 - val_accuracy: 0.8298\n",
      "Epoch 26/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4421 - accuracy: 0.8756 - val_loss: 0.3772 - val_accuracy: 0.8936\n",
      "Epoch 27/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.4039 - accuracy: 0.8923 - val_loss: 0.3826 - val_accuracy: 0.8936\n",
      "Epoch 28/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.4003 - accuracy: 0.8947 - val_loss: 0.3898 - val_accuracy: 0.8298\n",
      "Epoch 29/30\n",
      "14/14 [==============================] - 0s 24ms/step - loss: 0.3663 - accuracy: 0.9019 - val_loss: 0.3401 - val_accuracy: 0.9149\n",
      "Epoch 30/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3506 - accuracy: 0.9187 - val_loss: 0.3353 - val_accuracy: 0.9149\n",
      "1/1 [==============================] - 0s 65ms/step\n",
      "Predicted class: [60.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "MedH_columns = [ 'Past medications','Last refill date','Refill due date','Formulary visist YTD','Days of Therapy missed','Score_Medication_History']\n",
    "MedH_df=Scores_df[MedH_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "MedH_df.fillna(0, inplace=True)\n",
    "MedH_df['target']=MedH_df['Score_Medication_History'].astype(float)\n",
    "MedH_df['Last refill date']=MedH_df['Last refill date'].astype(str)\n",
    "MedH_df['Refill due date']=MedH_df['Refill due date'].astype(str)\n",
    "MedH_df['Formulary visist YTD']=MedH_df['Formulary visist YTD'].astype(str)\n",
    "MedH_df['Days of Therapy missed']=MedH_df['Days of Therapy missed'].astype(str)\n",
    "\n",
    "MedH_df=MedH_df.drop('Score_Medication_History',axis=1)\n",
    "display(MedH_df)\n",
    "MedH_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = MedH_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "MedH_columns = [ 'Past medications','Last refill date','Refill due date','Formulary visist YTD','Days of Therapy missed','Score_Medication_History']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['Past medications'] + ' ' + data['Last refill date'] + ' ' + data['Refill due date'] + ' ' + data['Formulary visist YTD']+ ' ' + data['Days of Therapy missed'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Past medications'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Last refill date'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Refill due date'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Formulary visist YTD'])\n",
    "X5 = tokenizer.texts_to_sequences(data['Days of Therapy missed'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "X5 = pad_sequences(X5, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4,X5, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Past_medications = Input(shape=(max_sequence_length,))\n",
    "Last_refill_date = Input(shape=(max_sequence_length,))\n",
    "Refill_due_date = Input(shape=(max_sequence_length,))\n",
    "Formulary_visist_YTD = Input(shape=(max_sequence_length,))\n",
    "Days_Therapy_missed = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embedding_layer_Past_medications = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Past_medications)\n",
    "embedding_layer_Last_refill_date = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Last_refill_date)\n",
    "embedding_layer_Refill_due_date = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Refill_due_date)\n",
    "embedding_layer_Formulary_visist_YTD = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Formulary_visist_YTD)\n",
    "embedding_layer_Days_Therapy_missed = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Days_Therapy_missed)\n",
    "\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Past_medications, embedding_layer_Last_refill_date, embedding_layer_Refill_due_date, embedding_layer_Formulary_visist_YTD,embedding_layer_Days_Therapy_missed])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Past_medications, Last_refill_date, Refill_due_date, Formulary_visist_YTD,Days_Therapy_missed], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train, X5_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Past_medications = \"Y\"\n",
    "single_input_Last_refill_date = \"2023-02-02\"\n",
    "single_input_Refill_due_date = \"2023-02-09\"\n",
    "single_input_Formulary_visist_YTD = \"0.0\"\n",
    "single_input_Days_Therapy_missed = \"0.0\"\n",
    "\n",
    "\n",
    "\n",
    "single_input_Past_medications = tokenizer.texts_to_sequences([single_input_Past_medications])\n",
    "single_input_Last_refill_date = tokenizer.texts_to_sequences([single_input_Last_refill_date])\n",
    "single_input_Refill_due_date = tokenizer.texts_to_sequences([single_input_Refill_due_date])\n",
    "single_input_Formulary_visist_YTD = tokenizer.texts_to_sequences([single_input_Formulary_visist_YTD])\n",
    "single_input_Days_Therapy_missed = tokenizer.texts_to_sequences([single_input_Days_Therapy_missed])\n",
    "\n",
    "\n",
    "\n",
    "single_input_Past_medications = pad_sequences(single_input_Family_History, maxlen=max_sequence_length)\n",
    "single_input_Last_refill_date = pad_sequences(single_input_Last_refill_date, maxlen=max_sequence_length)\n",
    "single_input_Refill_due_date = pad_sequences(single_input_Refill_due_date, maxlen=max_sequence_length)\n",
    "single_input_Formulary_visist_YTD = pad_sequences(single_input_Formulary_visist_YTD, maxlen=max_sequence_length)\n",
    "single_input_Days_Therapy_missed = pad_sequences(single_input_Days_Therapy_missed, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Past_medications, single_input_Last_refill_date, single_input_Refill_due_date, single_input_Formulary_visist_YTD,single_input_Days_Therapy_missed])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/MedH_Score.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "7380ac22-c6ed-4713-baa1-d0f0680c30e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 168ms/step\n",
      "Predicted class: 60.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_class_medh(model, tokenizer, max_sequence_length, Past_medications, Last_refill_date, Refill_due_date, Formulary_visist_YTD,Days_Therapy_missed):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Past_medications} {Last_refill_date} {Refill_due_date} {Formulary_visist_YTD} {Days_Therapy_missed}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Past_medications = tokenizer.texts_to_sequences([Past_medications])\n",
    "    single_input_sequence_Last_refill_date = tokenizer.texts_to_sequences([Last_refill_date])\n",
    "    single_input_sequence_Refill_due_date = tokenizer.texts_to_sequences([Refill_due_date])\n",
    "    single_input_sequence_Formulary_visist_YTD = tokenizer.texts_to_sequences([Formulary_visist_YTD])\n",
    "    single_input_sequence_Days_Therapy_missed = tokenizer.texts_to_sequences([Days_Therapy_missed])\n",
    "    \n",
    "    single_input_sequence_Past_medications = pad_sequences(single_input_sequence_Past_medications, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Last_refill_date = pad_sequences(single_input_sequence_Last_refill_date, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Refill_due_date = pad_sequences(single_input_sequence_Refill_due_date, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Formulary_visist_YTD = pad_sequences(single_input_sequence_Formulary_visist_YTD, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Days_Therapy_missed = pad_sequences(single_input_sequence_Days_Therapy_missed, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Past_medications, single_input_sequence_Last_refill_date, single_input_sequence_Refill_due_date, single_input_sequence_Formulary_visist_YTD,single_input_sequence_Days_Therapy_missed])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "Past_medications = \"Y\"\n",
    "Last_refill_date = \"2023-02-02\"\n",
    "Refill_due_date = \"2023-02-09\"\n",
    "Formulary_visist_YTD = \"0.0\"\n",
    "Days_Therapy_missed = \"0.0\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_medh(model, tokenizer, max_sequence_length,Past_medications, Last_refill_date, Refill_due_date, Formulary_visist_YTD,Days_Therapy_missed)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "00cb0a3f-fbcc-4c57-853d-e3315b09cd66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 21ms/step\n",
      "Predicted class: 60.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/MedH_Score.h5')\n",
    "\n",
    "Past_medications = \"Y\"\n",
    "Last_refill_date = \"2023-02-02\"\n",
    "Refill_due_date = \"2023-02-09\"\n",
    "Formulary_visist_YTD = \"0.0\"\n",
    "Days_Therapy_missed = \"0\"\n",
    "\n",
    "predicted_class = predict_class_medh(model, tokenizer, max_sequence_length, Past_medications, Last_refill_date, Refill_due_date, Formulary_visist_YTD,Days_Therapy_missed)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "0ae5e43c-b5fd-492a-8de3-6bcba78f279d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>Score_Physical_Status</th>\n",
       "      <th>Score_Lifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S.N                   Name     ID  Score_Demo  Score_Fin  \\\n",
       "0    265.0  Frist_1 Last name_133  NM265        40.0       30.0   \n",
       "1    202.0  Frist_2 Last name_102  NM202        80.0       40.0   \n",
       "2    490.0  Frist_2 Last name_246  NM490        70.0       60.0   \n",
       "3    580.0  Frist_2 Last name_291  NM580        90.0       40.0   \n",
       "4    274.0  Frist_2 Last name_138  NM274        80.0       70.0   \n",
       "..     ...                    ...    ...         ...        ...   \n",
       "577   85.0   Frist_1 Last name_43  NM085        40.0       50.0   \n",
       "578   29.0   Frist_1 Last name_15  NM029        50.0       60.0   \n",
       "579  573.0  Frist_1 Last name_287  NM573        50.0       60.0   \n",
       "580  565.0  Frist_1 Last name_283  NM565        40.0       40.0   \n",
       "581    5.0    Frist_1 Last name_3  NM005        40.0       40.0   \n",
       "\n",
       "     Score_Medical_History  Score_Medication_History  Score_Physical_Status  \\\n",
       "0                     70.0                      60.0                   50.0   \n",
       "1                     85.0                      60.0                   50.0   \n",
       "2                     85.0                      60.0                   50.0   \n",
       "3                     90.0                      60.0                   50.0   \n",
       "4                     85.0                      60.0                   50.0   \n",
       "..                     ...                       ...                    ...   \n",
       "577                   90.0                      60.0                   80.0   \n",
       "578                   90.0                      60.0                   80.0   \n",
       "579                   80.0                      60.0                   80.0   \n",
       "580                   65.0                      40.0                   80.0   \n",
       "581                   65.0                      30.0                   80.0   \n",
       "\n",
       "     Score_Lifestyle  \n",
       "0               80.0  \n",
       "1               80.0  \n",
       "2               80.0  \n",
       "3               80.0  \n",
       "4               80.0  \n",
       "..               ...  \n",
       "577            100.0  \n",
       "578            100.0  \n",
       "579            100.0  \n",
       "580            100.0  \n",
       "581            100.0  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1772704634.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PS_df.fillna(0, inplace=True)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1772704634.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PS_df['target']=PS_df['Score_Physical_Status'].astype(float)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1772704634.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PS_df['able to drive']=PS_df['able to drive'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1772704634.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PS_df['can walk long distance']=PS_df['can walk long distance'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1772704634.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PS_df['Dental']=PS_df['Dental'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1772704634.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PS_df['Visison']=PS_df['Visison'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/1772704634.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  PS_df['Other disability']=PS_df['Other disability'].astype(str)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>able to drive</th>\n",
       "      <th>can walk long distance</th>\n",
       "      <th>Dental</th>\n",
       "      <th>Visison</th>\n",
       "      <th>Other disability</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    able to drive can walk long distance Dental Visison Other disability  \\\n",
       "0               Y                      Y      N       N                N   \n",
       "1               Y                      Y      N       N                N   \n",
       "2               Y                      Y      N       N                N   \n",
       "3               Y                      Y      N       N                N   \n",
       "4               Y                      Y      N       N                N   \n",
       "..            ...                    ...    ...     ...              ...   \n",
       "577             N                      N      N       Y                N   \n",
       "578             N                      N      N       Y                N   \n",
       "579             N                      N      N       Y                N   \n",
       "580             N                      N      N       Y                N   \n",
       "581             N                      N      N       Y                N   \n",
       "\n",
       "     target  \n",
       "0      50.0  \n",
       "1      50.0  \n",
       "2      50.0  \n",
       "3      50.0  \n",
       "4      50.0  \n",
       "..      ...  \n",
       "577    80.0  \n",
       "578    80.0  \n",
       "579    80.0  \n",
       "580    80.0  \n",
       "581    80.0  \n",
       "\n",
       "[582 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 1.0652 - accuracy: 0.4593 - val_loss: 1.2067 - val_accuracy: 0.0638\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.9954 - accuracy: 0.4330 - val_loss: 0.8554 - val_accuracy: 0.4468\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.8754 - accuracy: 0.4545 - val_loss: 0.7503 - val_accuracy: 0.5532\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.6562 - accuracy: 0.7201 - val_loss: 0.5375 - val_accuracy: 0.9362\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.4916 - accuracy: 0.8206 - val_loss: 0.3906 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3243 - accuracy: 1.0000 - val_loss: 0.2946 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.2307 - accuracy: 1.0000 - val_loss: 0.1510 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.1200 - accuracy: 1.0000 - val_loss: 0.0879 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0712 - accuracy: 1.0000 - val_loss: 0.0549 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0431 - accuracy: 1.0000 - val_loss: 0.0349 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0284 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0103 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.0086 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0071 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.0060 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.0051 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.0047 - accuracy: 1.0000 - val_loss: 0.0044 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.0039 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.0028 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.0025 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 73ms/step\n",
      "Predicted class: [50.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "PS_columns = [ 'able to drive','can walk long distance','Dental','Visison','Other disability','Score_Physical_Status']\n",
    "PS_df=Scores_df[PS_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "PS_df.fillna(0, inplace=True)\n",
    "PS_df['target']=PS_df['Score_Physical_Status'].astype(float)\n",
    "PS_df['able to drive']=PS_df['able to drive'].astype(str)\n",
    "PS_df['can walk long distance']=PS_df['can walk long distance'].astype(str)\n",
    "PS_df['Dental']=PS_df['Dental'].astype(str)\n",
    "PS_df['Visison']=PS_df['Visison'].astype(str)\n",
    "PS_df['Other disability']=PS_df['Other disability'].astype(str)\n",
    "\n",
    "PS_df=PS_df.drop('Score_Physical_Status',axis=1)\n",
    "display(PS_df)\n",
    "PS_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = PS_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "PS_columns = [ 'able to drive','can walk long distance','Dental','Visison','Other disability','Score_Physical_Status']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['able to drive'] + ' ' + data['can walk long distance'] + ' ' + data['Dental'] + ' ' + data['Visison']+ ' ' + data['Other disability'])\n",
    "X1 = tokenizer.texts_to_sequences(data['able to drive'])\n",
    "X2 = tokenizer.texts_to_sequences(data['can walk long distance'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Dental'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Visison'])\n",
    "X5 = tokenizer.texts_to_sequences(data['Other disability'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "X5 = pad_sequences(X5, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4,X5, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "able_to_drive = Input(shape=(max_sequence_length,))\n",
    "can_walk_long_distance = Input(shape=(max_sequence_length,))\n",
    "Dental = Input(shape=(max_sequence_length,))\n",
    "Visison = Input(shape=(max_sequence_length,))\n",
    "Other_disability = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embedding_layer_able_to_drive = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(able_to_drive)\n",
    "embedding_layer_can_walk_long_distance = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(can_walk_long_distance)\n",
    "embedding_layer_Dental = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Dental)\n",
    "embedding_layer_Visison = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Visison)\n",
    "embedding_layer_Other_disability = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Other_disability)\n",
    "\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_able_to_drive, embedding_layer_can_walk_long_distance, embedding_layer_Dental, embedding_layer_Visison,embedding_layer_Other_disability])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[able_to_drive, can_walk_long_distance, Dental, Visison,Other_disability], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train, X5_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_able_to_drive = \"Y\"\n",
    "single_input_Last_can_walk_long_distance = \"Y\"\n",
    "single_input_Dental = \"N\"\n",
    "single_input_Visison = \"N\"\n",
    "single_input_Other_disability = \"N\"\n",
    "\n",
    "\n",
    "\n",
    "single_input_able_to_drive = tokenizer.texts_to_sequences([single_input_able_to_drive])\n",
    "single_input_Last_can_walk_long_distance = tokenizer.texts_to_sequences([single_input_Last_can_walk_long_distance])\n",
    "single_input_Dental = tokenizer.texts_to_sequences([single_input_Dental])\n",
    "single_input_Visison = tokenizer.texts_to_sequences([single_input_Visison])\n",
    "single_input_Other_disability = tokenizer.texts_to_sequences([single_input_Other_disability])\n",
    "\n",
    "\n",
    "\n",
    "single_input_able_to_drive = pad_sequences(single_input_able_to_drive, maxlen=max_sequence_length)\n",
    "single_input_Last_can_walk_long_distance = pad_sequences(single_input_Last_can_walk_long_distance, maxlen=max_sequence_length)\n",
    "single_input_Dental = pad_sequences(single_input_Dental, maxlen=max_sequence_length)\n",
    "single_input_Visison = pad_sequences(single_input_Visison, maxlen=max_sequence_length)\n",
    "single_input_Other_disability = pad_sequences(single_input_Other_disability, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_able_to_drive, single_input_Last_can_walk_long_distance, single_input_Dental, single_input_Visison,single_input_Other_disability])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/PS_Score.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "c473b33a-c78e-4209-b87c-4a730bf3f266",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step\n",
      "Predicted class: 80.0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def predict_class_ps(model, tokenizer, max_sequence_length, able_to_drive, can_walk_long_distance, Dental, Visison,Other_disability):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{able_to_drive} {can_walk_long_distance} {Dental} {Visison} {Other_disability}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_able_to_drive = tokenizer.texts_to_sequences([able_to_drive])\n",
    "    single_input_sequence_can_walk_long_distance = tokenizer.texts_to_sequences([can_walk_long_distance])\n",
    "    single_input_sequence_Dental = tokenizer.texts_to_sequences([Dental])\n",
    "    single_input_sequence_Visison = tokenizer.texts_to_sequences([Visison])\n",
    "    single_input_sequence_Other_disability = tokenizer.texts_to_sequences([Other_disability])\n",
    "    \n",
    "    single_input_sequence_able_to_drive = pad_sequences(single_input_sequence_able_to_drive, maxlen=max_sequence_length)\n",
    "    single_input_sequence_can_walk_long_distance = pad_sequences(single_input_sequence_can_walk_long_distance, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Dental = pad_sequences(single_input_sequence_Dental, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Visison = pad_sequences(single_input_sequence_Visison, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Other_disability = pad_sequences(single_input_sequence_Other_disability, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_able_to_drive, single_input_sequence_can_walk_long_distance, single_input_sequence_Dental, single_input_sequence_Visison,single_input_sequence_Other_disability])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "able_to_drive = \"N\"\n",
    "can_walk_long_distance = \"N\"\n",
    "Dental = \"N\"\n",
    "Visison = \"Y\"\n",
    "Other_disability = \"N\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_ps(model, tokenizer, max_sequence_length,able_to_drive, can_walk_long_distance, Dental, Visison,Other_disability)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "d90f57f7-2795-4b89-9f41-fce27c1ba674",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 83ms/step\n",
      "Predicted class: 80.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/PS_Score.h5')\n",
    "\n",
    "able_to_drive = \"N\"\n",
    "can_walk_long_distance = \"N\"\n",
    "Dental = \"N\"\n",
    "Visison = \"Y\"\n",
    "Other_disability = \"N\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_ps(loaded_model, tokenizer, max_sequence_length,able_to_drive, can_walk_long_distance, Dental, Visison,Other_disability)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "02e2af4e-b5cc-43e4-aaf0-cb25dcba50b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>Score_Physical_Status</th>\n",
       "      <th>Score_Lifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S.N                   Name     ID  Score_Demo  Score_Fin  \\\n",
       "0    265.0  Frist_1 Last name_133  NM265        40.0       30.0   \n",
       "1    202.0  Frist_2 Last name_102  NM202        80.0       40.0   \n",
       "2    490.0  Frist_2 Last name_246  NM490        70.0       60.0   \n",
       "3    580.0  Frist_2 Last name_291  NM580        90.0       40.0   \n",
       "4    274.0  Frist_2 Last name_138  NM274        80.0       70.0   \n",
       "..     ...                    ...    ...         ...        ...   \n",
       "577   85.0   Frist_1 Last name_43  NM085        40.0       50.0   \n",
       "578   29.0   Frist_1 Last name_15  NM029        50.0       60.0   \n",
       "579  573.0  Frist_1 Last name_287  NM573        50.0       60.0   \n",
       "580  565.0  Frist_1 Last name_283  NM565        40.0       40.0   \n",
       "581    5.0    Frist_1 Last name_3  NM005        40.0       40.0   \n",
       "\n",
       "     Score_Medical_History  Score_Medication_History  Score_Physical_Status  \\\n",
       "0                     70.0                      60.0                   50.0   \n",
       "1                     85.0                      60.0                   50.0   \n",
       "2                     85.0                      60.0                   50.0   \n",
       "3                     90.0                      60.0                   50.0   \n",
       "4                     85.0                      60.0                   50.0   \n",
       "..                     ...                       ...                    ...   \n",
       "577                   90.0                      60.0                   80.0   \n",
       "578                   90.0                      60.0                   80.0   \n",
       "579                   80.0                      60.0                   80.0   \n",
       "580                   65.0                      40.0                   80.0   \n",
       "581                   65.0                      30.0                   80.0   \n",
       "\n",
       "     Score_Lifestyle  \n",
       "0               80.0  \n",
       "1               80.0  \n",
       "2               80.0  \n",
       "3               80.0  \n",
       "4               80.0  \n",
       "..               ...  \n",
       "577            100.0  \n",
       "578            100.0  \n",
       "579            100.0  \n",
       "580            100.0  \n",
       "581            100.0  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3602760982.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  LS_df.fillna(0, inplace=True)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3602760982.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  LS_df['target']=LS_df['Score_Lifestyle'].astype(float)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3602760982.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  LS_df['Workouts( Walking/ Exercise)']=LS_df['Workouts( Walking/ Exercise)'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3602760982.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  LS_df['Alcoholic']=LS_df['Alcoholic'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3602760982.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  LS_df['Smoking']=LS_df['Smoking'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3602760982.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  LS_df['Professional hazards']=LS_df['Professional hazards'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3602760982.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  LS_df['Drug abuse']=LS_df['Drug abuse'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/3602760982.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  LS_df['Other factors']=LS_df['Other factors'].astype(str)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Workouts( Walking/ Exercise)</th>\n",
       "      <th>Alcoholic</th>\n",
       "      <th>Smoking</th>\n",
       "      <th>Professional hazards</th>\n",
       "      <th>Drug abuse</th>\n",
       "      <th>Other factors</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>N</td>\n",
       "      <td>Y</td>\n",
       "      <td>Y</td>\n",
       "      <td>N</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Workouts( Walking/ Exercise) Alcoholic Smoking Professional hazards  \\\n",
       "0                              Y         Y       N                    N   \n",
       "1                              Y         Y       N                    N   \n",
       "2                              Y         Y       N                    N   \n",
       "3                              Y         Y       N                    N   \n",
       "4                              Y         Y       N                    N   \n",
       "..                           ...       ...     ...                  ...   \n",
       "577                            N         N       N                    Y   \n",
       "578                            N         N       N                    Y   \n",
       "579                            N         N       N                    Y   \n",
       "580                            N         N       N                    Y   \n",
       "581                            N         N       N                    Y   \n",
       "\n",
       "    Drug abuse Other factors  target  \n",
       "0            N             N    80.0  \n",
       "1            N             N    80.0  \n",
       "2            N             N    80.0  \n",
       "3            N             N    80.0  \n",
       "4            N             N    80.0  \n",
       "..         ...           ...     ...  \n",
       "577          Y             N   100.0  \n",
       "578          Y             N   100.0  \n",
       "579          Y             N   100.0  \n",
       "580          Y             N   100.0  \n",
       "581          Y             N   100.0  \n",
       "\n",
       "[582 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 1.3269 - accuracy: 0.4187 - val_loss: 1.5299 - val_accuracy: 0.1915\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 1.2041 - accuracy: 0.5024 - val_loss: 1.1518 - val_accuracy: 0.6809\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.9843 - accuracy: 0.6746 - val_loss: 0.9131 - val_accuracy: 0.6809\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.7503 - accuracy: 0.6962 - val_loss: 0.6951 - val_accuracy: 0.6809\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.5159 - accuracy: 0.7847 - val_loss: 0.4274 - val_accuracy: 1.0000\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.3044 - accuracy: 0.9761 - val_loss: 0.2240 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.1631 - accuracy: 0.9976 - val_loss: 0.1145 - val_accuracy: 1.0000\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0844 - accuracy: 0.9976 - val_loss: 0.0564 - val_accuracy: 1.0000\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0511 - accuracy: 0.9976 - val_loss: 0.0355 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0386 - accuracy: 0.9976 - val_loss: 0.0248 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0320 - accuracy: 0.9976 - val_loss: 0.0189 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0275 - accuracy: 0.9976 - val_loss: 0.0151 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0241 - accuracy: 0.9976 - val_loss: 0.0129 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0227 - accuracy: 0.9976 - val_loss: 0.0111 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0203 - accuracy: 0.9976 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0195 - accuracy: 0.9976 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0173 - accuracy: 0.9976 - val_loss: 0.0082 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "14/14 [==============================] - 1s 40ms/step - loss: 0.0155 - accuracy: 0.9976 - val_loss: 0.0069 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "14/14 [==============================] - 1s 39ms/step - loss: 0.0157 - accuracy: 0.9976 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "14/14 [==============================] - 0s 33ms/step - loss: 0.0124 - accuracy: 0.9976 - val_loss: 0.0064 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0112 - accuracy: 0.9976 - val_loss: 0.0049 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0104 - accuracy: 0.9976 - val_loss: 0.0046 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0080 - accuracy: 0.9976 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.0038 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.0034 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.0029 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.0022 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 61ms/step\n",
      "Predicted class: [70.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "LS_columns = [ 'Workouts( Walking/ Exercise)','Alcoholic','Smoking','Professional hazards','Drug abuse','Other factors','Score_Lifestyle']\n",
    "LS_df=Scores_df[LS_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "LS_df.fillna(0, inplace=True)\n",
    "LS_df['target']=LS_df['Score_Lifestyle'].astype(float)\n",
    "LS_df['Workouts( Walking/ Exercise)']=LS_df['Workouts( Walking/ Exercise)'].astype(str)\n",
    "LS_df['Alcoholic']=LS_df['Alcoholic'].astype(str)\n",
    "LS_df['Smoking']=LS_df['Smoking'].astype(str)\n",
    "LS_df['Professional hazards']=LS_df['Professional hazards'].astype(str)\n",
    "LS_df['Drug abuse']=LS_df['Drug abuse'].astype(str)\n",
    "LS_df['Other factors']=LS_df['Other factors'].astype(str)\n",
    "\n",
    "LS_df=LS_df.drop('Score_Lifestyle',axis=1)\n",
    "display(LS_df)\n",
    "LS_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = LS_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "LS_columns = [ 'Workouts( Walking/ Exercise)','Alcoholic','Smoking','Professional hazards','Drug abuse','Other factors','Score_Lifestyle']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['Workouts( Walking/ Exercise)'] + ' ' + data['Alcoholic'] + ' ' + data['Smoking'] + ' ' + data['Professional hazards']+ ' ' + data['Drug abuse']+ ' ' + data['Other factors'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Workouts( Walking/ Exercise)'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Alcoholic'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Smoking'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Professional hazards'])\n",
    "X5 = tokenizer.texts_to_sequences(data['Drug abuse'])\n",
    "X6 = tokenizer.texts_to_sequences(data['Other factors'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "X5 = pad_sequences(X5, maxlen=max_sequence_length)\n",
    "X6 = pad_sequences(X6, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test,X6_train, X6_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4,X5,X6, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Workouts = Input(shape=(max_sequence_length,))\n",
    "Alcoholic = Input(shape=(max_sequence_length,))\n",
    "Smoking = Input(shape=(max_sequence_length,))\n",
    "Professional_hazards = Input(shape=(max_sequence_length,))\n",
    "Drug_abuse = Input(shape=(max_sequence_length,))\n",
    "Other_factors = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embedding_layer_Workouts = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Workouts)\n",
    "embedding_layer_Alcoholic = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Alcoholic)\n",
    "embedding_layer_Smoking = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Smoking)\n",
    "embedding_layer_Professional_hazards = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Professional_hazards)\n",
    "embedding_layer_Drug_abuse = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Drug_abuse)\n",
    "embedding_layer_Other_factors = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Other_factors)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Workouts, embedding_layer_Alcoholic, embedding_layer_Smoking, embedding_layer_Professional_hazards,embedding_layer_Drug_abuse,embedding_layer_Other_factors])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Workouts, Alcoholic, Smoking, Professional_hazards,Drug_abuse,Other_factors], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train, X5_train, X6_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Workouts = \"Y\"\n",
    "single_input_Alcoholic = \"N\"\n",
    "single_input_Smoking = \"N\"\n",
    "single_input_Professional_hazards = \"N\"\n",
    "single_input_Drug_abuse = \"N\"\n",
    "single_input_Other_factors = \"N\"\n",
    "\n",
    "\n",
    "single_input_Workouts = tokenizer.texts_to_sequences([single_input_Workouts])\n",
    "single_input_Alcoholic = tokenizer.texts_to_sequences([single_input_Alcoholic])\n",
    "single_input_Smoking = tokenizer.texts_to_sequences([single_input_Smoking])\n",
    "single_input_Professional_hazards = tokenizer.texts_to_sequences([single_input_Professional_hazards])\n",
    "single_input_Drug_abuse = tokenizer.texts_to_sequences([single_input_Drug_abuse])\n",
    "single_input_Other_factors = tokenizer.texts_to_sequences([single_input_Other_factors])\n",
    "\n",
    "\n",
    "single_input_Workouts = pad_sequences(single_input_Workouts, maxlen=max_sequence_length)\n",
    "single_input_Alcoholic = pad_sequences(single_input_Alcoholic, maxlen=max_sequence_length)\n",
    "single_input_Smoking = pad_sequences(single_input_Smoking, maxlen=max_sequence_length)\n",
    "single_input_Professional_hazards = pad_sequences(single_input_Professional_hazards, maxlen=max_sequence_length)\n",
    "single_input_Drug_abuse = pad_sequences(single_input_Drug_abuse, maxlen=max_sequence_length)\n",
    "single_input_Other_factors = pad_sequences(single_input_Other_factors, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Workouts, single_input_Alcoholic, single_input_Smoking, single_input_Professional_hazards,single_input_Drug_abuse,single_input_Other_factors])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/LS_Score.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "336c9e1a-aaf4-4e6b-9aad-2df3c1feaf58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 42ms/step\n",
      "Predicted class: 70.0\n"
     ]
    }
   ],
   "source": [
    "def predict_class_ls(model, tokenizer, max_sequence_length, Workouts, Alcoholic, Smoking, Professional_hazards,Drug_abuse,Other_factors):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Workouts} {Alcoholic} {Smoking} {Professional_hazards} {Drug_abuse} {Other_factors}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Workouts = tokenizer.texts_to_sequences([Workouts])\n",
    "    single_input_sequence_Alcoholic = tokenizer.texts_to_sequences([Alcoholic])\n",
    "    single_input_sequence_Smoking = tokenizer.texts_to_sequences([Smoking])\n",
    "    single_input_sequence_Professional_hazards = tokenizer.texts_to_sequences([Professional_hazards])\n",
    "    single_input_sequence_Drug_abuse = tokenizer.texts_to_sequences([Drug_abuse])\n",
    "    single_input_sequence_Other_factors = tokenizer.texts_to_sequences([Other_factors])\n",
    "    \n",
    "    single_input_sequence_Workouts = pad_sequences(single_input_sequence_Workouts, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Alcoholic = pad_sequences(single_input_sequence_Alcoholic, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Smoking = pad_sequences(single_input_sequence_Smoking, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Professional_hazards = pad_sequences(single_input_sequence_Professional_hazards, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Drug_abuse = pad_sequences(single_input_sequence_Drug_abuse, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Other_factors = pad_sequences(single_input_sequence_Other_factors, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Workouts, single_input_sequence_Alcoholic, single_input_sequence_Smoking, single_input_sequence_Professional_hazards,single_input_sequence_Drug_abuse,single_input_sequence_Other_factors])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "Workouts = \"Y\"\n",
    "Alcoholic = \"N\"\n",
    "Smoking = \"N\"\n",
    "Professional_hazards = \"N\"\n",
    "Drug_abuse = \"N\"\n",
    "Other_factors = \"N\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_ls(model, tokenizer, max_sequence_length,Workouts, Alcoholic, Smoking, Professional_hazards,Drug_abuse,Other_factors)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "64a266bb-1847-4d5b-935c-2a877776fea0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 64ms/step\n",
      "Predicted class: 70.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/LS_Score.h5')\n",
    "\n",
    "Workouts = \"Y\"\n",
    "Alcoholic = \"N\"\n",
    "Smoking = \"N\"\n",
    "Professional_hazards = \"N\"\n",
    "Drug_abuse = \"N\"\n",
    "Other_factors = \"N\"\n",
    "\n",
    "predicted_class = predict_class_ls(loaded_model, tokenizer, max_sequence_length, Workouts, Alcoholic, Smoking, Professional_hazards,Drug_abuse,Other_factors)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ae4544a5-3537-4b80-a934-ee738f6bf276",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S.N</th>\n",
       "      <th>Name</th>\n",
       "      <th>ID</th>\n",
       "      <th>Score_Demo</th>\n",
       "      <th>Score_Fin</th>\n",
       "      <th>Score_Medical_History</th>\n",
       "      <th>Score_Medication_History</th>\n",
       "      <th>Score_Physical_Status</th>\n",
       "      <th>Score_Lifestyle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>265.0</td>\n",
       "      <td>Frist_1 Last name_133</td>\n",
       "      <td>NM265</td>\n",
       "      <td>40.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>202.0</td>\n",
       "      <td>Frist_2 Last name_102</td>\n",
       "      <td>NM202</td>\n",
       "      <td>80.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>490.0</td>\n",
       "      <td>Frist_2 Last name_246</td>\n",
       "      <td>NM490</td>\n",
       "      <td>70.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>580.0</td>\n",
       "      <td>Frist_2 Last name_291</td>\n",
       "      <td>NM580</td>\n",
       "      <td>90.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>274.0</td>\n",
       "      <td>Frist_2 Last name_138</td>\n",
       "      <td>NM274</td>\n",
       "      <td>80.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>85.0</td>\n",
       "      <td>Frist_1 Last name_43</td>\n",
       "      <td>NM085</td>\n",
       "      <td>40.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>29.0</td>\n",
       "      <td>Frist_1 Last name_15</td>\n",
       "      <td>NM029</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>573.0</td>\n",
       "      <td>Frist_1 Last name_287</td>\n",
       "      <td>NM573</td>\n",
       "      <td>50.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>565.0</td>\n",
       "      <td>Frist_1 Last name_283</td>\n",
       "      <td>NM565</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Frist_1 Last name_3</td>\n",
       "      <td>NM005</td>\n",
       "      <td>40.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>65.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>80.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       S.N                   Name     ID  Score_Demo  Score_Fin  \\\n",
       "0    265.0  Frist_1 Last name_133  NM265        40.0       30.0   \n",
       "1    202.0  Frist_2 Last name_102  NM202        80.0       40.0   \n",
       "2    490.0  Frist_2 Last name_246  NM490        70.0       60.0   \n",
       "3    580.0  Frist_2 Last name_291  NM580        90.0       40.0   \n",
       "4    274.0  Frist_2 Last name_138  NM274        80.0       70.0   \n",
       "..     ...                    ...    ...         ...        ...   \n",
       "577   85.0   Frist_1 Last name_43  NM085        40.0       50.0   \n",
       "578   29.0   Frist_1 Last name_15  NM029        50.0       60.0   \n",
       "579  573.0  Frist_1 Last name_287  NM573        50.0       60.0   \n",
       "580  565.0  Frist_1 Last name_283  NM565        40.0       40.0   \n",
       "581    5.0    Frist_1 Last name_3  NM005        40.0       40.0   \n",
       "\n",
       "     Score_Medical_History  Score_Medication_History  Score_Physical_Status  \\\n",
       "0                     70.0                      60.0                   50.0   \n",
       "1                     85.0                      60.0                   50.0   \n",
       "2                     85.0                      60.0                   50.0   \n",
       "3                     90.0                      60.0                   50.0   \n",
       "4                     85.0                      60.0                   50.0   \n",
       "..                     ...                       ...                    ...   \n",
       "577                   90.0                      60.0                   80.0   \n",
       "578                   90.0                      60.0                   80.0   \n",
       "579                   80.0                      60.0                   80.0   \n",
       "580                   65.0                      40.0                   80.0   \n",
       "581                   65.0                      30.0                   80.0   \n",
       "\n",
       "     Score_Lifestyle  \n",
       "0               80.0  \n",
       "1               80.0  \n",
       "2               80.0  \n",
       "3               80.0  \n",
       "4               80.0  \n",
       "..               ...  \n",
       "577            100.0  \n",
       "578            100.0  \n",
       "579            100.0  \n",
       "580            100.0  \n",
       "581            100.0  \n",
       "\n",
       "[582 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/971773624.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SF_df.fillna(0, inplace=True)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/971773624.py:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SF_df['target']=SF_df['Score_Social_Factors'].astype(float)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/971773624.py:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SF_df['Awareness level']=SF_df['Awareness level'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/971773624.py:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SF_df['Social media activity']=SF_df['Social media activity'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/971773624.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SF_df['Social media influence']=SF_df['Social media influence'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/971773624.py:13: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SF_df['Peer group influence']=SF_df['Peer group influence'].astype(str)\n",
      "/var/folders/dl/226xlm_d0m76nf_1nrmrhwp80000gn/T/ipykernel_85314/971773624.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  SF_df['Other social activities']=SF_df['Other social activities'].astype(str)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Awareness level</th>\n",
       "      <th>Social media activity</th>\n",
       "      <th>Social media influence</th>\n",
       "      <th>Peer group influence</th>\n",
       "      <th>Other social activities</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>N</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>N</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>N</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>N</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>High</td>\n",
       "      <td>High</td>\n",
       "      <td>N</td>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>577</th>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>N</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>578</th>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>N</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>579</th>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>N</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>580</th>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>N</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>581</th>\n",
       "      <td>Low</td>\n",
       "      <td>Low</td>\n",
       "      <td>N</td>\n",
       "      <td>High</td>\n",
       "      <td>Low</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>582 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    Awareness level Social media activity Social media influence  \\\n",
       "0              High                  High                      N   \n",
       "1              High                  High                      N   \n",
       "2              High                  High                      N   \n",
       "3              High                  High                      N   \n",
       "4              High                  High                      N   \n",
       "..              ...                   ...                    ...   \n",
       "577             Low                   Low                      N   \n",
       "578             Low                   Low                      N   \n",
       "579             Low                   Low                      N   \n",
       "580             Low                   Low                      N   \n",
       "581             Low                   Low                      N   \n",
       "\n",
       "    Peer group influence Other social activities  target  \n",
       "0                    Low                     Low    60.0  \n",
       "1                    Low                     Low    60.0  \n",
       "2                    Low                     Low    60.0  \n",
       "3                    Low                     Low    60.0  \n",
       "4                    Low                     Low    60.0  \n",
       "..                   ...                     ...     ...  \n",
       "577                 High                     Low    50.0  \n",
       "578                 High                     Low    50.0  \n",
       "579                 High                     Low    50.0  \n",
       "580                 High                     Low    50.0  \n",
       "581                 High                     Low    50.0  \n",
       "\n",
       "[582 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "14/14 [==============================] - 1s 44ms/step - loss: 0.7481 - accuracy: 0.5191 - val_loss: 0.6927 - val_accuracy: 0.6383\n",
      "Epoch 2/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6782 - accuracy: 0.6196 - val_loss: 0.6595 - val_accuracy: 0.6383\n",
      "Epoch 3/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6763 - accuracy: 0.6196 - val_loss: 0.6532 - val_accuracy: 0.6383\n",
      "Epoch 4/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.6616 - accuracy: 0.6196 - val_loss: 0.6475 - val_accuracy: 0.6383\n",
      "Epoch 5/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6516 - accuracy: 0.6196 - val_loss: 0.6367 - val_accuracy: 0.6383\n",
      "Epoch 6/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6366 - accuracy: 0.6364 - val_loss: 0.6515 - val_accuracy: 1.0000\n",
      "Epoch 7/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.6009 - accuracy: 0.7321 - val_loss: 0.5768 - val_accuracy: 0.6383\n",
      "Epoch 8/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.5384 - accuracy: 0.7153 - val_loss: 0.4973 - val_accuracy: 0.7021\n",
      "Epoch 9/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.4289 - accuracy: 0.8086 - val_loss: 0.3768 - val_accuracy: 1.0000\n",
      "Epoch 10/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.3179 - accuracy: 0.9617 - val_loss: 0.2798 - val_accuracy: 1.0000\n",
      "Epoch 11/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.2165 - accuracy: 0.9785 - val_loss: 0.1710 - val_accuracy: 1.0000\n",
      "Epoch 12/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.1282 - accuracy: 1.0000 - val_loss: 0.1033 - val_accuracy: 1.0000\n",
      "Epoch 13/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0760 - accuracy: 1.0000 - val_loss: 0.0638 - val_accuracy: 1.0000\n",
      "Epoch 14/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.0392 - val_accuracy: 1.0000\n",
      "Epoch 15/30\n",
      "14/14 [==============================] - 0s 29ms/step - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0261 - val_accuracy: 1.0000\n",
      "Epoch 16/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0207 - accuracy: 1.0000 - val_loss: 0.0194 - val_accuracy: 1.0000\n",
      "Epoch 17/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0125 - val_accuracy: 1.0000\n",
      "Epoch 18/30\n",
      "14/14 [==============================] - 0s 25ms/step - loss: 0.0102 - accuracy: 1.0000 - val_loss: 0.0096 - val_accuracy: 1.0000\n",
      "Epoch 19/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.0075 - val_accuracy: 1.0000\n",
      "Epoch 20/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.0061 - val_accuracy: 1.0000\n",
      "Epoch 21/30\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.0050 - val_accuracy: 1.0000\n",
      "Epoch 22/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000\n",
      "Epoch 23/30\n",
      "14/14 [==============================] - 0s 30ms/step - loss: 0.0036 - accuracy: 1.0000 - val_loss: 0.0036 - val_accuracy: 1.0000\n",
      "Epoch 24/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.0031 - val_accuracy: 1.0000\n",
      "Epoch 25/30\n",
      "14/14 [==============================] - 0s 27ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.0027 - val_accuracy: 1.0000\n",
      "Epoch 26/30\n",
      "14/14 [==============================] - 0s 28ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.0024 - val_accuracy: 1.0000\n",
      "Epoch 27/30\n",
      "14/14 [==============================] - 0s 32ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.0021 - val_accuracy: 1.0000\n",
      "Epoch 28/30\n",
      "14/14 [==============================] - 0s 31ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.0019 - val_accuracy: 1.0000\n",
      "Epoch 29/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000\n",
      "Epoch 30/30\n",
      "14/14 [==============================] - 0s 26ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.0015 - val_accuracy: 1.0000\n",
      "1/1 [==============================] - 0s 66ms/step\n",
      "Predicted class: [60.]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "selected_columns = ['S.N', 'Name', 'ID','Score_Demo','Score_Fin','Score_Medical_History','Score_Medication_History','Score_Physical_Status','Score_Lifestyle']\n",
    "Scores1_df=Scores_df[selected_columns]\n",
    "Scores_df.describe()\n",
    "SF_columns = [ 'Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities','Score_Social_Factors']\n",
    "SF_df=Scores_df[SF_columns]\n",
    "display(Scores1_df)\n",
    "Scores1_df.describe()\n",
    "SF_df.fillna(0, inplace=True)\n",
    "SF_df['target']=SF_df['Score_Social_Factors'].astype(float)\n",
    "SF_df['Awareness level']=SF_df['Awareness level'].astype(str)\n",
    "SF_df['Social media activity']=SF_df['Social media activity'].astype(str)\n",
    "SF_df['Social media influence']=SF_df['Social media influence'].astype(str)\n",
    "SF_df['Peer group influence']=SF_df['Peer group influence'].astype(str)\n",
    "SF_df['Other social activities']=SF_df['Other social activities'].astype(str)\n",
    "\n",
    "SF_df=SF_df.drop('Score_Social_Factors',axis=1)\n",
    "display(SF_df)\n",
    "SF_df.describe()\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, Input, Concatenate, Flatten, Dense\n",
    "\n",
    "# Load your DataFrame\n",
    "data = SF_df\n",
    "\n",
    "# Preprocess your text data and encode labels\n",
    "tokenizer = Tokenizer()\n",
    "SF_columns = [ 'Awareness level','Social media activity','Social media influence','Peer group influence','Other social activities','Score_Social_Factors']\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(data['Awareness level'] + ' ' + data['Social media activity'] + ' ' + data['Social media influence'] + ' ' + data['Peer group influence']+ ' ' + data['Other social activities'])\n",
    "X1 = tokenizer.texts_to_sequences(data['Awareness level'])\n",
    "X2 = tokenizer.texts_to_sequences(data['Social media activity'])\n",
    "X3 = tokenizer.texts_to_sequences(data['Social media influence'])\n",
    "X4 = tokenizer.texts_to_sequences(data['Peer group influence'])\n",
    "X5 = tokenizer.texts_to_sequences(data['Other social activities'])\n",
    "X1 = pad_sequences(X1, maxlen=max_sequence_length)\n",
    "X2 = pad_sequences(X2, maxlen=max_sequence_length)\n",
    "X3 = pad_sequences(X3, maxlen=max_sequence_length)\n",
    "X4 = pad_sequences(X4, maxlen=max_sequence_length)\n",
    "X5 = pad_sequences(X5, maxlen=max_sequence_length)\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(data['target'])\n",
    "\n",
    "# Define the number of classes based on your dataset\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X1_train, X1_test, X2_train, X2_test, X3_train, X3_test, X4_train, X4_test, X5_train, X5_test,X6_train, X6_test, y_train, y_test = train_test_split(\n",
    "    X1, X2, X3, X4,X5,X6, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Create a multi-input text classification model\n",
    "Awareness_level = Input(shape=(max_sequence_length,))\n",
    "Social_media_activity = Input(shape=(max_sequence_length,))\n",
    "Social_media_influence = Input(shape=(max_sequence_length,))\n",
    "Peer_group_influence = Input(shape=(max_sequence_length,))\n",
    "Other_social_activities = Input(shape=(max_sequence_length,))\n",
    "\n",
    "embedding_layer_Awareness_level = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Awareness_level)\n",
    "embedding_layer_Social_media_activity = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Social_media_activity)\n",
    "embedding_layer_Social_media_influence = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Social_media_influence)\n",
    "embedding_layer_Peer_group_influence = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Peer_group_influence)\n",
    "embedding_layer_Other_social_activities = Embedding(input_dim=vocab_size, output_dim=embedding_dim)(Other_social_activities)\n",
    "\n",
    "# Concatenate all four embeddings\n",
    "concatenated_input = Concatenate()([embedding_layer_Awareness_level, embedding_layer_Social_media_activity, embedding_layer_Social_media_influence, embedding_layer_Peer_group_influence,embedding_layer_Other_social_activities])\n",
    "\n",
    "flatten_layer = Flatten()(concatenated_input)\n",
    "dense_layer = Dense(128, activation='relu')(flatten_layer)\n",
    "output_layer = Dense(num_classes, activation='softmax')(dense_layer)\n",
    "\n",
    "model = keras.Model(inputs=[Awareness_level, Social_media_activity, Social_media_influence, Peer_group_influence,Other_social_activities], outputs=output_layer)\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit([X1_train, X2_train, X3_train, X4_train, X5_train], y_train, epochs=30, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Prepare a single input for prediction\n",
    "single_input_Awareness_level = \"High\"\n",
    "single_input_Social_media_activity = \"High\"\n",
    "single_input_Social_media_influence = \"N\"\n",
    "single_input_Peer_group_influence = \"Low\"\n",
    "single_input_Other_social_activities = \"Low\"\n",
    "\n",
    "\n",
    "single_input_Awareness_level = tokenizer.texts_to_sequences([single_input_Awareness_level])\n",
    "single_input_Social_media_activity = tokenizer.texts_to_sequences([single_input_Social_media_activity])\n",
    "single_input_Social_media_influence = tokenizer.texts_to_sequences([single_input_Social_media_influence])\n",
    "single_input_Peer_group_influence = tokenizer.texts_to_sequences([single_input_Peer_group_influence])\n",
    "single_input_Other_social_activities = tokenizer.texts_to_sequences([single_input_Other_social_activities])\n",
    "\n",
    "\n",
    "single_input_Awareness_level = pad_sequences(single_input_Awareness_level, maxlen=max_sequence_length)\n",
    "single_input_Social_media_activity = pad_sequences(single_input_Social_media_activity, maxlen=max_sequence_length)\n",
    "single_input_Social_media_influence = pad_sequences(single_input_Social_media_influence, maxlen=max_sequence_length)\n",
    "single_input_Peer_group_influence = pad_sequences(single_input_Peer_group_influence, maxlen=max_sequence_length)\n",
    "single_input_Other_social_activities = pad_sequences(single_input_Other_social_activities, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "# Make predictions on the single input\n",
    "predictions = model.predict([single_input_Awareness_level, single_input_Social_media_activity, single_input_Social_media_influence, single_input_Peer_group_influence,single_input_Other_social_activities])\n",
    "\n",
    "# Decode predictions (if needed)\n",
    "predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "\n",
    "print(f'Predicted class: {predicted_class}')\n",
    "model.save('/Users/rajeshwarrao/Downloads/SF_Score.h5')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ba8316f6-afd6-4328-a026-5e66503d40a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 28ms/step\n",
      "Predicted class: 60.0\n"
     ]
    }
   ],
   "source": [
    "def predict_class_sf(model, tokenizer, max_sequence_length, Awareness_level, Social_media_activity, Social_media_influence, Peer_group_influence,Other_social_activities):\n",
    "    # Combine the four input columns into a single input string\n",
    "    single_input_text = f\"{Awareness_level} {Social_media_activity} {Social_media_influence} {Peer_group_influence} {Other_social_activities}\"\n",
    "    \n",
    "    # Tokenize and pad the single input\n",
    "    #single_input_sequence = tokenizer.texts_to_sequences([single_input_text])\n",
    "    #single_input_sequence = pad_sequences(single_input_sequence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Awareness_level = tokenizer.texts_to_sequences([Awareness_level])\n",
    "    single_input_sequence_Social_media_activity = tokenizer.texts_to_sequences([Social_media_activity])\n",
    "    single_input_sequence_Social_media_influence = tokenizer.texts_to_sequences([Social_media_influence])\n",
    "    single_input_sequence_Peer_group_influence = tokenizer.texts_to_sequences([Peer_group_influence])\n",
    "    single_input_sequence_Other_social_activities = tokenizer.texts_to_sequences([Other_social_activities])\n",
    "    \n",
    "    single_input_sequence_Awareness_level = pad_sequences(single_input_sequence_Awareness_level, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Social_media_activity = pad_sequences(single_input_sequence_Social_media_activity, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Social_media_influence = pad_sequences(single_input_sequence_Social_media_influence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Peer_group_influence = pad_sequences(single_input_sequence_Peer_group_influence, maxlen=max_sequence_length)\n",
    "    single_input_sequence_Other_social_activities = pad_sequences(single_input_sequence_Other_social_activities, maxlen=max_sequence_length)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Make predictions on the single input\n",
    "    predictions = model.predict([single_input_sequence_Awareness_level, single_input_sequence_Social_media_activity, single_input_sequence_Social_media_influence, single_input_sequence_Peer_group_influence,single_input_sequence_Other_social_activities])\n",
    "    \n",
    "    # Decode predictions\n",
    "    predicted_class = label_encoder.inverse_transform(np.argmax(predictions, axis=1))\n",
    "    \n",
    "    return predicted_class[0]  # Return the predicted class as a string\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "Awareness_level = \"High\"\n",
    "Social_media_activity = \"High\"\n",
    "Social_media_influence = \"N\"\n",
    "Peer_group_influence = \"Low\"\n",
    "Other_social_activities = \"Low\"\n",
    "\n",
    "\n",
    "predicted_class = predict_class_sf(model, tokenizer, max_sequence_length,Awareness_level, Social_media_activity, Social_media_influence, Peer_group_influence,Other_social_activities)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "090b8834-2bfd-4755-b581-3572ab9f77ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 68ms/step\n",
      "Predicted class: 50.0\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "# Load the saved model\n",
    "loaded_model = load_model('/Users/rajeshwarrao/Downloads/SF_Score.h5')\n",
    "\n",
    "Awareness_level = \"Low\"\n",
    "Social_media_activity = \"Low\"\n",
    "Social_media_influence = \"N\"\n",
    "Peer_group_influence = \"High\"\n",
    "Other_social_activities = \"Low\"\n",
    "\n",
    "predicted_class = predict_class_sf(loaded_model, tokenizer, max_sequence_length, Awareness_level, Social_media_activity, Social_media_influence, Peer_group_influence,Other_social_activities)\n",
    "print(f'Predicted class: {predicted_class}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "e8f43c05-1fb2-429e-8c44-bd025a940d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rajeshwarrao/opt/anaconda3/bin\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f341d1f1-e9ca-42c9-9946-9fbd1e040432",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
